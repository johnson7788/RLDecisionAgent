# Emojiç¿»è¯‘æœº

ä½¿ç”¨SFTå’Œå¼ºåŒ–å­¦ä¹ è¿›è¡Œè®­ç»ƒ

è¾“å…¥ç¤ºä¾‹:
Please convert the string "h-e-l-l-o-1-2" to emojis.

æ¨ç†ï¼š
h â†’ ğŸ•³ï¸  
e â†’ ğŸ˜  
l â†’ ğŸ¦  
l â†’ ğŸ¦  
o â†’ ğŸ™  
1 â†’ 1ï¸âƒ£  
2 â†’ 2ï¸âƒ£

è¾“å‡ºï¼š
Final emoji string: \boxed{ğŸ•³ï¸ğŸ˜ğŸ¦ğŸ¦ğŸ™1ï¸âƒ£2ï¸âƒ£


# æ–‡ä»¶
| æ–‡ä»¶                              | è¯´æ˜                                  |
| ------------------------------- | ----------------------------------- |
| `create_dataset.py`             | è‡ªåŠ¨ç”Ÿæˆè®­ç»ƒ/éªŒè¯æ•°æ®ï¼ˆSFT å’Œ RLï¼‰               |
| `reward_function.py`            | æ¯”è¾ƒæ¨¡å‹è¾“å‡ºçš„ emoji åºåˆ—ä¸ ground truth æ˜¯å¦ä¸€è‡´ |
| `train_sft.sh`, `train_grpo.sh` | åŸºæœ¬ç»“æ„æ— éœ€æ›´æ”¹ï¼Œä»…æ”¹è·¯å¾„å’Œ reward å             |


# è§£é‡Štrain_grpo.sh
```
python3 -m verl.trainer.main_ppo \
å¯åŠ¨ Verl ä¸­çš„ main_ppo æ¨¡å—è¿›è¡Œè®­ç»ƒ

algorithm.adv_estimator=grpo
algorithm.use_kl_in_reward=False
ä½¿ç”¨ GRPO æ¥ä¼°è®¡ Advantageï¼ˆè€Œéæ ‡å‡† GAEï¼‰ã€‚
è®­ç»ƒæ—¶ ä¸æŠŠ KL æ•£åº¦æƒ©ç½šé¡¹åŠ å…¥ rewardï¼Œå®Œå…¨é  reward_function.py é‡Œçš„ rewardã€‚

æŒ‡å®šè®­ç»ƒå’ŒéªŒè¯æ–‡ä»¶ï¼Œæ ¼å¼ä¸º .parquetã€‚
train_batch_size=128ï¼šè®­ç»ƒæ—¶æ¯ä¸ª batch å« 128 æ¡æ•°æ®ã€‚
truncation='error'ï¼šè‹¥ prompt/response è¶…å‡ºé•¿åº¦å°±æŠ¥é”™ã€‚

data.train_files=$HOME/data/speek_emoji/rl/train.parquet
data.val_files=$HOME/data/speek_emoji/rl/test.parquet
data.train_batch_size=128
data.max_prompt_length=128
data.max_response_length=128
data.filter_overlong_prompts=False
data.truncation='error'

åˆå§‹åŒ–ç­–ç•¥æ¨¡å‹ï¼ˆActorï¼‰å’Œå‚è€ƒæ¨¡å‹ï¼ˆRefï¼‰ä½¿ç”¨ SFT æ¨¡å‹çš„ç¬¬ 105 æ­¥æ£€æŸ¥ç‚¹ã€‚
actor_rollout_ref.model.path=./models/sft/global_step_105


ä¼˜åŒ–å™¨å­¦ä¹ ç‡ 1e-6ã€‚
åŠ¨æ€ batch sizeï¼Œæœ€å¤§ token æ€»æ•°é™åˆ¶ä¸º 5000ã€‚
use_remove_padding=Trueï¼šæ¨¡å‹ forward æ—¶å»é™¤å¤šä½™ paddingã€‚
actor_rollout_ref.actor.optim.lr=1e-6
actor_rollout_ref.model.use_remove_padding=True
actor_rollout_ref.actor.ppo_mini_batch_size=16
actor_rollout_ref.actor.use_dynamic_bsz=True
actor_rollout_ref.actor.ppo_max_token_len_per_gpu=5000


actor_rollout_ref.actor.use_kl_loss=False
actor_rollout_ref.actor.kl_loss_coef=0.0
actor_rollout_ref.actor.kl_loss_type=low_var_kl
ä¸ä½¿ç”¨ KL lossï¼Œkl_loss_coef=0.0ã€‚
kl_loss_type=low_var_kl åªæ˜¯å ä½ï¼ˆæ— æ•ˆï¼Œå›  kl loss è¢«å…³äº†ï¼‰ã€‚


actor_rollout_ref.actor.entropy_coeff=0
ä¸åŠ  entropy regularizationï¼ˆä¸é¼“åŠ±æ¢ç´¢ï¼‰ã€‚



actor_rollout_ref.model.enable_gradient_checkpointing=True
actor_rollout_ref.actor.fsdp_config.param_offload=True
actor_rollout_ref.actor.fsdp_config.optimizer_offload=True
actor_rollout_ref.ref.fsdp_config.param_offload=True
ä½¿ç”¨ FSDPï¼ˆFully Sharded Data Parallelï¼‰èŠ‚çœæ˜¾å­˜ã€‚
å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹æŠ€æœ¯ï¼ˆèŠ‚çœæ˜¾å­˜ï¼Œä½†è®­ç»ƒç¨æ…¢ï¼‰ã€‚


actor_rollout_ref.rollout.tensor_model_parallel_size=1
actor_rollout_ref.rollout.name=vllm
actor_rollout_ref.rollout.gpu_memory_utilization=0.7
actor_rollout_ref.rollout.n=8
ä½¿ç”¨ vllm æ¨ç†å¼•æ“ï¼ˆé«˜æ€§èƒ½ç”Ÿæˆï¼‰ã€‚
æ¯è½® rollout ç”Ÿæˆ 8 ä¸ªæ ·æœ¬ã€‚
é™åˆ¶æ¨ç†ä½¿ç”¨æ˜¾å­˜æœ€å¤š 70%ã€‚


trainer.critic_warmup=0
trainer.logger='["console","tensorboard"]'
trainer.project_name='verl_example'
trainer.experiment_name='smol135m_grpo'
trainer.val_before_train=True
trainer.n_gpus_per_node=1
trainer.nnodes=1
trainer.save_freq=-1
trainer.test_freq=5
trainer.total_epochs=2
critic_warmup=0ï¼šCritic å’Œ Actor ä¸€èµ·è®­ç»ƒã€‚
è¾“å‡ºæ—¥å¿—åˆ°æ§åˆ¶å°å’Œ TensorBoardã€‚
æ€»è®­ç»ƒ 2 ä¸ª epochã€‚
æ¯ 5 è½®æµ‹è¯•ä¸€æ¬¡ã€‚
save_freq=-1ï¼šç”¨é»˜è®¤ç­–ç•¥ä¿å­˜æ¨¡å‹ã€‚

custom_reward_function.path=reward_function.py
custom_reward_function.name=char_to_emoji_reward_function
è‡ªå®šä¹‰å¥–åŠ±å‡½æ•°åœ¨å½“å‰ç›®å½•çš„ reward_function.py æ–‡ä»¶ä¸­ã€‚
ä½¿ç”¨å…¶ä¸­çš„ char_to_emoji_reward_function å‡½æ•°ï¼Œæ¯”å¦‚æ ¹æ®è¾“å‡ºå†…å®¹è®¡ç®— rewardã€‚

```