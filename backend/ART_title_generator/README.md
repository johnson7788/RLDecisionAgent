# æ ‡é¢˜ç”Ÿæˆçš„è®­ç»ƒä»»åŠ¡ï¼Œ å¯¹æ¯”ä¸åŒçš„è®­ç»ƒæ–¹æ³•
ç»™hacker newsç”Ÿæˆæ–°é—»æ ‡é¢˜ã€‚

## ä½¿ç”¨trlæ¡†æ¶è®­ç»ƒGRPO
[reference_grpo_trainer.py](reference_grpo_trainer.py)

## ä½¿ç”¨ARTæ¡†æ¶è®­ç»ƒGRPO
[train.py](train.py)

# å¤„ç†æç¤ºè¯ï¼Œè·å–æ•°æ®ç­‰
utils.py


# è®­ç»ƒè„šæœ¬ (`reference_grpo_trainer.py` å’Œ `train.py`) çš„æ ¸å¿ƒå·®åˆ«åœ¨äº **è®­ç»ƒæ¡†æ¶ã€è®­ç»ƒé€»è¾‘ã€æ¨¡å‹è°ƒç”¨æ–¹å¼**ï¼Œä»¥åŠ **å¥–åŠ±å‡½æ•°çš„å®ç°æ–¹å¼**ã€‚
---

## 1. ä½¿ç”¨çš„è®­ç»ƒæ¡†æ¶

* **`reference_grpo_trainer.py`**

  * ä½¿ç”¨ **TRL åº“çš„ `GRPOTrainer`**ï¼ˆGroup Relative Policy Optimizationï¼Œä¸€ç§RLHF/RLAIFå¼ºåŒ–å­¦ä¹ è®­ç»ƒå™¨ï¼‰ã€‚
  * è®­ç»ƒè¿‡ç¨‹æ˜¯â€œæ ‡å‡†åŒ–â€çš„ï¼šé…ç½®è¶…å‚æ•° â†’ åŠ è½½æ¨¡å‹ï¼ˆUnsloth åŠ é€Ÿ + LoRA é€‚é…å™¨ï¼‰â†’ è°ƒç”¨ `GRPOTrainer.train()`ã€‚

* **`train.py`**

  * ä½¿ç”¨ **ART  + OpenPipe** çš„è‡ªå®šä¹‰è®­ç»ƒæ¡†æ¶ã€‚
  * è®­ç»ƒå¾ªç¯æ˜¯ **æ‰‹å†™çš„**ï¼šæ‰‹åŠ¨ rollout â†’ reward â†’ trajectory â†’ `model.train()`ã€‚

ğŸ”‘ åŒºåˆ«ï¼šå‰è€…æ˜¯**åº“æä¾›çš„é«˜é˜¶å°è£…**ï¼Œåè€…æ˜¯**è‡ªå®šä¹‰æ§åˆ¶æ›´çµæ´»**çš„è®­ç»ƒå¾ªç¯ã€‚

---

## 2. æ¨¡å‹è°ƒç”¨æ–¹å¼

* **`reference_grpo_trainer.py`**

  * ç›´æ¥åœ¨æœ¬åœ°åŠ è½½ `Qwen/Qwen2.5-0.5B-Instruct` æ¨¡å‹ã€‚
  * ç”¨ **Unsloth + VLLM** åŠ é€Ÿæ¨ç† (`FastLanguageModel.fast_generate`)ã€‚
  * LoRA å‚æ•°åˆå¹¶åˆ°æœ¬åœ°æ¨¡å‹åè¿›è¡Œè®­ç»ƒã€‚

* **`train.py`**

  * å®šä¹‰äº†ä¸€ä¸ª **ART TrainableModel**ï¼Œç”¨ OpenAI/ART API çš„æ–¹å¼æ¥è°ƒç”¨æ¨¡å‹ã€‚
  * ç”Ÿæˆå’ŒéªŒè¯æ ‡é¢˜æ—¶è°ƒç”¨ `openai.AsyncOpenAI` å®¢æˆ·ç«¯ã€‚
  * æ¨ç†èµ° **OpenAI/ART API**ï¼Œè€Œä¸æ˜¯ç›´æ¥ç”¨æœ¬åœ° HuggingFace æ¨¡å‹ã€‚

ğŸ”‘ åŒºåˆ«ï¼šå‰è€…å®Œå…¨æœ¬åœ°è·‘ï¼Œåè€…åŸºäº API å¼‚æ­¥è°ƒç”¨ï¼ˆå¯åˆ†å¸ƒå¼ã€å¯è§‚æµ‹æ€§æ›´å¼ºï¼‰ã€‚

---

## 3. å¥–åŠ±å‡½æ•°å®ç°

* **`reference_grpo_trainer.py`**

  * åœ¨ `reward_func` é‡ŒåŒæ­¥è°ƒç”¨ `calculate_rewards`ã€‚
  * å¥–åŠ±ç”±ä¸¤ä¸ªéƒ¨åˆ†ç»„æˆï¼š

    1. **Reward Model (RM)** åˆ†æ•°ï¼ˆè°ƒç”¨ `score_title` æœåŠ¡ï¼‰ã€‚
    2. **æ ‡é¢˜ä¸æ­£æ–‡åŒ¹é…éªŒè¯**ï¼ˆç”¨åŒä¸€ä¸ª Qwen æ¨¡å‹å¿«é€Ÿåˆ¤æ–­ True/Falseï¼‰ã€‚
  * æœ€ç»ˆ reward = RM åˆ†æ•° \* æ˜¯å¦åŒ¹é…ã€‚

* **`train.py`**

  * `rollout` é˜¶æ®µç”Ÿæˆæ ‡é¢˜ â†’ è°ƒç”¨ `check_title_matches_body`ï¼ˆç”¨ OpenAI API è°ƒç”¨åŸºæ¨¡å‹åˆ¤æ–­ True/Falseï¼‰ã€‚
  * ç„¶åå†è¯·æ±‚ **å¤–éƒ¨ Reward Model API** (`score_title`)ã€‚
  * reward åŒæ ·æ˜¯åŒ¹é…åˆ¤å®šåæ‰ä¿ç•™ RM åˆ†æ•°ï¼Œå¦åˆ™ç½® 0ã€‚

ğŸ”‘ åŒºåˆ«ï¼š**Reference ç”¨æœ¬åœ°æ¨¡å‹åšåŒ¹é…åˆ¤æ–­**ï¼Œ**Train ç”¨ API åšåŒ¹é…åˆ¤æ–­**ã€‚åè€…æ›´ä¸€è‡´ä½†æ›´ä¾èµ–å¤–éƒ¨æœåŠ¡ã€‚

---

## 4. è®­ç»ƒå¾ªç¯ä¸éªŒè¯

* **`reference_grpo_trainer.py`**

  * å†…ç½® **ValidationCallback**ï¼Œå®šæœŸä¿å­˜ LoRA æƒé‡å¹¶åšéªŒè¯ç”Ÿæˆã€‚
  * éªŒè¯é›† rollouts + reward è®¡ç®—åœ¨ callback å†…å®Œæˆã€‚

* **`train.py`**

  * è‡ªå·±å†™çš„è®­ç»ƒ loopï¼š

    * `for batch in data_iterator` â†’ å¤šæ¬¡ rollout â†’ è¿‡æ»¤æœ‰æ•ˆ trajectories â†’ è°ƒç”¨ `model.train()`ã€‚
    * å®šæœŸæ‰‹åŠ¨è§¦å‘éªŒè¯ (`if batch.step % EVAL_STEPS == 0`)ã€‚
  * éªŒè¯æ—¶ç›´æ¥ rollout å…¨éƒ¨ val æ•°æ® â†’ `model.log()`ã€‚

ğŸ”‘ åŒºåˆ«ï¼šReference è„šæœ¬è®­ç»ƒ-éªŒè¯æ˜¯**trainer æ¡†æ¶è‡ªåŠ¨åŒ–**ï¼ŒTrain è„šæœ¬æ˜¯**æ˜¾å¼å†™å¾ªç¯**ã€‚

---

## 5. æ•°æ®ä¸é¢„å¤„ç†

* **ä¸¤è€…ç›¸åŒç‚¹**

  * æ•°æ®æºéƒ½æ¥è‡ª HuggingFace `OpenPipe/hacker-news-scraped-stories-filtered`ã€‚
  * éƒ½ä¼šè¿‡æ»¤è¿‡é•¿çš„æ ·æœ¬ï¼ˆtoken é•¿åº¦ > 8192 æ—¶ä¸¢å¼ƒï¼‰ã€‚
  * Prompt æ ¼å¼ä¸€è‡´ï¼šsystem æŒ‡ä»¤ + user æä¾›æ­£æ–‡ã€‚

* **ç»†èŠ‚å·®å¼‚**

  * `reference_grpo_trainer.py`ï¼šè¿‡æ»¤å‡½æ•° `filter_on_length` æ˜¯åŸºäº `PreTrainedTokenizer`ã€‚
  * `train.py`ï¼šç”¨ `AutoTokenizer`ï¼ŒåŠ äº†å¼‚å¸¸å¤„ç†ï¼Œå®¹é”™æ€§æ›´å¼ºã€‚

---

## æ€»ç»“

* **`reference_grpo_trainer.py` = HuggingFace TRL + GRPOTrainer çš„æ ‡å‡†åŒ–å®ç°**

  * é€‚åˆå¿«é€Ÿè¯•éªŒå’Œå¤ç°è®ºæ–‡ã€‚
  * æœ¬åœ°æ¨¡å‹æ¨ç† & LoRA é«˜æ•ˆè®­ç»ƒã€‚
  * æ¡†æ¶å¸®ä½ ç®¡ç†è®­ç»ƒ loopã€æ—¥å¿—ã€checkpointã€‚

* **`train.py` = ART/OpenPipe çš„è‡ªå®šä¹‰è®­ç»ƒ loop**

  * æ›´çµæ´»ï¼Œå¯ä»¥æ§åˆ¶ rolloutã€reward è®¡ç®—å’Œæ—¥å¿—ä¸ŠæŠ¥ã€‚
  * æ”¯æŒå¼‚æ­¥ API è°ƒç”¨ï¼Œé€‚åˆå¤§è§„æ¨¡åˆ†å¸ƒå¼å®éªŒã€‚
  * è®­ç»ƒé€»è¾‘ç”±ç”¨æˆ·å®Œå…¨æŒæ§ï¼Œä½†å®ç°æ›´å¤æ‚ã€‚


# Train.pyçš„ä¼ªä»£ç 
load train/val datasets â†’ map(scraped_body â†’ chat messages) â†’ length filter
init ART backend & TrainableModel (LoRA, grad clip, etc.)
get start_step
for each epoch/step batch:
  for each item in batch:
    generate NUM_GENERATIONS titles with trainable model (logprobs on)
    for each title:
      check match = validator(base_model) â†’ True/False
      rm = reward_model_score(HTTP)
      reward = 0 if not match else rm
      build trajectory(messages+choice, reward, metrics)
    group trajectories per item
  filter groups with â‰¥2 valid trajectories
  if any:
    model.train(groups, lr)
  if step % EVAL_STEPS == 0:
    val_trajectories = rollout on all val items
    model.log(val_trajectories); model.delete_checkpoints()


# è®­ç»ƒ
1. å¯åŠ¨æœ¬åœ°å¥–åŠ±æ¨¡å‹
python local_reward.py

2. æµ‹è¯•æœ¬åœ°å¥–åŠ±æ¨¡å‹æ˜¯å¦OK
python reward_local_test.py

3. åœ¨.envä¸­åŠ ä¸Š
REWARD_MODEL_URL=http://127.0.0.1:7000/score

4. æµ‹è¯•æ•°æ®åŠ è½½æ˜¯å¦æ­£å¸¸
python check_dataset.py

5. å¯åŠ¨è®­ç»ƒ
export CUDA_VISIBLE_DEVICES=1
export HF_ENDPOINT=https://hf-mirror.com
python train.py

6. è¿›è¡Œæµ‹è¯•