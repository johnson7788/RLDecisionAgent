#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æ¨¡å—åŒ– Unsloth SFT è®­ç»ƒè„šæœ¬ï¼ˆå¯å¤ç”¨æ¨¡æ¿ï¼‰
----------------------------------------------------
å°†åŸå§‹è„šæœ¬å°è£…ä¸ºé€šç”¨è®­ç»ƒæ¨¡å—ï¼š
1) å‡½æ•°åŒ–ç»“æ„ + é…ç½®åŒ–å‚æ•°ï¼ˆdataclass/CLIï¼‰
2) ä¸­æ–‡æ³¨é‡Šï¼Œä¾¿äºå›¢é˜Ÿç»´æŠ¤
3) ç»Ÿä¸€æ—¥å¿—ï¼ˆæ§åˆ¶å° + æ–‡ä»¶ï¼‰ï¼Œè®°å½•è®­ç»ƒå…¨æµç¨‹
4) æ”¯æŒ 4bit/8bit/LoRA ä¸ Qwen3 èŠå¤©æ¨¡æ¿ç¤ºä¾‹
5) æ•°æ®é›†æ ‡å‡†åŒ– + ChatTemplate æ ¼å¼åŒ– + ä»…è®­ç»ƒ Assistant å“åº”

ä¾èµ–ï¼š
  - unsloth >= 2024.XX
  - transformers, peft, trl, datasets, bitsandbytes (è‹¥ä½¿ç”¨ 8bit/4bit)
  - torch, numpy

ç¤ºä¾‹ç”¨æ³•ï¼š
python unsloth_sft.py

æ³¨æ„ï¼šé»˜è®¤ä½¿ç”¨ Qwen3 æŒ‡ä»¤æ¨¡æ¿ï¼ˆqwen3-instructï¼‰ï¼Œå¹¶ä¸”åªè®­ç»ƒ assistant æ®µè½ã€‚
"""
from __future__ import annotations

import os
import sys
import json
import time
import random
import logging
import argparse
from dataclasses import dataclass, asdict, field
from typing import List, Optional

import numpy as np
import torch

# Unsloth & è®­ç»ƒç›¸å…³åº“
from unsloth import FastModel
from unsloth.chat_templates import (
    get_chat_template,
    standardize_data_formats,
    train_on_responses_only,
)
from datasets import load_dataset, Dataset
from trl import SFTTrainer, SFTConfig


# ==============================
# é…ç½®å®šä¹‰
# ==============================
@dataclass
class TrainConfig:
    # æ¨¡å‹ / Tokenizer
    model_name: str = "unsloth/Qwen3-4B-Instruct-2507"
    max_seq_length: int = 2048
    load_in_4bit: bool = True
    load_in_8bit: bool = False
    full_finetuning: bool = False
    hf_token: Optional[str] = None  # è‹¥æ¨¡å‹ä¸º gatedï¼Œå¯åœ¨æ­¤ä¼ é€’ token

    # LoRA / PEFT è®¾ç½®
    lora_r: int = 32
    lora_alpha: int = 32
    lora_dropout: float = 0.0
    bias: str = "none"
    use_gradient_checkpointing: str | bool = "unsloth"  # True / False / "unsloth"
    use_rslora: bool = False
    loftq_config: Optional[dict] = None
    target_modules: List[str] = field(default_factory=lambda: [
        "q_proj", "k_proj", "v_proj", "o_proj",
        "gate_proj", "up_proj", "down_proj",
    ])

    # Chat æ¨¡æ¿/æ•°æ®å¤„ç†
    chat_template: str = "qwen3-instruct"
    dataset_name: str = "mlabonne/FineTome-100k"
    dataset_split: str = "train"
    dataset_text_field: str = "text"  # ç”Ÿæˆåçš„æ–‡æœ¬å­—æ®µå

    # ä»…è®­ç»ƒ assistant å“åº”çš„åˆ†éš”ç¬¦ï¼ˆä¾æ®æ¨¡æ¿ï¼‰ï¼š
    instruction_part: str = "<|im_start|>user\n"
    response_part: str = "<|im_start|>assistant\n"

    # è®­ç»ƒè¶…å‚
    per_device_train_batch_size: int = 2
    gradient_accumulation_steps: int = 4
    warmup_steps: int = 5
    max_steps: int = 60  # æˆ–ä½¿ç”¨ num_train_epochs
    num_train_epochs: Optional[int] = 4
    learning_rate: float = 2e-4
    logging_steps: int = 1
    optim: str = "adamw_8bit"  # éœ€ bitsandbytes
    weight_decay: float = 0.01
    lr_scheduler_type: str = "linear"
    seed: int = 3407
    report_to: str = "none"  # å¯åˆ‡æ¢ä¸º wandb

    # è¾“å‡º/ä¿å­˜
    output_dir: str = "./outputs/qwen3_4b_lora"
    save_steps: Optional[int] = 2  # å®šæœŸä¿å­˜
    save_total_limit: Optional[int] = None
    logging_dir: Optional[str] = None  # è‡ªå®šä¹‰æ—¥å¿—ç›®å½•


# ==============================
# æ—¥å¿—å·¥å…·
# ==============================

def setup_logging(output_dir: str, logging_dir: Optional[str] = None) -> logging.Logger:
    """é…ç½®æ—¥å¿—ï¼šæ§åˆ¶å° + æ–‡ä»¶ã€‚
    """
    os.makedirs(output_dir, exist_ok=True)
    log_dir = logging_dir or os.path.join(output_dir, "logs")
    os.makedirs(log_dir, exist_ok=True)

    log_file = os.path.join(log_dir, time.strftime("train_%Y%m%d_%H%M%S.log"))

    logger = logging.getLogger("unsloth_sft")
    logger.setLevel(logging.INFO)
    logger.handlers.clear()

    # æ§åˆ¶å°è¾“å‡º
    ch = logging.StreamHandler(sys.stdout)
    ch.setLevel(logging.INFO)
    ch_formatter = logging.Formatter("[%(asctime)s] %(levelname)s - %(message)s", "%H:%M:%S")
    ch.setFormatter(ch_formatter)

    # æ–‡ä»¶è¾“å‡º
    fh = logging.FileHandler(log_file, encoding="utf-8")
    fh.setLevel(logging.INFO)
    fh_formatter = logging.Formatter(
        "[%(asctime)s] %(levelname)s %(name)s:%(lineno)d - %(message)s",
        "%Y-%m-%d %H:%M:%S",
    )
    fh.setFormatter(fh_formatter)

    logger.addHandler(ch)
    logger.addHandler(fh)

    logger.info(f"æ—¥å¿—å†™å…¥: {log_file}")
    return logger


# ==============================
# é€šç”¨å·¥å…·
# ==============================

def set_seed(seed: int, logger: logging.Logger | None = None) -> None:
    """è®¾ç½®éšæœºç§å­ï¼Œä¿è¯å¯å¤ç°ã€‚"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = False  # æ›´å¿«è®­ç»ƒï¼›å¦‚éœ€å®Œå…¨ç¡®å®šæ€§å¯è®¾ True
    torch.backends.cudnn.benchmark = True
    if logger:
        logger.info(f"éšæœºç§å­å·²è®¾ç½®: {seed}")


def log_env_info(logger: logging.Logger) -> None:
    """è®°å½•ç¯å¢ƒä¸ GPU ä¿¡æ¯ã€‚"""
    logger.info(f"PyTorch: {torch.__version__}")
    if torch.cuda.is_available():
        gpu = torch.cuda.get_device_properties(0)
        max_mem = round(gpu.total_memory / 1024 / 1024 / 1024, 3)
        start_reserved = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)
        logger.info(f"GPU: {gpu.name}  æ˜¾å­˜ä¸Šé™: {max_mem} GB  å¯åŠ¨ä¿ç•™: {start_reserved} GB")
    else:
        logger.warning("æœªæ£€æµ‹åˆ°å¯ç”¨ CUDAï¼Œè®­ç»ƒå°†ä½¿ç”¨ CPUï¼ˆå¯èƒ½éå¸¸æ…¢ï¼‰")


# ==============================
# æ„å»ºæ¨¡å‹ & Tokenizer
# ==============================

def build_model_and_tokenizer(cfg: TrainConfig, logger: logging.Logger):
    """åŠ è½½åŸºç¡€æ¨¡å‹ï¼Œå¹¶æ³¨å…¥ LoRA é€‚é…å™¨ï¼›åº”ç”¨ Chat æ¨¡æ¿ã€‚"""
    if cfg.load_in_4bit and cfg.load_in_8bit:
        logger.warning("4bit ä¸ 8bit ä»…èƒ½äºŒé€‰ä¸€ï¼Œå·²ä¼˜å…ˆä½¿ç”¨ 4bitï¼Œå¹¶å…³é—­ 8bitï¼")
        cfg.load_in_8bit = False

    logger.info("å¼€å§‹åŠ è½½åŸºç¡€æ¨¡å‹â€¦")
    model, tokenizer = FastModel.from_pretrained(
        model_name=cfg.model_name,
        max_seq_length=cfg.max_seq_length,
        load_in_4bit=cfg.load_in_4bit,
        load_in_8bit=cfg.load_in_8bit,
        full_finetuning=cfg.full_finetuning,
        token=cfg.hf_token,
    )
    logger.info("åŸºç¡€æ¨¡å‹åŠ è½½å®Œæˆã€‚")

    logger.info("æ³¨å…¥ LoRA é€‚é…å™¨â€¦")
    model = FastModel.get_peft_model(
        model,
        r=cfg.lora_r,
        target_modules=cfg.target_modules,
        lora_alpha=cfg.lora_alpha,
        lora_dropout=cfg.lora_dropout,
        bias=cfg.bias,
        use_gradient_checkpointing=cfg.use_gradient_checkpointing,
        random_state=cfg.seed,
        use_rslora=cfg.use_rslora,
        loftq_config=cfg.loftq_config,
    )
    logger.info("LoRA é€‚é…å™¨æ³¨å…¥å®Œæˆã€‚")

    logger.info(f"åº”ç”¨ Chat æ¨¡æ¿: {cfg.chat_template}")
    tokenizer = get_chat_template(tokenizer, chat_template=cfg.chat_template)
    return model, tokenizer


# ==============================
# æ•°æ®é›†å‡†å¤‡
# ==============================

def load_and_prepare_dataset(cfg: TrainConfig, tokenizer, logger: logging.Logger) -> Dataset:
    """åŠ è½½æ•°æ®é›†ï¼Œç»Ÿä¸€å¯¹è¯æ ¼å¼ï¼Œå¹¶è½¬æˆè®­ç»ƒå­—æ®µ textã€‚"""
    logger.info(f"åŠ è½½æ•°æ®é›†: {cfg.dataset_name} [{cfg.dataset_split}] â€¦")
    dataset = load_dataset(cfg.dataset_name, split=cfg.dataset_split)

    logger.info("æ ‡å‡†åŒ–ä¸ºé€šç”¨ conversations æ ¼å¼â€¦")
    dataset = standardize_data_formats(dataset)

    def formatting_prompts_func(examples):
        convos = examples["conversations"]
        # å°†å¤šè½®å¯¹è¯ç”¨ ChatTemplate æ¸²æŸ“ä¸ºçº¯æ–‡æœ¬ï¼ˆå«ç‰¹æ®Šæ ‡è®°ï¼‰
        texts = [
            tokenizer.apply_chat_template(
                convo, tokenize=False, add_generation_prompt=False
            )
            for convo in convos
        ]
        return {cfg.dataset_text_field: texts}

    logger.info("æ ¹æ® Chat æ¨¡æ¿æ¸²æŸ“æ–‡æœ¬å­—æ®µâ€¦")
    dataset = dataset.map(formatting_prompts_func, batched=True, desc="apply_chat_template")

    # æ‰“å°/è®°å½•ä¸€ä¸ªæ ·æœ¬ä¾¿äºæ£€æŸ¥
    try:
        sample_txt = dataset[0][cfg.dataset_text_field][:256].replace("\n", " ")
        logger.info(f"æ ·æœ¬é¢„è§ˆ: {sample_txt}â€¦")
    except Exception as e:
        logger.warning(f"æ ·æœ¬é¢„è§ˆå¤±è´¥: {e}")

    return dataset


# ==============================
# æ„å»º Trainer
# ==============================

def build_trainer(model, tokenizer, dataset: Dataset, cfg: TrainConfig, logger: logging.Logger) -> SFTTrainer:
    """åˆ›å»º SFTTrainerï¼Œå¹¶è®¾ç½®ä»…è®­ç»ƒå›ç­”æ®µè½ã€‚"""
    logger.info("åˆ›å»º SFTTrainerâ€¦")

    training_args = SFTConfig(
        output_dir=cfg.output_dir,
        dataset_text_field=cfg.dataset_text_field,
        per_device_train_batch_size=cfg.per_device_train_batch_size,
        gradient_accumulation_steps=cfg.gradient_accumulation_steps,
        warmup_steps=cfg.warmup_steps,
        max_steps=cfg.max_steps,
        # ä¹Ÿå¯é€‰æ‹©è®¾å®š num_train_epochsï¼ˆä¸¤è€…ä¸è¦åŒæ—¶å¼ºåˆ¶ï¼‰
        num_train_epochs=cfg.num_train_epochs,
        learning_rate=cfg.learning_rate,
        logging_steps=cfg.logging_steps,
        optim=cfg.optim,
        weight_decay=cfg.weight_decay,
        lr_scheduler_type=cfg.lr_scheduler_type,
        seed=cfg.seed,
        report_to=cfg.report_to,
        save_steps=cfg.save_steps,
        save_total_limit=cfg.save_total_limit,
    )

    trainer = SFTTrainer(
        model=model,
        tokenizer=tokenizer,
        train_dataset=dataset,
        eval_dataset=None,
        args=training_args,
    )

    logger.info("åˆ‡æ¢ä¸ºä»…å­¦ä¹  assistant å“åº”ï¼ˆå¿½ç•¥ user æŒ‡ä»¤éƒ¨åˆ†çš„ lossï¼‰â€¦")
    trainer = train_on_responses_only(
        trainer,
        instruction_part=cfg.instruction_part,
        response_part=cfg.response_part,
    )

    # è®°å½•ä¸€ä¸ªç¼–ç åçš„æ ·æœ¬ï¼Œç¡®è®¤ mask æ˜¯å¦åˆç†ï¼ˆåªåšæ—¥å¿—é¢„è§ˆï¼Œä¸å½±å“è®­ç»ƒï¼‰
    try:
        decoded = tokenizer.decode(trainer.train_dataset[0]["input_ids"][:256])
        logger.info(f"ç¼–ç æ ·æœ¬é¢„è§ˆ: {decoded}")
    except Exception as e:
        logger.warning(f"ç¼–ç æ ·æœ¬é¢„è§ˆå¤±è´¥: {e}")

    return trainer


# ==============================
# è®­ç»ƒä¸åº¦é‡
# ==============================

def train_and_report(trainer: SFTTrainer, logger: logging.Logger):
    """å¯åŠ¨è®­ç»ƒå¹¶è®°å½•æ˜¾å­˜ä¸è€—æ—¶ã€‚"""
    gpu_stats = torch.cuda.get_device_properties(0) if torch.cuda.is_available() else None
    start_reserved = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3) if torch.cuda.is_available() else 0.0
    max_mem = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3) if gpu_stats else 0.0

    if gpu_stats:
        logger.info(f"GPU = {gpu_stats.name}. Max memory = {max_mem} GB.")
        logger.info(f"å¯åŠ¨æ—¶ä¿ç•™æ˜¾å­˜ = {start_reserved} GB.")

    logger.info("å¼€å§‹è®­ç»ƒâ€¦")
    trainer_stats = trainer.train()
    logger.info("è®­ç»ƒå®Œæˆã€‚")

    used_reserved = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3) if torch.cuda.is_available() else 0.0
    used_for_lora = round(used_reserved - start_reserved, 3)
    used_pct = round(used_reserved / max_mem * 100, 3) if max_mem else 0.0
    lora_pct = round(used_for_lora / max_mem * 100, 3) if max_mem else 0.0

    rt = trainer_stats.metrics.get('train_runtime', 0.0)
    logger.info(f"è®­ç»ƒè€—æ—¶ {rt:.2f} ç§’ï¼ˆçº¦ {rt/60:.2f} åˆ†é’Ÿï¼‰ã€‚")
    logger.info(f"å³°å€¼ä¿ç•™æ˜¾å­˜ = {used_reserved} GBï¼›å…¶ä¸­è®­ç»ƒå¢é‡ = {used_for_lora} GBã€‚")
    if max_mem:
        logger.info(f"æ˜¾å­˜å ç”¨å³°å€¼å æ¯” = {used_pct}%ï¼›è®­ç»ƒå¢é‡å æ¯” = {lora_pct}%ã€‚")

    return trainer_stats


# ==============================
# ä¿å­˜æ¨¡å‹
# ==============================

def save_model(trainer: SFTTrainer, tokenizer, output_dir: str, logger: logging.Logger) -> None:
    """å°è¯•ä½¿ç”¨TRL/Transformers ä¿å­˜ã€‚"""
    os.makedirs(output_dir, exist_ok=True)
    logger.info(f"Unsloth ä¿å­˜æ¨¡å‹ Trainer.save_model")
    try:
        trainer.save_model(output_dir)
        tokenizer.save_pretrained(output_dir)
        logger.info(f"æ¨¡å‹å·²ä¿å­˜è‡³: {output_dir}")
    except Exception as ee:
        logger.error(f"ä¿å­˜å¤±è´¥: {ee}")
        raise

# ==============================
# ä¸»æµç¨‹
# ==============================

def parse_args() -> TrainConfig:
    parser = argparse.ArgumentParser(description="Unsloth SFT è®­ç»ƒè„šæœ¬ï¼ˆé€šç”¨æ¨¡æ¿ï¼‰")

    # ä»…åˆ—å‡ºå¸¸ç”¨é¡¹ï¼›å…¶ä½™è¯·ç›´æ¥åœ¨ dataclass é»˜è®¤å€¼ä¸­æ”¹
    parser.add_argument("--model_name", type=str, default=TrainConfig.model_name)
    parser.add_argument("--max_seq_length", type=int, default=TrainConfig.max_seq_length)
    parser.add_argument("--load_in_4bit", action="store_true", default=True)
    parser.add_argument("--load_in_8bit", action="store_true", default=False)
    parser.add_argument("--full_finetuning", action="store_true", default=False)
    parser.add_argument("--hf_token", type=str, default=None)

    parser.add_argument("--lora_r", type=int, default=TrainConfig.lora_r)
    parser.add_argument("--lora_alpha", type=int, default=TrainConfig.lora_alpha)
    parser.add_argument("--lora_dropout", type=float, default=TrainConfig.lora_dropout)

    parser.add_argument("--chat_template", type=str, default=TrainConfig.chat_template)
    parser.add_argument("--dataset_name", type=str, default=TrainConfig.dataset_name)
    parser.add_argument("--dataset_split", type=str, default=TrainConfig.dataset_split)

    parser.add_argument("--per_device_train_batch_size", type=int, default=TrainConfig.per_device_train_batch_size)
    parser.add_argument("--gradient_accumulation_steps", type=int, default=TrainConfig.gradient_accumulation_steps)
    parser.add_argument("--warmup_steps", type=int, default=TrainConfig.warmup_steps)
    parser.add_argument("--max_steps", type=int, default=TrainConfig.max_steps)
    parser.add_argument("--num_train_epochs", type=float, default=TrainConfig.num_train_epochs)
    parser.add_argument("--learning_rate", type=float, default=TrainConfig.learning_rate)
    parser.add_argument("--logging_steps", type=int, default=TrainConfig.logging_steps)
    parser.add_argument("--optim", type=str, default=TrainConfig.optim)
    parser.add_argument("--weight_decay", type=float, default=TrainConfig.weight_decay)
    parser.add_argument("--lr_scheduler_type", type=str, default=TrainConfig.lr_scheduler_type)
    parser.add_argument("--seed", type=int, default=TrainConfig.seed)
    parser.add_argument("--report_to", type=str, default=TrainConfig.report_to)

    parser.add_argument("--output_dir", type=str, default=TrainConfig.output_dir)
    parser.add_argument("--save_steps", type=int, default=TrainConfig.save_steps)
    parser.add_argument("--save_total_limit", type=int, default=None)

    args = parser.parse_args()

    # å°† argparse åˆå¹¶åˆ° dataclassï¼ˆæœªæš´éœ²çš„å­—æ®µæ²¿ç”¨é»˜è®¤å€¼ï¼‰
    cfg = TrainConfig(
        model_name=args.model_name,
        max_seq_length=args.max_seq_length,
        load_in_4bit=args.load_in_4bit,
        load_in_8bit=args.load_in_8bit,
        full_finetuning=args.full_finetuning,
        hf_token=args.hf_token,
        lora_r=args.lora_r,
        lora_alpha=args.lora_alpha,
        lora_dropout=args.lora_dropout,
        chat_template=args.chat_template,
        dataset_name=args.dataset_name,
        dataset_split=args.dataset_split,
        per_device_train_batch_size=args.per_device_train_batch_size,
        gradient_accumulation_steps=args.gradient_accumulation_steps,
        warmup_steps=args.warmup_steps,
        max_steps=args.max_steps,
        num_train_epochs=args.num_train_epochs,
        learning_rate=args.learning_rate,
        logging_steps=args.logging_steps,
        optim=args.optim,
        weight_decay=args.weight_decay,
        lr_scheduler_type=args.lr_scheduler_type,
        seed=args.seed,
        report_to=args.report_to,
        output_dir=args.output_dir,
        save_steps=args.save_steps,
        save_total_limit=args.save_total_limit,
    )
    return cfg


def main(cfg: TrainConfig | None = None) -> None:
    cfg = cfg or TrainConfig()

    # æ—¥å¿—
    logger = setup_logging(cfg.output_dir, cfg.logging_dir)

    # æ‰“å°é…ç½®
    logger.info("===== è®­ç»ƒé…ç½® =====")
    logger.info(json.dumps(asdict(cfg), ensure_ascii=False, indent=2))

    # éšæœºç§å­ & ç¯å¢ƒä¿¡æ¯
    set_seed(cfg.seed, logger)
    log_env_info(logger)

    # æ„å»ºæ¨¡å‹ä¸ tokenizer
    model, tokenizer = build_model_and_tokenizer(cfg, logger)

    # å‡†å¤‡æ•°æ®é›†
    dataset = load_and_prepare_dataset(cfg, tokenizer, logger)

    # æ„å»º Trainer
    trainer = build_trainer(model, tokenizer, dataset, cfg, logger)

    # è®­ç»ƒå¹¶æŠ¥å‘Š
    try:
        stats = train_and_report(trainer, logger)
    except Exception as e:
        logger.exception(f"è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        raise

    # ä¿å­˜æ¨¡å‹
    try:
        save_model(trainer, tokenizer, cfg.output_dir, logger)
    except Exception as e:
        logger.exception(f"ä¿å­˜æ¨¡å‹å¤±è´¥: {e}")
        raise

    logger.info("ğŸ‰ å…¨æµç¨‹ç»“æŸã€‚")


if __name__ == "__main__":
    main(parse_args())
