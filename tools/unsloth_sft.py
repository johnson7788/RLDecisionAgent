#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æ¨¡å—åŒ– Unsloth SFT è®­ç»ƒè„šæœ¬ï¼ˆå¯å¤ç”¨æ¨¡æ¿ï¼‰ + Weights & Biases é›†æˆ
----------------------------------------------------
æ–°å¢èƒ½åŠ›ï¼š
1) è®­ç»ƒè¿‡ç¨‹è‡ªåŠ¨ä¸ŠæŠ¥åˆ° W&Bï¼ˆloss/learning_rate/steps ç­‰ç”± TRL å†…ç½®ä¸ŠæŠ¥ï¼‰
2) å…³é”®èµ„æºæŒ‡æ ‡ï¼ˆGPU æ˜¾å­˜å³°å€¼ã€è®­ç»ƒè€—æ—¶ç­‰ï¼‰è‡ªå®šä¹‰ wandb.log
3) è®­ç»ƒå¼‚å¸¸æ•è·å¹¶ä¸ŠæŠ¥åˆ° W&Bï¼ˆalert + run.summary æ ‡è®°å¤±è´¥ï¼Œfinish(exit_code=1)ï¼‰
4) è®­ç»ƒå®Œæˆè‡ªåŠ¨æ ‡è®° run æˆåŠŸçŠ¶æ€å¹¶å¯é€‰ä¸Šä¼ æœ€ç»ˆæ¨¡å‹ä¸º artifact
5) æ”¯æŒå‘½ä»¤è¡Œå¼€å…³ï¼ˆ--use_wandbã€--wandb_projectã€--wandb_run_name ...ï¼‰

# .envä¸­é…ç½®WANDB_BASE_URLå’ŒWANDB_API_KEY

ä¾èµ–ï¼š
  - python-dotenv
  - wandb
  - unsloth >= 2024.XX
  - transformers, peft, trl, datasets, bitsandbytes (è‹¥ä½¿ç”¨ 8bit/4bit)
  - torch, numpy

ç¤ºä¾‹ç”¨æ³•ï¼š
  pip install wandb
  wandb login  # æˆ–è®¾ç½®ç¯å¢ƒå˜é‡ WANDB_API_KEY
  python unsloth_sft_wandb.py \
    --report_to wandb \
    --wandb_project unsloth-sft \
    --wandb_run_name qwen3-4b-lora \
    --output_dir ./outputs/qwen3_4b_lora

æ³¨æ„ï¼šé»˜è®¤å¯ç”¨ W&Bï¼ˆuse_wandb=Trueï¼‰ã€‚å¦‚éœ€ç¦ç”¨ï¼š--no_use_wandbã€‚
"""
from __future__ import annotations
import dotenv
import os
import sys
import json
import time
import random
import logging
import argparse
from dataclasses import dataclass, asdict, field
from typing import List, Optional
from pathlib import Path
import numpy as np
import torch
dotenv.load_dotenv()

# ---------- å¯é€‰å¯¼å…¥ wandb ----------
try:
    import wandb
    WANDB_AVAILABLE = True
    print(f"WANDB_AVAILABLEæ˜¯å¯ç”¨çš„ï¼Œå·²ç»å®‰è£…äº†wandb")
except Exception:
    wandb = None  # type: ignore
    WANDB_AVAILABLE = False
    print(f"WANDB_AVAILABLEä¸å¯ç”¨ï¼Œæ²¡æœ‰å®‰è£…wandb")

# Unsloth & è®­ç»ƒç›¸å…³åº“
from unsloth import FastModel
from unsloth.chat_templates import (
    get_chat_template,
    standardize_data_formats,
    train_on_responses_only,
)
from datasets import load_dataset, Dataset
from trl import SFTTrainer, SFTConfig


# ==============================
# é…ç½®å®šä¹‰
# ==============================
@dataclass
class TrainConfig:
    # æ¨¡å‹ / Tokenizer
    model_name: str = "unsloth/Qwen3-4B-Instruct-2507"
    max_seq_length: int = 2048
    load_in_4bit: bool = True
    load_in_8bit: bool = False
    full_finetuning: bool = False
    hf_token: Optional[str] = None  # è‹¥æ¨¡å‹ä¸º gatedï¼Œå¯åœ¨æ­¤ä¼ é€’ token

    # LoRA / PEFT è®¾ç½®
    lora_r: int = 32
    lora_alpha: int = 32
    lora_dropout: float = 0.0
    bias: str = "none"
    use_gradient_checkpointing: str | bool = "unsloth"  # True / False / "unsloth"
    use_rslora: bool = False
    loftq_config: Optional[dict] = None
    target_modules: List[str] = field(default_factory=lambda: [
        "q_proj", "k_proj", "v_proj", "o_proj",
        "gate_proj", "up_proj", "down_proj",
    ])

    # Chat æ¨¡æ¿/æ•°æ®å¤„ç†
    chat_template: str = "qwen3-instruct"
    dataset_name: str = "mlabonne/FineTome-100k"
    dataset_split: str = "train"
    dataset_text_field: str = "text"  # ç”Ÿæˆåçš„æ–‡æœ¬å­—æ®µå

    # ä»…è®­ç»ƒ assistant å“åº”çš„åˆ†éš”ç¬¦ï¼ˆä¾æ®æ¨¡æ¿ï¼‰ï¼š
    instruction_part: str = "<|im_start|>user\n"
    response_part: str = "<|im_start|>assistant\n"

    # è®­ç»ƒè¶…å‚
    per_device_train_batch_size: int = 2
    gradient_accumulation_steps: int = 4
    warmup_steps: int = 5
    max_steps: int = 60  # æˆ–ä½¿ç”¨ num_train_epochs
    num_train_epochs: Optional[int] = 4
    learning_rate: float = 2e-4
    logging_steps: int = 1
    optim: str = "adamw_8bit"  # éœ€ bitsandbytes
    weight_decay: float = 0.01
    lr_scheduler_type: str = "linear"
    seed: int = 3407
    report_to: str = "wandb"  # æ”¹ä¸ºé»˜è®¤åŒæ­¥åˆ° wandb

    # W&B ç›¸å…³
    use_wandb: bool = True
    wandb_project: Optional[str] = "unsloth-sft"
    wandb_entity: Optional[str] = None
    wandb_run_name: Optional[str] = None
    wandb_tags: List[str] = field(default_factory=lambda: ["unsloth", "sft", "qwen3"])
    wandb_dir: Optional[str] = None
    wandb_group: Optional[str] = None
    wandb_job_type: str = "train"
    wandb_mode: Optional[str] = None  # None/"online"/"offline"/"disabled"
    wandb_notes: Optional[str] = None
    wandb_log_model: bool | str = False  # True/False/"checkpoint"/"end"

    # è¾“å‡º/ä¿å­˜
    output_dir: str = "./outputs/qwen3_4b_lora"
    save_steps: Optional[int] = 2  # å®šæœŸä¿å­˜
    save_total_limit: Optional[int] = None
    logging_dir: Optional[str] = None  # è‡ªå®šä¹‰æ—¥å¿—ç›®å½•


# ==============================
# æ—¥å¿—å·¥å…·
# ==============================

def setup_logging(output_dir: str, logging_dir: Optional[str] = None) -> logging.Logger:
    """é…ç½®æ—¥å¿—ï¼šæ§åˆ¶å° + æ–‡ä»¶ã€‚"""
    os.makedirs(output_dir, exist_ok=True)
    log_dir = logging_dir or os.path.join(output_dir, "logs")
    os.makedirs(log_dir, exist_ok=True)

    log_file = os.path.join(log_dir, time.strftime("train_%Y%m%d_%H%M%S.log"))

    logger = logging.getLogger("unsloth_sft")
    logger.setLevel(logging.INFO)
    logger.handlers.clear()

    # æ§åˆ¶å°è¾“å‡º
    ch = logging.StreamHandler(sys.stdout)
    ch.setLevel(logging.INFO)
    ch_formatter = logging.Formatter("[%(asctime)s] %(levelname)s - %(message)s", "%H:%M:%S")
    ch.setFormatter(ch_formatter)

    # æ–‡ä»¶è¾“å‡º
    fh = logging.FileHandler(log_file, encoding="utf-8")
    fh.setLevel(logging.INFO)
    fh_formatter = logging.Formatter(
        "[%(asctime)s] %(levelname)s %(name)s:%(lineno)d - %(message)s",
        "%Y-%m-%d %H:%M:%S",
    )
    fh.setFormatter(fh_formatter)

    logger.addHandler(ch)
    logger.addHandler(fh)

    logger.info(f"æ—¥å¿—å†™å…¥: {log_file}")
    return logger


# ==============================
# W&B å·¥å…·
# ==============================

def setup_wandb(cfg: TrainConfig, logger: logging.Logger):
    """åˆå§‹åŒ– wandbã€‚è¿”å› run å¯¹è±¡æˆ– Noneã€‚"""
    if not cfg.use_wandb:
        logger.info("å·²ç¦ç”¨ W&Bã€‚")
        return None
    if not WANDB_AVAILABLE:
        logger.warning("æœªæ£€æµ‹åˆ° wandb åŒ…ï¼Œå·²è·³è¿‡ W&B é›†æˆã€‚pip install wandb")
        return None

    # å…è®¸é€šè¿‡ç¯å¢ƒå˜é‡è¦†ç›–
    project = cfg.wandb_project or os.getenv("WANDB_PROJECT") or "unsloth-sft"
    entity = cfg.wandb_entity or os.getenv("WANDB_ENTITY")

    run = wandb.init(
        project=project,
        entity=entity,
        name=cfg.wandb_run_name,
        group=cfg.wandb_group,
        job_type=cfg.wandb_job_type,
        dir=cfg.wandb_dir,
        tags=cfg.wandb_tags or None,
        notes=cfg.wandb_notes,
        mode=cfg.wandb_mode,  # None ä½¿ç”¨é»˜è®¤
        config=asdict(cfg),
        reinit=False,
    )

    # è®°å½•å½“å‰è„šæœ¬ä»£ç ï¼Œä¾¿äºå¤ç°
    try:
        wandb.run.log_code(root=str(Path(__file__).resolve().parent))
    except Exception:
        pass

    logger.info(f"å·²è¿æ¥ W&Bï¼šproject={project}, run={run.name}")
    return run


def wandb_log_metrics(metrics: dict):
    if WANDB_AVAILABLE and wandb.run is not None:
        wandb.log(metrics)


def wandb_on_error(e: Exception, logger: logging.Logger):
    if WANDB_AVAILABLE and wandb.run is not None:
        try:
            # å°è¯•å‘å‡ºå‘Šè­¦ï¼ˆä¼ä¸š/å›¢é˜Ÿç‰ˆæ›´å‹å¥½ï¼‰ï¼Œå¤±è´¥åˆ™é™çº§ä¸ºæ™®é€šæ—¥å¿—
            try:
                wandb.alert(title="Training crashed", text=str(e), level=wandb.AlertLevel.ERROR)
            except Exception:
                pass
            wandb.run.summary["status"] = "failed"
            wandb.log({"error/exception": str(e)})
            wandb.finish(exit_code=1)
            logger.error("å·²å°†å¼‚å¸¸ä¸ŠæŠ¥è‡³ W&Bï¼ˆstatus=failedï¼‰")
        except Exception as ee:
            logger.error(f"ä¸ŠæŠ¥ W&B å¼‚å¸¸å¤±è´¥: {ee}")


def wandb_on_success(extra_summary: dict | None = None, exit_code: int = 0):
    if WANDB_AVAILABLE and wandb.run is not None:
        if extra_summary:
            for k, v in extra_summary.items():
                wandb.run.summary[k] = v
        wandb.run.summary["status"] = "success"
        wandb.finish(exit_code=exit_code)


# ==============================
# é€šç”¨å·¥å…·
# ==============================

def set_seed(seed: int, logger: logging.Logger | None = None) -> None:
    """è®¾ç½®éšæœºç§å­ï¼Œä¿è¯å¯å¤ç°ã€‚"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = False  # æ›´å¿«è®­ç»ƒï¼›å¦‚éœ€å®Œå…¨ç¡®å®šæ€§å¯è®¾ True
    torch.backends.cudnn.benchmark = True
    if logger:
        logger.info(f"éšæœºç§å­å·²è®¾ç½®: {seed}")


def log_env_info(logger: logging.Logger) -> None:
    """è®°å½•ç¯å¢ƒä¸ GPU ä¿¡æ¯ã€‚"""
    logger.info(f"PyTorch: {torch.__version__}")
    if torch.cuda.is_available():
        gpu = torch.cuda.get_device_properties(0)
        max_mem = round(gpu.total_memory / 1024 / 1024 / 1024, 3)
        start_reserved = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)
        logger.info(f"GPU: {gpu.name}  æ˜¾å­˜ä¸Šé™: {max_mem} GB  å¯åŠ¨ä¿ç•™: {start_reserved} GB")
    else:
        logger.warning("æœªæ£€æµ‹åˆ°å¯ç”¨ CUDAï¼Œè®­ç»ƒå°†ä½¿ç”¨ CPUï¼ˆå¯èƒ½éå¸¸æ…¢ï¼‰")


# ==============================
# æ„å»ºæ¨¡å‹ & Tokenizer
# ==============================

def build_model_and_tokenizer(cfg: TrainConfig, logger: logging.Logger):
    """åŠ è½½åŸºç¡€æ¨¡å‹ï¼Œå¹¶æ³¨å…¥ LoRA é€‚é…å™¨ï¼›åº”ç”¨ Chat æ¨¡æ¿ã€‚"""
    if cfg.load_in_4bit and cfg.load_in_8bit:
        logger.warning("4bit ä¸ 8bit ä»…èƒ½äºŒé€‰ä¸€ï¼Œå·²ä¼˜å…ˆä½¿ç”¨ 4bitï¼Œå¹¶å…³é—­ 8bitï¼")
        cfg.load_in_8bit = False

    logger.info("å¼€å§‹åŠ è½½åŸºç¡€æ¨¡å‹â€¦")
    model, tokenizer = FastModel.from_pretrained(
        model_name=cfg.model_name,
        max_seq_length=cfg.max_seq_length,
        load_in_4bit=cfg.load_in_4bit,
        load_in_8bit=cfg.load_in_8bit,
        full_finetuning=cfg.full_finetuning,
        token=cfg.hf_token,
    )
    logger.info("åŸºç¡€æ¨¡å‹åŠ è½½å®Œæˆã€‚")

    logger.info("æ³¨å…¥ LoRA é€‚é…å™¨â€¦")
    model = FastModel.get_peft_model(
        model,
        r=cfg.lora_r,
        target_modules=cfg.target_modules,
        lora_alpha=cfg.lora_alpha,
        lora_dropout=cfg.lora_dropout,
        bias=cfg.bias,
        use_gradient_checkpointing=cfg.use_gradient_checkpointing,
        random_state=cfg.seed,
        use_rslora=cfg.use_rslora,
        loftq_config=cfg.loftq_config,
    )
    logger.info("LoRA é€‚é…å™¨æ³¨å…¥å®Œæˆã€‚")

    logger.info(f"åº”ç”¨ Chat æ¨¡æ¿: {cfg.chat_template}")
    tokenizer = get_chat_template(tokenizer, chat_template=cfg.chat_template)
    return model, tokenizer


# ==============================
# æ•°æ®é›†å‡†å¤‡
# ==============================

def load_and_prepare_dataset(cfg: TrainConfig, tokenizer, logger: logging.Logger) -> Dataset:
    """åŠ è½½æ•°æ®é›†ï¼Œç»Ÿä¸€å¯¹è¯æ ¼å¼ï¼Œå¹¶è½¬æˆè®­ç»ƒå­—æ®µ textã€‚"""
    logger.info(f"åŠ è½½æ•°æ®é›†: {cfg.dataset_name} [{cfg.dataset_split}] â€¦")
    dataset = load_dataset(cfg.dataset_name, split=cfg.dataset_split)

    logger.info("æ ‡å‡†åŒ–ä¸ºé€šç”¨ conversations æ ¼å¼â€¦")
    dataset = standardize_data_formats(dataset)

    def formatting_prompts_func(examples):
        convos = examples["conversations"]
        # å°†å¤šè½®å¯¹è¯ç”¨ ChatTemplate æ¸²æŸ“ä¸ºçº¯æ–‡æœ¬ï¼ˆå«ç‰¹æ®Šæ ‡è®°ï¼‰
        texts = [
            tokenizer.apply_chat_template(
                convo, tokenize=False, add_generation_prompt=False
            )
            for convo in convos
        ]
        return {cfg.dataset_text_field: texts}

    logger.info("æ ¹æ® Chat æ¨¡æ¿æ¸²æŸ“æ–‡æœ¬å­—æ®µâ€¦")
    dataset = dataset.map(formatting_prompts_func, batched=True, desc="apply_chat_template")

    # æ‰“å°/è®°å½•ä¸€ä¸ªæ ·æœ¬ä¾¿äºæ£€æŸ¥
    try:
        sample_txt = dataset[0][cfg.dataset_text_field]
        logger.info(f"æ ·æœ¬é¢„è§ˆ: {sample_txt}")
    except Exception as e:
        logger.warning(f"æ ·æœ¬é¢„è§ˆå¤±è´¥: {e}")

    return dataset


# ==============================
# æ„å»º Trainer
# ==============================

def build_trainer(model, tokenizer, dataset: Dataset, cfg: TrainConfig, logger: logging.Logger) -> SFTTrainer:
    """åˆ›å»º SFTTrainerï¼Œå¹¶è®¾ç½®ä»…è®­ç»ƒå›ç­”æ®µè½ã€‚"""
    logger.info("åˆ›å»º SFTTrainerâ€¦")

    training_args = SFTConfig(
        output_dir=cfg.output_dir,
        dataset_text_field=cfg.dataset_text_field,
        per_device_train_batch_size=cfg.per_device_train_batch_size,
        gradient_accumulation_steps=cfg.gradient_accumulation_steps,
        warmup_steps=cfg.warmup_steps,
        max_steps=cfg.max_steps,
        # ä¹Ÿå¯é€‰æ‹©è®¾å®š num_train_epochsï¼ˆä¸¤è€…ä¸è¦åŒæ—¶å¼ºåˆ¶ï¼‰
        num_train_epochs=cfg.num_train_epochs,
        learning_rate=cfg.learning_rate,
        logging_steps=cfg.logging_steps,
        optim=cfg.optim,
        weight_decay=cfg.weight_decay,
        lr_scheduler_type=cfg.lr_scheduler_type,
        seed=cfg.seed,
        report_to=cfg.report_to,  # â†’ "wandb" æ—¶å°†è‡ªåŠ¨ä¸ŠæŠ¥
        save_steps=cfg.save_steps,
        save_total_limit=cfg.save_total_limit,
    )

    trainer = SFTTrainer(
        model=model,
        tokenizer=tokenizer,
        train_dataset=dataset,
        eval_dataset=None,
        args=training_args,
    )

    logger.info("åˆ‡æ¢ä¸ºä»…å­¦ä¹  assistant å“åº”ï¼ˆå¿½ç•¥ user æŒ‡ä»¤éƒ¨åˆ†çš„ lossï¼‰â€¦")
    trainer = train_on_responses_only(
        trainer,
        instruction_part=cfg.instruction_part,
        response_part=cfg.response_part,
    )

    # è®°å½•ä¸€ä¸ªç¼–ç åçš„æ ·æœ¬ï¼Œç¡®è®¤ mask æ˜¯å¦åˆç†ï¼ˆåªåšæ—¥å¿—é¢„è§ˆï¼Œä¸å½±å“è®­ç»ƒï¼‰
    try:
        decoded = tokenizer.decode(trainer.train_dataset[0]["input_ids"][:256])
        logger.info(f"ç¼–ç æ ·æœ¬é¢„è§ˆ: {decoded}")
    except Exception as e:
        logger.warning(f"ç¼–ç æ ·æœ¬é¢„è§ˆå¤±è´¥: {e}")

    # å°†æ¢¯åº¦/æƒé‡å˜åŒ–å‘é€åˆ° W&Bï¼ˆå¯é€‰ï¼‰
    if WANDB_AVAILABLE and wandb.run is not None and cfg.use_wandb:
        try:
            wandb.watch(trainer.model, log="gradients", log_freq=max(1, cfg.logging_steps))
        except Exception:
            pass

    return trainer


# ==============================
# è®­ç»ƒä¸åº¦é‡
# ==============================

def train_and_report(trainer: SFTTrainer, logger: logging.Logger):
    """å¯åŠ¨è®­ç»ƒå¹¶è®°å½•æ˜¾å­˜ä¸è€—æ—¶ã€‚"""
    gpu_stats = torch.cuda.get_device_properties(0) if torch.cuda.is_available() else None
    start_reserved = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3) if torch.cuda.is_available() else 0.0
    max_mem = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3) if gpu_stats else 0.0

    if gpu_stats:
        logger.info(f"GPU = {gpu_stats.name}. Max memory = {max_mem} GB.")
        logger.info(f"å¯åŠ¨æ—¶ä¿ç•™æ˜¾å­˜ = {start_reserved} GB.")
        wandb_log_metrics({
            "env/gpu_name": gpu_stats.name,
            "env/gpu_mem_gb": max_mem,
            "memory/start_reserved_gb": start_reserved,
        })

    logger.info("å¼€å§‹è®­ç»ƒâ€¦")
    trainer_stats = trainer.train()
    logger.info("è®­ç»ƒå®Œæˆã€‚")

    used_reserved = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3) if torch.cuda.is_available() else 0.0
    used_for_lora = round(used_reserved - start_reserved, 3)
    used_pct = round(used_reserved / max_mem * 100, 3) if max_mem else 0.0
    lora_pct = round(used_for_lora / max_mem * 100, 3) if max_mem else 0.0

    rt = float(trainer_stats.metrics.get('train_runtime', 0.0))
    logger.info(f"è®­ç»ƒè€—æ—¶ {rt:.2f} ç§’ï¼ˆçº¦ {rt/60:.2f} åˆ†é’Ÿï¼‰ã€‚")
    logger.info(f"å³°å€¼ä¿ç•™æ˜¾å­˜ = {used_reserved} GBï¼›å…¶ä¸­è®­ç»ƒå¢é‡ = {used_for_lora} GBã€‚")
    if max_mem:
        logger.info(f"æ˜¾å­˜å ç”¨å³°å€¼å æ¯” = {used_pct}%ï¼›è®­ç»ƒå¢é‡å æ¯” = {lora_pct}%ã€‚")

    # è‡ªå®šä¹‰æŒ‡æ ‡ä¸ŠæŠ¥åˆ° W&B
    wandb_log_metrics({
        "memory/peak_reserved_gb": used_reserved,
        "memory/peak_reserved_pct": used_pct,
        "memory/lora_delta_gb": used_for_lora,
        "memory/lora_delta_pct": lora_pct,
        "time/train_runtime_sec": rt,
        "trainer/global_step": getattr(trainer.state, "global_step", 0),
    })

    return trainer_stats


# ==============================
# ä¿å­˜æ¨¡å‹
# ==============================

def save_model(trainer: SFTTrainer, tokenizer, output_dir: str, logger: logging.Logger, *, log_artifact: bool | str = False) -> None:
    """å°è¯•ä½¿ç”¨TRL/Transformers ä¿å­˜ï¼Œå¹¶å¯é€‰ä¸Šä¼ åˆ° W&B Artifactã€‚"""
    os.makedirs(output_dir, exist_ok=True)
    logger.info(f"Unsloth ä¿å­˜æ¨¡å‹ Trainer.save_model")
    try:
        trainer.save_model(output_dir)
        tokenizer.save_pretrained(output_dir)
        logger.info(f"æ¨¡å‹å·²ä¿å­˜è‡³: {output_dir}")

        # å¯é€‰ï¼šä¸Šä¼ æ¨¡å‹ç›®å½•ä¸º artifact
        if log_artifact and WANDB_AVAILABLE and wandb.run is not None:
            art = wandb.Artifact(
                name=f"{Path(output_dir).name}-{wandb.run.id}",
                type="model",
                metadata={"framework": "transformers", "task": "sft"},
            )
            art.add_dir(output_dir)
            aliases = ["latest"]
            if isinstance(log_artifact, str):
                aliases.append(log_artifact)
            wandb.log_artifact(art, aliases=aliases)
            logger.info("æ¨¡å‹å·²ä½œä¸º W&B Artifact ä¸Šä¼ ã€‚")
    except Exception as ee:
        logger.error(f"ä¿å­˜å¤±è´¥: {ee}")
        raise

# ==============================
# ä¸»æµç¨‹
# ==============================

def parse_bool_flag(parser: argparse.ArgumentParser, true_flag: str, false_flag: str, default: bool):
    """åŒæ—¶æ”¯æŒ --flag / --no_flag çš„å¸ƒå°”å¼€å…³ã€‚è¿”å›å­˜å…¥ args çš„ç›®æ ‡åã€‚"""
    dest = true_flag.replace("--", "").replace("-", "_")
    group = parser.add_mutually_exclusive_group()
    group.add_argument(true_flag, dest=dest, action="store_true")
    group.add_argument(false_flag, dest=dest, action="store_false")
    parser.set_defaults(**{dest: default})
    return dest


def parse_args() -> TrainConfig:
    parser = argparse.ArgumentParser(description="Unsloth SFT è®­ç»ƒè„šæœ¬ï¼ˆé€šç”¨æ¨¡æ¿ + W&Bï¼‰")

    # ä»…åˆ—å‡ºå¸¸ç”¨é¡¹ï¼›å…¶ä½™è¯·ç›´æ¥åœ¨ dataclass é»˜è®¤å€¼ä¸­æ”¹
    parser.add_argument("--model_name", type=str, default=TrainConfig.model_name)
    parser.add_argument("--max_seq_length", type=int, default=TrainConfig.max_seq_length)
    parse_bool_flag(parser, "--load_in_4bit", "--no_load_in_4bit", default=TrainConfig.load_in_4bit)
    parse_bool_flag(parser, "--load_in_8bit", "--no_load_in_8bit", default=TrainConfig.load_in_8bit)
    parse_bool_flag(parser, "--full_finetuning", "--no_full_finetuning", default=TrainConfig.full_finetuning)
    parser.add_argument("--hf_token", type=str, default=None)

    parser.add_argument("--lora_r", type=int, default=TrainConfig.lora_r)
    parser.add_argument("--lora_alpha", type=int, default=TrainConfig.lora_alpha)
    parser.add_argument("--lora_dropout", type=float, default=TrainConfig.lora_dropout)

    parser.add_argument("--chat_template", type=str, default=TrainConfig.chat_template)
    parser.add_argument("--dataset_name", type=str, default=TrainConfig.dataset_name)
    parser.add_argument("--dataset_split", type=str, default=TrainConfig.dataset_split)

    parser.add_argument("--per_device_train_batch_size", type=int, default=TrainConfig.per_device_train_batch_size)
    parser.add_argument("--gradient_accumulation_steps", type=int, default=TrainConfig.gradient_accumulation_steps)
    parser.add_argument("--warmup_steps", type=int, default=TrainConfig.warmup_steps)
    parser.add_argument("--max_steps", type=int, default=TrainConfig.max_steps)
    parser.add_argument("--num_train_epochs", type=float, default=TrainConfig.num_train_epochs)
    parser.add_argument("--learning_rate", type=float, default=TrainConfig.learning_rate)
    parser.add_argument("--logging_steps", type=int, default=TrainConfig.logging_steps)
    parser.add_argument("--optim", type=str, default=TrainConfig.optim)
    parser.add_argument("--weight_decay", type=float, default=TrainConfig.weight_decay)
    parser.add_argument("--lr_scheduler_type", type=str, default=TrainConfig.lr_scheduler_type)
    parser.add_argument("--seed", type=int, default=TrainConfig.seed)
    parser.add_argument("--report_to", type=str, default=TrainConfig.report_to)

    parser.add_argument("--output_dir", type=str, default=TrainConfig.output_dir)
    parser.add_argument("--save_steps", type=int, default=TrainConfig.save_steps)
    parser.add_argument("--save_total_limit", type=int, default=None)

    # W&B ç›¸å…³å¼€å…³
    parse_bool_flag(parser, "--use_wandb", "--no_use_wandb", default=TrainConfig.use_wandb)
    parser.add_argument("--wandb_project", type=str, default=TrainConfig.wandb_project)
    parser.add_argument("--wandb_entity", type=str, default=TrainConfig.wandb_entity)
    parser.add_argument("--wandb_run_name", type=str, default=TrainConfig.wandb_run_name)
    parser.add_argument("--wandb_group", type=str, default=TrainConfig.wandb_group)
    parser.add_argument("--wandb_job_type", type=str, default=TrainConfig.wandb_job_type)
    parser.add_argument("--wandb_mode", type=str, default=TrainConfig.wandb_mode)
    parser.add_argument("--wandb_dir", type=str, default=TrainConfig.wandb_dir)
    parser.add_argument("--wandb_notes", type=str, default=TrainConfig.wandb_notes)
    parser.add_argument("--wandb_log_model", type=str, default=str(TrainConfig.wandb_log_model))
    parser.add_argument("--wandb_tags", type=str, nargs="*", default=None, help="ç©ºæ ¼åˆ†éš”çš„ tag åˆ—è¡¨")

    args = parser.parse_args()

    # è§£æ wandb_log_model ä¸º bool/str
    wandb_log_model: bool | str
    if str(args.wandb_log_model).lower() in {"true", "1", "yes"}:
        wandb_log_model = True
    elif str(args.wandb_log_model).lower() in {"false", "0", "no"}:
        wandb_log_model = False
    else:
        wandb_log_model = str(args.wandb_log_model)

    cfg = TrainConfig(
        model_name=args.model_name,
        max_seq_length=args.max_seq_length,
        load_in_4bit=args.load_in_4bit,
        load_in_8bit=args.load_in_8bit,
        full_finetuning=args.full_finetuning,
        hf_token=args.hf_token,
        lora_r=args.lora_r,
        lora_alpha=args.lora_alpha,
        lora_dropout=args.lora_dropout,
        chat_template=args.chat_template,
        dataset_name=args.dataset_name,
        dataset_split=args.dataset_split,
        per_device_train_batch_size=args.per_device_train_batch_size,
        gradient_accumulation_steps=args.gradient_accumulation_steps,
        warmup_steps=args.warmup_steps,
        max_steps=args.max_steps,
        num_train_epochs=args.num_train_epochs,
        learning_rate=args.learning_rate,
        logging_steps=args.logging_steps,
        optim=args.optim,
        weight_decay=args.weight_decay,
        lr_scheduler_type=args.lr_scheduler_type,
        seed=args.seed,
        report_to=args.report_to,
        output_dir=args.output_dir,
        save_steps=args.save_steps,
        save_total_limit=args.save_total_limit,
        use_wandb=args.use_wandb,
        wandb_project=args.wandb_project,
        wandb_entity=args.wandb_entity,
        wandb_run_name=args.wandb_run_name,
        wandb_tags=args.wandb_tags or TrainConfig().wandb_tags,
        wandb_dir=args.wandb_dir,
        wandb_group=args.wandb_group,
        wandb_job_type=args.wandb_job_type,
        wandb_mode=args.wandb_mode,
        wandb_notes=args.wandb_notes,
        wandb_log_model=wandb_log_model,
    )
    return cfg


def main(cfg: TrainConfig | None = None) -> None:
    cfg = cfg or TrainConfig()

    # æ—¥å¿—
    logger = setup_logging(cfg.output_dir, cfg.logging_dir)

    # æ‰“å°é…ç½®
    logger.info("===== è®­ç»ƒé…ç½® =====")
    logger.info(json.dumps(asdict(cfg), ensure_ascii=False, indent=2))

    # éšæœºç§å­ & ç¯å¢ƒä¿¡æ¯
    set_seed(cfg.seed, logger)
    log_env_info(logger)

    # åˆå§‹åŒ– W&Bï¼ˆå°½æ—©å»ºç«‹ runï¼Œè®°å½•ç¯å¢ƒ/é…ç½®ï¼‰
    run = setup_wandb(cfg, logger)

    try:
        # æ„å»ºæ¨¡å‹ä¸ tokenizer
        model, tokenizer = build_model_and_tokenizer(cfg, logger)

        # å‡†å¤‡æ•°æ®é›†
        dataset = load_and_prepare_dataset(cfg, tokenizer, logger)

        # æ„å»º Trainer
        trainer = build_trainer(model, tokenizer, dataset, cfg, logger)

        # è®­ç»ƒå¹¶æŠ¥å‘Š
        stats = train_and_report(trainer, logger)

        # ä¿å­˜æ¨¡å‹ & å¯é€‰ä¸Šä¼  artifactï¼ˆåˆ«åï¼šfinalï¼‰
        save_model(
            trainer,
            tokenizer,
            cfg.output_dir,
            logger,
            log_artifact=(cfg.wandb_log_model if cfg.wandb_log_model else False),
        )

        # æˆåŠŸæ”¶å°¾
        extra = {"metrics/train_runtime_sec": float(stats.metrics.get("train_runtime", 0.0))}
        wandb_on_success(extra_summary=extra, exit_code=0)

    except Exception as e:
        logger.exception(f"è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        wandb_on_error(e, logger)
        raise

    logger.info("ğŸ‰ å…¨æµç¨‹ç»“æŸã€‚")


if __name__ == "__main__":
    main(parse_args())
