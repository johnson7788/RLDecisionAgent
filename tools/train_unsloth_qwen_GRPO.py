#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
https://github.com/unslothai/notebooks/blob/main/nb/Kaggle-Qwen3_(4B)-GRPO.ipynb
å°†åŸºäº Unsloth çš„ GRPO è®­ç»ƒæµç¨‹ä» Jupyter ç¬”è®°æœ¬è½¬æ¢ä¸ºçº¯ Python è„šæœ¬ã€‚
ä¸»è¦æ­¥éª¤ï¼š
1) å¯é€‰ï¼šè‡ªåŠ¨å®‰è£…ä¾èµ–ï¼ˆæ ¹æ®æ˜¯å¦æ˜¯ T4 æ˜¾å¡é€‰æ‹© vLLM / Triton ç‰ˆæœ¬ï¼‰
2) åŠ è½½ Qwen3-4B-Baseï¼Œå¹¶åº”ç”¨ LoRAï¼ˆ16bitï¼‰
3) è®¾ç½®è‡ªå®šä¹‰â€œæ€è€ƒ + ç­”æ¡ˆâ€èŠå¤©æ¨¡æ¿ï¼ˆ<start_working_out> / <end_working_out> / <SOLUTION>ï¼‰
4) ä½¿ç”¨å°‘é‡é«˜è´¨é‡æ ·æœ¬åšé¢„å¾®è°ƒï¼ˆSFTï¼‰ï¼Œè®©æ¨¡å‹å­¦ä¼šè¾“å‡ºæ ¼å¼
5) å‡†å¤‡ Open R1 æ•°å­¦æ•°æ®é›†ä¸å¥–åŠ±å‡½æ•°ï¼ˆæ ¼å¼åŒ¹é… + æ•°å€¼æ¯”å¯¹ï¼‰
6) é…ç½®å¹¶è¿è¡Œ GRPO è®­ç»ƒ
7) æ¨ç†ï¼šå¯¹æ¯”æœªåŠ è½½ LoRA ä¸åŠ è½½ LoRA çš„è¾“å‡º
8) ä¿å­˜ LoRAï¼›ï¼ˆå¯é€‰ï¼‰æ¼”ç¤ºå¤šç§ä¿å­˜/é‡åŒ–æ–¹å¼çš„ä»£ç å¼€å…³

ä½¿ç”¨æ–¹æ³•ï¼š
    python grpo_qwen3_4b.py \
        --run_install 1 --run_sft 1 --run_grpo 1 --run_infer 1 \
        --max_steps 100 --max_seq_length 2048 --lora_rank 32
"""

import os
import re
import gc
import sys
import json
import math
import time
import subprocess
import argparse
from typing import List, Tuple

# -----------------------------
# å‘½ä»¤è¡Œå‚æ•°
# -----------------------------
parser = argparse.ArgumentParser(description="åŸºäº Unsloth çš„ GRPO è®­ç»ƒè„šæœ¬ï¼ˆä¸­æ–‡æ³¨é‡Šç‰ˆï¼‰")
parser.add_argument("--run_install", type=int, default=1, help="æ˜¯å¦è‡ªåŠ¨å®‰è£…ä¾èµ–ï¼ˆ1=æ˜¯ï¼Œ0=å¦ï¼‰")
parser.add_argument("--run_sft", type=int, default=1, help="æ˜¯å¦è¿è¡Œé¢„å¾®è°ƒé˜¶æ®µï¼ˆSFTï¼‰ï¼ˆ1=æ˜¯ï¼Œ0=å¦ï¼‰")
parser.add_argument("--run_grpo", type=int, default=1, help="æ˜¯å¦è¿è¡Œ GRPO è®­ç»ƒï¼ˆ1=æ˜¯ï¼Œ0=å¦ï¼‰")
parser.add_argument("--run_infer", type=int, default=1, help="æ˜¯å¦åœ¨è®­ç»ƒåè¿›è¡Œæ¨ç†ï¼ˆ1=æ˜¯ï¼Œ0=å¦ï¼‰")
parser.add_argument("--max_seq_length", type=int, default=2048, help="æœ€å¤§åºåˆ—é•¿åº¦ï¼ˆå¯å¢å¤§ä»¥å®¹çº³æ›´é•¿æ¨ç†è½¨è¿¹ï¼‰")
parser.add_argument("--lora_rank", type=int, default=32, help="LoRA Rankï¼Œè¶Šå¤§è¶Šâ€œèªæ˜â€ä½†è¶Šæ…¢")
parser.add_argument("--max_steps", type=int, default=100, help="GRPO è®­ç»ƒçš„æœ€å¤§æ­¥æ•°ï¼ˆä¸ num_train_epochs äºŒé€‰ä¸€ï¼‰")
parser.add_argument("--save_dir", type=str, default="outputs", help="è®­ç»ƒè¾“å‡ºç›®å½•")
parser.add_argument("--seed", type=int, default=3407, help="éšæœºç§å­")
parser.add_argument("--model_name", type=str, default="unsloth/Qwen3-4B-Base", help="åŸºç¡€æ¨¡å‹åç§°")
args = parser.parse_args()

# -----------------------------
# å®ç”¨å‡½æ•°ï¼šå®‰å…¨æ‰§è¡Œ pip å®‰è£…
# -----------------------------
def pip_install(pkgs: List[str]):
    """ç”¨ pip å®‰è£…ä¾èµ–ã€‚"""
    print("ğŸ“¦ æ­£åœ¨å®‰è£…ä¾èµ–ï¼š", " ".join(pkgs))
    cmd = [sys.executable, "-m", "pip", "install", "--upgrade"] + pkgs
    subprocess.check_call(cmd)

def detect_gpu_is_t4() -> bool:
    """æ£€æµ‹æ˜¯å¦ä¸º Tesla T4ã€‚"""
    try:
        out = subprocess.check_output(["nvidia-smi"], stderr=subprocess.STDOUT).decode("utf-8", "ignore")
        return "Tesla T4" in out
    except Exception:
        return False

# -----------------------------
# ç¬¬ 0 æ­¥ï¼šå¯é€‰å®‰è£…ä¾èµ–
# -----------------------------
if args.run_install:
    print("ğŸ”§ [å®‰è£…] æ­£åœ¨æ£€æµ‹ç¯å¢ƒå¹¶å®‰è£…ä¾èµ–ï¼ˆå¦‚å·²æ»¡è¶³å¯è·³è¿‡ï¼‰...")
    is_t4 = detect_gpu_is_t4()
    # å¯¹ T4 æœºå‹å›ºå®š vLLM / Triton ç‰ˆæœ¬æ›´ç¨³ï¼›å…¶ä»–ç¯å¢ƒä¸å›ºå®š
    get_vllm    = "vllm==0.10.1" if is_t4 else "vllm"
    get_triton  = "triton==3.2.0" if is_t4 else "triton"
    # datasets é™åˆ¶åœ¨ 3.xï¼Œé¿å…æœªæ¥ 4.x å¯èƒ½çš„ç ´åæ€§å˜æ›´
    deps_main = [
        "unsloth",
        get_vllm,
        "torchvision",
        "bitsandbytes",
        "xformers",
        get_triton,
        'huggingface_hub>=0.34.0',
        'datasets>=3.4.1,<4.0.0',
        'transformers==4.55.4',
        'safetensors',
        'pandas',
        'numpy',
        'accelerate>=0.30.0',
        'trl>=0.10.0'  # ç¡®ä¿åŒ…å« GRPO/SFT æ‰€éœ€
    ]
    pip_install(deps_main)
    print("âœ… [å®‰è£…] ä¾èµ–å®‰è£…å®Œæˆã€‚\n")

# -----------------------------
# ç¬¬ 1 æ­¥ï¼šå¯¼å…¥åº“ & åŸºæœ¬é…ç½®
# -----------------------------
print("ğŸ§  æ­£åœ¨å¯¼å…¥è®­ç»ƒæ‰€éœ€åº“...")
import torch
from unsloth import FastLanguageModel
from datasets import load_dataset, Dataset
import pandas as pd
import numpy as np
from trl import SFTTrainer, SFTConfig, GRPOConfig, GRPOTrainer
from vllm import SamplingParams
from transformers import TextStreamer
print("âœ… åº“å¯¼å…¥å®Œæˆã€‚")

# å›ºå®šéšæœºç§å­
def set_seed(seed: int):
    import random
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)

set_seed(args.seed)

# -----------------------------
# ç¬¬ 2 æ­¥ï¼šåŠ è½½æ¨¡å‹å¹¶æ„å»º LoRA
# -----------------------------
print("ğŸš€ æ­£åœ¨åŠ è½½åŸºç¡€æ¨¡å‹ï¼š{} ...".format(args.model_name))
max_seq_length = args.max_seq_length
lora_rank      = args.lora_rank

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = args.model_name,
    max_seq_length = max_seq_length,
    load_in_4bit = False,            # LoRA ç”¨ 16-bit
    fast_inference = True,           # å¯ç”¨ vLLM æ¨ç†åŠ é€Ÿ
    max_lora_rank = lora_rank,
    gpu_memory_utilization = 0.7,    # æ˜¾å­˜ç´§å¼ å¯ä¸‹è°ƒ
)

model = FastLanguageModel.get_peft_model(
    model,
    r = lora_rank,
    target_modules = [
        "q_proj", "k_proj", "v_proj", "o_proj",
        "gate_proj", "up_proj", "down_proj",
    ],
    lora_alpha = lora_rank * 2,      # *2 å¯åŠ é€Ÿæ”¶æ•›
    use_gradient_checkpointing = "unsloth",
    random_state = args.seed,
)
print("âœ… æ¨¡å‹ä¸ LoRA åˆå§‹åŒ–å®Œæˆã€‚\n")

# -----------------------------
# ç¬¬ 3 æ­¥ï¼šè®¾ç½®è‡ªå®šä¹‰èŠå¤©æ¨¡æ¿ï¼ˆGRPO æ‰€éœ€æ ¼å¼ï¼‰
# -----------------------------
print("ğŸ§© æ­£åœ¨è®¾ç½®è‡ªå®šä¹‰èŠå¤©æ¨¡æ¿ï¼ˆå«â€œæ€è€ƒ/ç­”æ¡ˆâ€æ ‡è®°ï¼‰...")
reasoning_start = "<start_working_out>"
reasoning_end   = "<end_working_out>"
solution_start  = "<SOLUTION>"
solution_end    = "</SOLUTION>"

system_prompt = (
    f"You are given a problem.\n"
    f"Think about the problem and provide your working out.\n"
    f"Place it between {reasoning_start} and {reasoning_end}.\n"
    f"Then, provide your solution between {solution_start}{solution_end}"
)

# Jinja æ¨¡æ¿ï¼šå°† system + å†å²å¯¹è¯ + ç”Ÿæˆæç¤º ä¸²æ¥èµ·æ¥
chat_template = (
    "{% if messages[0]['role'] == 'system' %}"
        "{{ messages[0]['content'] + eos_token }}"
        "{% set loop_messages = messages[1:] %}"
    "{% else %}"
        "{{ '" + system_prompt + "' + eos_token }}"
        "{% set loop_messages = messages %}"
    "{% endif %}"
    "{% for message in loop_messages %}"
        "{% if message['role'] == 'user' %}"
            "{{ message['content'] }}"
        "{% elif message['role'] == 'assistant' %}"
            "{{ message['content'] + eos_token }}"
        "{% endif %}"
    "{% endfor %}"
    "{% if add_generation_prompt %}{{ '" + reasoning_start + "' }}"
    "{% endif %}"
)
tokenizer.chat_template = chat_template
print("âœ… èŠå¤©æ¨¡æ¿å·²è®¾ç½®ã€‚\n")

# -----------------------------
# ç¬¬ 4 æ­¥ï¼šé¢„å¾®è°ƒï¼ˆSFTï¼‰ä»¥å­¦ä¹ è¾“å‡ºæ ¼å¼
# -----------------------------
if args.run_sft:
    print("ğŸ“š [SFT] æ­£åœ¨åŠ è½½ç”¨äºâ€œå­¦æ ¼å¼â€çš„å°æ ·æœ¬æ•°æ®é›†ï¼ˆunsloth/OpenMathReasoning-miniï¼‰...")
    df = load_dataset("unsloth/OpenMathReasoning-mini", split="cot").to_pandas()
    df = df[["expected_answer", "problem", "generated_solution"]]

    # ä»…ä¿ç•™ç­”æ¡ˆä¸ºâ€œæ•°å€¼â€çš„æ ·æœ¬
    is_number = pd.to_numeric(pd.Series(df["expected_answer"]), errors="coerce").notnull()
    df = df.iloc[np.where(is_number)[0]]
    print(f"ğŸ” [SFT] åŸå§‹æ ·æœ¬æ•°ï¼š{len(df)}")

    def format_dataset_row(x):
        expected_answer = x["expected_answer"]
        problem = x["problem"]
        # å»é™¤ <think> æ ‡ç­¾
        thoughts = x["generated_solution"].replace("<think>", "").replace("</think>", "").strip()
        final_prompt = (
            reasoning_start + thoughts + reasoning_end +
            solution_start + str(expected_answer) + solution_end
        )
        return [
            {"role": "system",    "content": system_prompt},
            {"role": "user",      "content": problem},
            {"role": "assistant", "content": final_prompt},
        ]

    df["Messages"] = df.apply(format_dataset_row, axis=1)

    # æˆªæ–­åˆ° max_seq_length/2ï¼Œé¿å…è¿‡é•¿è½¨è¿¹å½±å“æ ¼å¼å­¦ä¹ 
    print("âœ‚ï¸ [SFT] æ­£åœ¨ä¾æ®é•¿åº¦æˆªæ–­æ ·æœ¬ï¼ˆ<= max_seq_length/2ï¼‰...")
    df["N"] = df["Messages"].apply(lambda msgs: len(tokenizer.apply_chat_template(msgs)))
    df = df.loc[df["N"] <= max_seq_length / 2].copy()
    print(f"âœ… [SFT] æˆªæ–­åæ ·æœ¬æ•°ï¼š{df.shape[0]}")

    # æ„å»º HF datasets
    df["text"] = tokenizer.apply_chat_template(df["Messages"].values.tolist(), tokenize=False)
    sft_ds = Dataset.from_pandas(df)

    print("ğŸ‹ï¸ [SFT] æ­£åœ¨å¯åŠ¨ SFT è®­ç»ƒä»¥è®©æ¨¡å‹å­¦ä¼šè‡ªå®šä¹‰æ ¼å¼...")
    sft_trainer = SFTTrainer(
        model = model,
        tokenizer = tokenizer,
        train_dataset = sft_ds,
        args = SFTConfig(
            dataset_text_field = "text",
            per_device_train_batch_size = 1,
            gradient_accumulation_steps = 1,
            warmup_steps = 5,
            num_train_epochs = 2,
            learning_rate = 2e-4,      # è‹¥é•¿è®­å¯é™è‡³ 2e-5
            logging_steps = 5,
            optim = "adamw_8bit",
            weight_decay = 0.01,
            lr_scheduler_type = "linear",
            seed = args.seed,
            report_to = "none",
        ),
    )
    sft_trainer.train()
    print("âœ… [SFT] é¢„å¾®è°ƒå®Œæˆã€‚\n")

    # ç®€å•éªŒè¯ï¼šæ˜¯å¦éµå®ˆæ¨¡æ¿
    try:
        test_text = tokenizer.apply_chat_template(
            sft_ds[0]["Messages"][:2], tokenize=False, add_generation_prompt=True
        )
        _ = model.generate(
            **tokenizer(test_text, return_tensors="pt").to("cuda"),
            temperature=0,
            max_new_tokens=256,
        )
        print("ğŸ” [SFT] æ¨¡å‹å·²å­¦ä¼šåŸºæœ¬æ ¼å¼ï¼ˆå·²å®Œæˆä¸€æ¬¡æ— æ¸©åº¦é‡‡æ ·æµ‹è¯•ï¼‰ã€‚")
    except Exception as e:
        print(f"âš ï¸ [SFT] æµ‹è¯•ç”Ÿæˆå¤±è´¥ï¼š{e}")

    # é‡Šæ”¾æ— ç”¨å¼•ç”¨
    del sft_ds, df
    torch.cuda.empty_cache()
    gc.collect()
else:
    print("â­ï¸ å·²è·³è¿‡ SFT é¢„å¾®è°ƒé˜¶æ®µã€‚\n")

# -----------------------------
# ç¬¬ 5 æ­¥ï¼šæ•°æ®å‡†å¤‡ï¼ˆOpen R1 æ•°å­¦æ•°æ®é›†ï¼‰
# -----------------------------
print("ğŸ§® æ­£åœ¨åŠ è½½ Open R1 æ•°å­¦æ•°æ®é›†ï¼ˆopen-r1/DAPO-Math-17k-Processed, split=trainï¼‰...")
r1_ds = load_dataset("open-r1/DAPO-Math-17k-Processed", "en", split="train")
print(f"âœ… æ•°æ®é›†åŠ è½½å®Œæˆï¼Œæ ·æœ¬æ•°ï¼š{len(r1_ds)}")

def extract_hash_answer(text: str) -> str:
    # Open R1 çš„ç­”æ¡ˆæ— éœ€ä»â€œ####â€åæå–ï¼Œç›´æ¥è¿”å›
    return text

# æ˜ å°„ä¸º (prompt, answer)
r1_ds = r1_ds.map(lambda x: {
    "prompt": [
        {"role": "system", "content": system_prompt},
        {"role": "user",   "content": x["prompt"]},
    ],
    "answer": extract_hash_answer(x["solution"]),
})
print("âœ… æ•°æ®å­—æ®µå·²è½¬æ¢ä¸º prompt/answerã€‚")

# -----------------------------
# ç¬¬ 6 æ­¥ï¼šæ­£åˆ™ä¸å¥–åŠ±å‡½æ•°
# -----------------------------
print("ğŸ§ª æ­£åœ¨æ„å»ºå¥–åŠ±å‡½æ•°ï¼ˆæ ¼å¼åŒ¹é… + æ•°å€¼æå–/æ¯”å¯¹ï¼‰...")

solution_end_regex = r"</SOLUTION>[\s]{0,}" + "(?:" + re.escape(tokenizer.eos_token) + ")?"

match_format = re.compile(
    rf"{re.escape(reasoning_end)}.*?"
    rf"{re.escape(solution_start)}(.+?){solution_end_regex}"
    rf"[\s]{{0,}}$",
    flags=re.MULTILINE | re.DOTALL
)

def match_format_exactly(completions, **kwargs):
    """æ ¼å¼å®Œå…¨åŒ¹é…ï¼š+3 åˆ†"""
    scores = []
    for completion in completions:
        score = 0.0
        response = completion[0]["content"]
        if match_format.search(response) is not None:
            score += 3.0
        scores.append(score)
    return scores

def match_format_approximately(completions, **kwargs):
    """æ ¼å¼éƒ¨åˆ†åŒ¹é…ï¼šæ¯å‘½ä¸­ä¸€ä¸ªå…³é”®æ ‡è®° +0.5ï¼Œç¼ºå¤±æˆ–è¶…é‡åˆ™ -1."""
    scores = []
    for completion in completions:
        score = 0.0
        response = completion[0]["content"]
        # <start_working_out> åœ¨æ¨¡æ¿ä¸­ä¼šé¢„ç½®ï¼Œä¸å¥–åŠ±
        score += 0.5 if response.count(reasoning_end)   == 1 else -1.0
        score += 0.5 if response.count(solution_start)  == 1 else -1.0
        score += 0.5 if response.count(solution_end)    == 1 else -1.0
        scores.append(score)
    return scores

def check_answer(prompts, completions, answer, **kwargs):
    """ç­”æ¡ˆæ¯”å¯¹ï¼šå®Œå…¨ç›¸ç­‰ +5ï¼›å»ç©ºæ ¼ç›¸ç­‰ +3.5ï¼›æ•°å€¼æ¯”å€¼æ¥è¿‘ +1.5~2ï¼›å¦åˆ™æ‰£åˆ†ã€‚"""
    responses = [completion[0]["content"] for completion in completions]
    extracted_resps = [
        g.group(1) if (g := match_format.search(r)) is not None else None
        for r in responses
    ]
    scores = []
    for guess, truth in zip(extracted_resps, answer):
        score = 0.0
        if guess is None:
            scores.append(-2.0)
            continue
        if guess == truth:
            score += 5.0
        elif guess.strip() == str(truth).strip():
            score += 3.5
        else:
            try:
                ratio = float(guess) / float(truth)
                if 0.9 <= ratio <= 1.1:
                    score += 2.0
                elif 0.8 <= ratio <= 1.2:
                    score += 1.5
                else:
                    score -= 2.5
            except Exception:
                score -= 4.5
        scores.append(score)
    return scores

match_numbers = re.compile(
    re.escape(solution_start) + r".*?[\s]{0,}([-]?[\d\.\,]{1,})",
    flags=re.MULTILINE | re.DOTALL,
)

PRINTED_TIMES = 0
PRINT_EVERY_STEPS = 5

def check_numbers(prompts, completions, answer, **kwargs):
    """ä» <SOLUTION> ä¸­æå–ç¬¬ä¸€ä¸ªæ•°å­—ï¼Œä¸çœŸå€¼åš float æ¯”è¾ƒï¼šç›¸ç­‰ +3.5ï¼Œå¦åˆ™ -1.5ã€‚"""
    global PRINTED_TIMES, PRINT_EVERY_STEPS
    question = prompts[0][-1]["content"]
    responses = [completion[0]["content"] for completion in completions]
    extracted = [
        g.group(1) if (g := match_numbers.search(r)) is not None else None
        for r in responses
    ]

    # æ¯éš”è‹¥å¹² step æ‰“å°ä¸€æ¬¡è§‚æµ‹ï¼ˆä¸­æ–‡æç¤ºï¼‰
    if PRINTED_TIMES % PRINT_EVERY_STEPS == 0:
        print(
            "ğŸ§ªã€è§‚æµ‹ã€‘\n" +
            "é¢˜ç›®:\n{}\n\næ ‡å‡†ç­”æ¡ˆ:\n{}\n\næ¨¡å‹è¾“å‡º:\n{}\n\næå–æ•°å­—:\n{}\n".format(
                question, answer[0], responses[0], extracted[0]
            )
        )
    PRINTED_TIMES += 1

    scores = []
    for guess, truth in zip(extracted, answer):
        if guess is None:
            scores.append(-2.5)
            continue
        try:
            t = float(str(truth).strip())
            g = float(str(guess).strip().replace(",", ""))
            scores.append(3.5 if g == t else -1.5)
        except Exception:
            scores.append(0.0)
    return scores

# -----------------------------
# ç¬¬ 7 æ­¥ï¼šé•¿åº¦ç»Ÿè®¡ï¼Œé¿å…è¶…é•¿æˆªæ–­
# -----------------------------
print("ğŸ“ æ­£åœ¨ç»Ÿè®¡æç¤ºé•¿åº¦ï¼ˆä¿ç•™ 90% åˆ†ä½æ•°ä»¥å†…çš„æ ·æœ¬ï¼Œä»¥å‡å°‘æˆªæ–­é£é™©ï¼‰...")
tokenized = r1_ds.map(
    lambda x: {"tokens": tokenizer.apply_chat_template(x["prompt"], add_generation_prompt=True, tokenize=True)},
    batched=True,
)
tokenized = tokenized.map(lambda x: {"L": len(x["tokens"])})
maximum_length = int(np.quantile(tokenized["L"], 0.9))
print("âœ… è®¡ç®—å¾—åˆ°çš„ 90%% åˆ†ä½æç¤ºé•¿åº¦ï¼š", maximum_length)

keep_indices = np.where(np.array(tokenized["L"]) <= maximum_length)[0]
r1_ds = r1_ds.select(keep_indices.tolist())
del tokenized
print(f"âœ… è¿‡æ»¤åè®­ç»ƒæ ·æœ¬æ•°ï¼š{len(r1_ds)}\n")

# -----------------------------
# ç¬¬ 8 æ­¥ï¼šé…ç½®å¹¶è¿è¡Œ GRPO è®­ç»ƒ
# -----------------------------
if args.run_grpo:
    print("ğŸ [GRPO] æ­£åœ¨å‡†å¤‡è®­ç»ƒå‚æ•°å¹¶å¯åŠ¨è®­ç»ƒ ...")
    max_prompt_length     = maximum_length + 1
    max_completion_length = max_seq_length - max_prompt_length

    vllm_sampling_params = SamplingParams(
        min_p = 0.1,
        top_p = 1.0,
        top_k = -1,
        seed  = args.seed,
        stop  = [tokenizer.eos_token],
        include_stop_str_in_output = True,
    )

    training_args = GRPOConfig(
        vllm_sampling_params = vllm_sampling_params,
        temperature = 1.0,
        learning_rate = 5e-6,
        weight_decay = 0.01,
        warmup_ratio = 0.1,
        lr_scheduler_type = "linear",
        optim = "adamw_8bit",
        logging_steps = 1,
        per_device_train_batch_size = 1,
        gradient_accumulation_steps = 1,  # å¯å¢å¤§ä¸º 4 è®©è®­ç»ƒæ›´å¹³æ»‘
        num_generations = 4,               # æ˜¾å­˜ä¸è¶³å¯é™ä½
        max_prompt_length = max_prompt_length,
        max_completion_length = max_completion_length,
        max_steps = args.max_steps,        # æˆ–è€…ä½¿ç”¨ num_train_epochs
        save_steps = args.max_steps,
        report_to = "none",
        output_dir = args.save_dir,
    )

    grpo_trainer = GRPOTrainer(
        model = model,
        processing_class = tokenizer,
        reward_funcs = [
            match_format_exactly,
            match_format_approximately,
            check_answer,
            check_numbers,
        ],
        args = training_args,
        train_dataset = r1_ds,
    )

    grpo_trainer.train()
    print("âœ… [GRPO] è®­ç»ƒå®Œæˆã€‚\n")
else:
    print("â­ï¸ å·²è·³è¿‡ GRPO è®­ç»ƒé˜¶æ®µã€‚\n")

# -----------------------------
# ç¬¬ 9 æ­¥ï¼šæ¨ç†å¯¹æ¯”ï¼ˆæœªåŠ è½½ LoRA vs. åŠ è½½ LoRAï¼‰
# -----------------------------
if args.run_infer:
    print("ğŸ§ª æ­£åœ¨è¿›è¡Œæ¨ç†å¯¹æ¯”ï¼ˆæœªåŠ è½½ LoRA / åŠ è½½ LoR Aï¼‰...")
    test_question = "What is the sqrt of 101?"

    # æœªåŠ è½½ LoRAï¼ˆåŸºåº§ + vLLMï¼‰
    sampling_params = SamplingParams(
        temperature = 1.0,
        top_k = 50,
        max_tokens = 256,
    )
    out_plain = model.fast_generate(
        [test_question],
        sampling_params = sampling_params,
        lora_request = None,
    )[0].outputs[0].text
    print("\nğŸ”¹ æœªåŠ è½½ LoRA çš„è¾“å‡ºï¼š\n", out_plain)

    # ä¿å­˜ LoRAï¼Œå¹¶åŠ è½½å†æ¨ç†
    save_lora_dir = os.path.join(args.save_dir, "grpo_saved_lora")
    os.makedirs(save_lora_dir, exist_ok=True)
    model.save_lora(save_lora_dir)
    print(f"\nğŸ’¾ LoRA å·²ä¿å­˜åˆ°ï¼š{save_lora_dir}")

    # ç®€å•æ ¡éªŒ LoRA å‚æ•°éå…¨é›¶
    from safetensors import safe_open
    with safe_open(os.path.join(save_lora_dir, "adapter_model.safetensors"), framework="pt") as f:
        for k in f.keys():
            t = f.get_tensor(k)
            n_zeros = (t == 0).sum().item()
            assert n_zeros != t.numel(), "æ£€æµ‹åˆ° LoRA æƒé‡ä¸ºå…¨é›¶ï¼Œå¯èƒ½è®­ç»ƒä¸å……åˆ†æˆ–ä¿å­˜å¤±è´¥ã€‚"
    print("âœ… LoRA æƒé‡æ£€æŸ¥é€šè¿‡ï¼ˆéå…¨é›¶ï¼‰ã€‚")

    # åŠ è½½ LoRA åæ¨ç†
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user",   "content": "What is the sqrt of 101?"},
    ]
    text_for_gen = tokenizer.apply_chat_template(
        messages, add_generation_prompt=True, tokenize=False
    )
    out_lora = model.fast_generate(
        text_for_gen,
        sampling_params = SamplingParams(temperature=1.0, top_k=50, max_tokens=512),
        lora_request = model.load_lora(save_lora_dir),
    )[0].outputs[0].text
    print("\nğŸ”¸ åŠ è½½ LoRA åçš„è¾“å‡ºï¼š\n", out_lora, "\n")
else:
    print("â­ï¸ å·²è·³è¿‡æ¨ç†é˜¶æ®µã€‚\n")

# -----------------------------
# ç¬¬ 10 æ­¥ï¼šå¯é€‰çš„ä¿å­˜/å¯¼å‡ºç¤ºä¾‹ï¼ˆæŒ‰éœ€æ‰“å¼€ï¼‰
# -----------------------------
print("ğŸ“¦ å¦‚éœ€è¿›ä¸€æ­¥å¯¼å‡ºä¸º 16bit/4bit/GGUFï¼Œè¯·å‚è€ƒä»¥ä¸‹æ³¨é‡Šä»£ç ï¼Œè‡ªè¡Œåˆ‡æ¢ä¸º True å³å¯ï¼š")
print("""
# 16bit åˆå¹¶ä¿å­˜ï¼ˆç”¨äº vLLMï¼‰
# if True: model.save_pretrained_merged("model", tokenizer, save_method="merged_16bit")

# 4bit åˆå¹¶ä¿å­˜
# if True: model.save_pretrained_merged("model", tokenizer, save_method="merged_4bit")

# ä»…ä¿å­˜ LoRA é€‚é…å™¨ï¼ˆå·²åœ¨ä¸Šæ–¹ä¿å­˜ï¼‰
# if True:
#     model.save_pretrained("model")
#     tokenizer.save_pretrained("model")

# GGUF / llama.cpp å¯¼å‡ºç¤ºä¾‹
# if True: model.save_pretrained_gguf("model", tokenizer)                 # q8_0
# if True: model.save_pretrained_gguf("model", tokenizer, quantization_method="f16")
# if True: model.save_pretrained_gguf("model", tokenizer, quantization_method="q4_k_m")
# if True:
#     model.push_to_hub_gguf(
#         "hf/your-model",
#         tokenizer,
#         quantization_method=["q4_k_m", "q8_0", "q5_k_m"],
#         token=""
#     )
""")

print("ğŸ‰ å…¨éƒ¨æµç¨‹å·²ç»“æŸã€‚ç¥è®­ç»ƒé¡ºåˆ©ï¼")

