# train_sft.py
/opt/conda/lib/python3.11/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
WANDB_AVAILABLEæ˜¯å¯ç”¨çš„ï¼Œå·²ç»å®‰è£…äº†wandb
ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.
/opt/conda/lib/python3.11/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-09-07 09:51:48,647] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
INFO 09-07 09:51:50 [__init__.py:241] Automatically detected platform cuda.
Unsloth: Your Flash Attention 2 installation seems to be broken?
A possible explanation is you have a new CUDA version which isn't
yet compatible with FA2? Please file a ticket to Unsloth or FA2.
We shall now use Xformers instead, which does not have any performance hits!
We found this negligible impact by benchmarking on 1x A100.
ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!
[09:51:54] INFO - æ—¥å¿—å†™å…¥: ./outputs/qwen3_4b_sft_lora/logs/train_20250907_095154.log
[09:51:54] INFO - ===== è®­ç»ƒé…ç½® =====
[09:51:54] INFO - {
  "model_name": "unsloth/Qwen3-4B-Instruct-2507",
  "max_seq_length": 2048,
  "load_in_4bit": true,
  "load_in_8bit": false,
  "full_finetuning": false,
  "hf_token": null,
  "lora_r": 32,
  "lora_alpha": 32,
  "lora_dropout": 0.0,
  "bias": "none",
  "use_gradient_checkpointing": "unsloth",
  "use_rslora": false,
  "loftq_config": null,
  "target_modules": [
    "q_proj",
    "k_proj",
    "v_proj",
    "o_proj",
    "gate_proj",
    "up_proj",
    "down_proj"
  ],
  "chat_template": "qwen3-instruct",
  "dataset_name": "mlabonne/FineTome-100k",
  "dataset_split": "train",
  "dataset_text_field": "text",
  "instruction_part": "<|im_start|>user\n",
  "response_part": "<|im_start|>assistant\n",
  "per_device_train_batch_size": 2,
  "gradient_accumulation_steps": 4,
  "warmup_steps": 5,
  "max_steps": 60,
  "num_train_epochs": 2,
  "learning_rate": 0.0002,
  "logging_steps": 1,
  "optim": "adamw_8bit",
  "weight_decay": 0.01,
  "lr_scheduler_type": "linear",
  "seed": 3407,
  "report_to": "wandb",
  "use_wandb": true,
  "wandb_project": "unsloth-sft",
  "wandb_entity": null,
  "wandb_run_name": null,
  "wandb_tags": null,
  "wandb_dir": null,
  "wandb_group": null,
  "wandb_job_type": "train",
  "wandb_mode": null,
  "wandb_notes": null,
  "wandb_log_model": false,
  "output_dir": "./outputs/qwen3_4b_sft_lora",
  "save_steps": 2,
  "save_total_limit": null,
  "logging_dir": null
}
[09:51:54] INFO - éšæœºç§å­å·²è®¾ç½®: 3407
[09:51:54] INFO - PyTorch: 2.7.1+cu126
[09:51:54] INFO - GPU: NVIDIA GeForce RTX 4090 D  æ˜¾å­˜ä¸Šé™: 23.546 GB  å¯åŠ¨ä¿ç•™: 0.0 GB
wandb: WARNING Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.
wandb: Tracking run with wandb version 0.21.3
wandb: W&B syncing is set to `offline` in this directory. Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
wandb: Run data is saved locally in /workspace/verl/docs/Unsloth/wandb/offline-run-20250907_095154-lfe5p81x
[09:51:55] INFO - å·²è¿æ¥ W&Bï¼šproject=unsloth-sft, run=None
[09:51:55] INFO - å¼€å§‹åŠ è½½åŸºç¡€æ¨¡å‹â€¦
==((====))==  Unsloth 2025.9.1: Fast Qwen3 patching. Transformers: 4.55.4. vLLM: 0.10.1.1.
   \\   /|    NVIDIA GeForce RTX 4090 D. Num GPUs = 1. Max memory: 23.546 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.7.1+cu126. CUDA: 8.9. CUDA Toolkit: 12.6. Triton: 3.3.1
\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
[09:52:21] INFO - åŸºç¡€æ¨¡å‹åŠ è½½å®Œæˆã€‚
[09:52:21] INFO - æ³¨å…¥ LoRA é€‚é…å™¨â€¦
Unsloth: Making `model.base_model.model.model` require gradients
[09:52:26] INFO - LoRA é€‚é…å™¨æ³¨å…¥å®Œæˆã€‚
[09:52:26] INFO - åº”ç”¨ Chat æ¨¡æ¿: qwen3-instruct
[09:52:26] INFO - åŠ è½½æ•°æ®é›†: mlabonne/FineTome-100k [train] â€¦
[09:52:36] INFO - åŸå§‹æ ·æœ¬é¢„è§ˆï¼ˆå‰ 3 æ¡ï¼‰:
[09:52:36] INFO - [#0] åŸå§‹(raw): {
  "conversations": [
    {
      "from": "human",
      "value": "Explain what boolean operators are, what they do, and provide examples of how they can be used in programming. Additionally, describe the concept of operator precedence and provide examples of how it affects the evaluation of boolean expressions. Discuss the difference between short-circuit evaluation and normal evaluation in boolean expressions and demonstrate their usage in code. \n\nFurthermore, add the requirement that the c â€¦ <+3672 chars>
[09:52:36] INFO - [#1] åŸå§‹(raw): {
  "conversations": [
    {
      "from": "human",
      "value": "Explain how recursion works and provide a recursive function in Python that calculates the factorial of a given number."
    },
    {
      "from": "gpt",
      "value": "Recursion is a programming technique where a function calls itself to solve a problem. It breaks down a complex problem into smaller, more manageable subproblems until a base case is reached. The base case is a condition where the function does not call itself, â€¦ <+1488 chars>
[09:52:36] INFO - [#2] åŸå§‹(raw): {
  "conversations": [
    {
      "from": "human",
      "value": "Explain what boolean operators are, what they do, and provide examples of how they can be used in programming. Additionally, describe the concept of operator precedence and provide examples of how it affects the evaluation of boolean expressions.\n\nFurthermore, discuss the difference between short-circuit evaluation and normal evaluation in boolean expressions and demonstrate their usage in code. Finally, delve into the concept â€¦ <+3906 chars>
[09:52:36] INFO - æ ‡å‡†åŒ–ä¸º conversations â€¦
[09:52:37] INFO - æ¸²æŸ“ä¸ºæ–‡æœ¬å­—æ®µâ€¦
apply_chat_template: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100000/100000 [00:16<00:00, 6167.87 examples/s]
[09:52:53] INFO - æ¸²æŸ“æ ·æœ¬æ ¡éªŒ: <|im_start|>user
Explain what boolean operators are, what they do, and provide examples of how they can be used in programming. Additionally, describe the concept of operator precedence and provide examples of how it affects the evaluation of boolean expressions. Discuss the difference between short-circuit evaluation and normal evaluation in boolean expressions and demonstrate their usage in code.

Furthermore, add the requirement that the code must be written in a language that does not support short-circuit evaluation natively, forcing the test taker to implement their own logic for short-circuit evaluation.

Finally, delve into the concept of truthiness and falsiness in programming languages, explaining how it affects the evaluation of boolean expressions. Add the constraint that the test taker must write code that handles cases where truthiness and falsiness are implemented differently across different programming languages.<|im_end|>
<|im_start|>assistant
Boolean operators are l â€¦ <+2944 chars>
[09:52:53] INFO - åˆ›å»º SFTTrainerâ€¦
/workspace/verl/docs/Unsloth/unsloth_compiled_cache/UnslothSFTTrainer.py:581: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
Map (num_proc=92): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100000/100000 [00:15<00:00, 6299.16 examples/s]
/workspace/verl/docs/Unsloth/unsloth_compiled_cache/UnslothSFTTrainer.py:714: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `UnslothSFTTrainer.__init__`. Use `processing_class` instead.
  super().__init__(
[09:53:12] INFO - åˆ‡æ¢ä¸ºä»…å­¦ä¹  assistant å“åº”ï¼ˆå¿½ç•¥ user æŒ‡ä»¤éƒ¨åˆ†çš„ lossï¼‰â€¦
Map (num_proc=88): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100000/100000 [00:04<00:00, 20061.16 examples/s]
[09:53:19] INFO - ç¼–ç æ ·æœ¬é¢„è§ˆ: <|im_start|>user
Explain what boolean operators are, what they do, and provide examples of how they can be used in programming. Additionally, describe the concept of operator precedence and provide examples of how it affects the evaluation of boolean expressions. Discuss the difference between short-circuit evaluation and normal evaluation in boolean expressions and demonstrate their usage in code.

Furthermore, add the requirement that the code must be written in a language that does not support short-circuit evaluation natively, forcing the test taker to implement their own logic for short-circuit evaluation.

Finally, delve into the concept of truthiness and falsiness in programming languages, explaining how it affects the evaluation of boolean expressions. Add the constraint that the test taker must write code that handles cases where truthiness and falsiness are implemented differently across different programming languages.<|im_end|>
<|im_start|>assistant
Boolean operators are logical operators used in programming to manipulate boolean values. They operate on one or more boolean operands and return a boolean result. The three main boolean operators are "AND" (&&), "OR" (||), and "NOT" (!).

The "AND" operator returns true if both of its operands are true, and false otherwise. For example:

```python
x = 5
y = 10

[09:53:19] INFO - GPU = NVIDIA GeForce RTX 4090 D. Max memory = 23.546 GB.
[09:53:19] INFO - å¯åŠ¨æ—¶ä¿ç•™æ˜¾å­˜ = 3.816 GB.
[09:53:19] INFO - å¼€å§‹è®­ç»ƒâ€¦
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 100,000 | Num Epochs = 1 | Total steps = 60
O^O/ \_/ \    Batch size per device = 2 | Gradient accumulation steps = 4
\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8
 "-____-"     Trainable parameters = 66,060,288 of 4,088,528,384 (1.62% trained)
  0%|                                                                                                | 0/60 [00:00<?, ?it/s]Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 1.2341, 'grad_norm': 1.26889169216156, 'learning_rate': 0.0, 'epoch': 0.0}
{'loss': 1.1838, 'grad_norm': 1.1245009899139404, 'learning_rate': 4e-05, 'epoch': 0.0}
{'loss': 1.7394, 'grad_norm': 1.8781607151031494, 'learning_rate': 8e-05, 'epoch': 0.0}
{'loss': 1.3852, 'grad_norm': 1.3905386924743652, 'learning_rate': 0.00012, 'epoch': 0.0}
{'loss': 0.964, 'grad_norm': 1.0623937845230103, 'learning_rate': 0.00016, 'epoch': 0.0}
{'loss': 1.0264, 'grad_norm': 1.0224908590316772, 'learning_rate': 0.0002, 'epoch': 0.0}
{'loss': 0.5411, 'grad_norm': 0.26971349120140076, 'learning_rate': 0.00019636363636363636, 'epoch': 0.0}
{'loss': 0.8925, 'grad_norm': 0.32616934180259705, 'learning_rate': 0.00019272727272727274, 'epoch': 0.0}
{'loss': 0.7271, 'grad_norm': 0.254302054643631, 'learning_rate': 0.0001890909090909091, 'epoch': 0.0}
{'loss': 0.673, 'grad_norm': 0.22280950844287872, 'learning_rate': 0.00018545454545454545, 'epoch': 0.0}
{'loss': 0.7977, 'grad_norm': 0.24312573671340942, 'learning_rate': 0.00018181818181818183, 'epoch': 0.0}
{'loss': 1.0145, 'grad_norm': 0.3125922679901123, 'learning_rate': 0.0001781818181818182, 'epoch': 0.0}
{'loss': 0.8676, 'grad_norm': 0.272381991147995, 'learning_rate': 0.00017454545454545454, 'epoch': 0.0}
{'loss': 0.5513, 'grad_norm': 0.2676119804382324, 'learning_rate': 0.0001709090909090909, 'epoch': 0.0}
{'loss': 0.7952, 'grad_norm': 0.24834468960762024, 'learning_rate': 0.00016727272727272728, 'epoch': 0.0}
{'loss': 0.5473, 'grad_norm': 0.27768614888191223, 'learning_rate': 0.00016363636363636366, 'epoch': 0.0}
{'loss': 0.8955, 'grad_norm': 0.20760919153690338, 'learning_rate': 0.00016, 'epoch': 0.0}
{'loss': 0.7086, 'grad_norm': 0.21113522350788116, 'learning_rate': 0.00015636363636363637, 'epoch': 0.0}
{'loss': 0.6976, 'grad_norm': 0.16633240878582, 'learning_rate': 0.00015272727272727275, 'epoch': 0.0}
{'loss': 0.833, 'grad_norm': 0.21081769466400146, 'learning_rate': 0.0001490909090909091, 'epoch': 0.0}
{'loss': 0.8049, 'grad_norm': 0.26581352949142456, 'learning_rate': 0.00014545454545454546, 'epoch': 0.0}
{'loss': 0.7765, 'grad_norm': 0.19209444522857666, 'learning_rate': 0.00014181818181818184, 'epoch': 0.0}
{'loss': 0.9735, 'grad_norm': 0.1767033189535141, 'learning_rate': 0.0001381818181818182, 'epoch': 0.0}
{'loss': 0.741, 'grad_norm': 0.21236300468444824, 'learning_rate': 0.00013454545454545455, 'epoch': 0.0}
{'loss': 0.5395, 'grad_norm': 0.16105987131595612, 'learning_rate': 0.00013090909090909093, 'epoch': 0.0}
{'loss': 0.6744, 'grad_norm': 0.17266200482845306, 'learning_rate': 0.00012727272727272728, 'epoch': 0.0}
{'loss': 0.6948, 'grad_norm': 0.20043684542179108, 'learning_rate': 0.00012363636363636364, 'epoch': 0.0}
{'loss': 0.6862, 'grad_norm': 0.19250285625457764, 'learning_rate': 0.00012, 'epoch': 0.0}
{'loss': 1.0101, 'grad_norm': 0.2222357541322708, 'learning_rate': 0.00011636363636363636, 'epoch': 0.0}
{'loss': 0.8951, 'grad_norm': 0.18886493146419525, 'learning_rate': 0.00011272727272727272, 'epoch': 0.0}
{'loss': 0.63, 'grad_norm': 0.1961446851491928, 'learning_rate': 0.00010909090909090909, 'epoch': 0.0}
{'loss': 0.4651, 'grad_norm': 0.1477976143360138, 'learning_rate': 0.00010545454545454545, 'epoch': 0.0}
{'loss': 0.5622, 'grad_norm': 0.21210059523582458, 'learning_rate': 0.00010181818181818181, 'epoch': 0.0}
{'loss': 0.4495, 'grad_norm': 0.15238900482654572, 'learning_rate': 9.818181818181818e-05, 'epoch': 0.0}
{'loss': 0.6806, 'grad_norm': 0.19014455378055573, 'learning_rate': 9.454545454545455e-05, 'epoch': 0.0}
{'loss': 0.9175, 'grad_norm': 0.19885402917861938, 'learning_rate': 9.090909090909092e-05, 'epoch': 0.0}
{'loss': 0.7991, 'grad_norm': 0.1999858021736145, 'learning_rate': 8.727272727272727e-05, 'epoch': 0.0}
{'loss': 0.5879, 'grad_norm': 0.1652590036392212, 'learning_rate': 8.363636363636364e-05, 'epoch': 0.0}
{'loss': 0.6464, 'grad_norm': 0.18798349797725677, 'learning_rate': 8e-05, 'epoch': 0.0}
{'loss': 0.8446, 'grad_norm': 0.19232279062271118, 'learning_rate': 7.636363636363637e-05, 'epoch': 0.0}
{'loss': 0.6498, 'grad_norm': 0.19501300156116486, 'learning_rate': 7.272727272727273e-05, 'epoch': 0.0}
{'loss': 0.8335, 'grad_norm': 0.1964937299489975, 'learning_rate': 6.90909090909091e-05, 'epoch': 0.0}
{'loss': 0.667, 'grad_norm': 0.16009770333766937, 'learning_rate': 6.545454545454546e-05, 'epoch': 0.0}
{'loss': 0.6401, 'grad_norm': 0.1928119957447052, 'learning_rate': 6.181818181818182e-05, 'epoch': 0.0}
{'loss': 0.6539, 'grad_norm': 0.18661463260650635, 'learning_rate': 5.818181818181818e-05, 'epoch': 0.0}
{'loss': 0.7584, 'grad_norm': 0.20336568355560303, 'learning_rate': 5.4545454545454546e-05, 'epoch': 0.0}
{'loss': 0.6525, 'grad_norm': 0.20021581649780273, 'learning_rate': 5.090909090909091e-05, 'epoch': 0.0}
{'loss': 0.5513, 'grad_norm': 0.1703331023454666, 'learning_rate': 4.7272727272727275e-05, 'epoch': 0.0}
{'loss': 0.8928, 'grad_norm': 0.22873395681381226, 'learning_rate': 4.3636363636363636e-05, 'epoch': 0.0}
{'loss': 0.8995, 'grad_norm': 0.15478254854679108, 'learning_rate': 4e-05, 'epoch': 0.0}
{'loss': 0.3854, 'grad_norm': 0.15127526223659515, 'learning_rate': 3.6363636363636364e-05, 'epoch': 0.0}
{'loss': 0.8053, 'grad_norm': 0.15952803194522858, 'learning_rate': 3.272727272727273e-05, 'epoch': 0.0}
{'loss': 1.2978, 'grad_norm': 0.25887489318847656, 'learning_rate': 2.909090909090909e-05, 'epoch': 0.0}
{'train_runtime': 466.9755, 'train_samples_per_second': 1.028, 'train_steps_per_second': 0.128, 'train_loss': 0.7906134098768234, 'epoch': 0.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 60/60 [07:46<00:00,  7.78s/it]
[10:01:08] INFO - è®­ç»ƒå®Œæˆã€‚
[10:01:08] INFO - è®­ç»ƒè€—æ—¶ 466.98 ç§’ï¼ˆçº¦ 7.78 åˆ†é’Ÿï¼‰ã€‚
[10:01:08] INFO - å³°å€¼ä¿ç•™æ˜¾å­˜ = 5.025 GBï¼›å…¶ä¸­è®­ç»ƒå¢é‡ = 1.209 GBã€‚
[10:01:08] INFO - æ˜¾å­˜å ç”¨å³°å€¼å æ¯” = 21.341%ï¼›è®­ç»ƒå¢é‡å æ¯” = 5.135%ã€‚
[10:01:08] INFO - Unsloth ä¿å­˜æ¨¡å‹ Trainer.save_model
[10:01:12] INFO - æ¨¡å‹å·²ä¿å­˜è‡³: ./outputs/qwen3_4b_sft_lora
wandb:
wandb:
wandb: Run history:
wandb:           env/gpu_mem_gb â–
wandb:     memory/lora_delta_gb â–
wandb:    memory/lora_delta_pct â–
wandb:  memory/peak_reserved_gb â–
wandb: memory/peak_reserved_pct â–
wandb: memory/start_reserved_gb â–
wandb:   time/train_runtime_sec â–
wandb:              train/epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:        train/global_step â–â–â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:          train/grad_norm â–ˆâ–†â–…â–…â–â–â–â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                       +3 ...
wandb:
wandb: Run summary:
wandb:            env/gpu_mem_gb 23.546
wandb:              env/gpu_name NVIDIA GeForce RTX 4...
wandb:      memory/lora_delta_gb 1.209
wandb:     memory/lora_delta_pct 5.135
wandb:   memory/peak_reserved_gb 5.025
wandb:  memory/peak_reserved_pct 21.341
wandb:  memory/start_reserved_gb 3.816
wandb: metrics/train_runtime_sec 466.9755
wandb:                    status success
wandb:    time/train_runtime_sec 466.9755
wandb:                       +11 ...
wandb:
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /workspace/verl/docs/Unsloth/wandb/offline-run-20250907_095154-lfe5p81x
wandb: Find logs at: ./wandb/offline-run-20250907_095154-lfe5p81x/logs
[10:01:12] INFO - 466.98 ç§’ used for training.
[10:01:12] INFO - ğŸ‰  è®­ç»ƒç»“æŸã€‚



# python train_sft.py --data_files xiaosen_sft.jsonl
WANDB_AVAILABLEæ˜¯å¯ç”¨çš„ï¼Œå·²ç»å®‰è£…äº†wandb
ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.
INFO 09-07 22:21:29 [importing.py:53] Triton module has been replaced with a placeholder.
INFO 09-07 22:21:29 [__init__.py:239] Automatically detected platform cuda.
ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!
[22:21:37] INFO - æ—¥å¿—å†™å…¥: ./outputs/qwen3_4b_sft_lora/logs/train_20250907_222137.log
[22:21:37] INFO - ===== è®­ç»ƒé…ç½® =====
[22:21:37] INFO - {
  "model_name": "unsloth/Qwen3-4B-Instruct-2507",
  "max_seq_length": 2048,
  "load_in_4bit": true,
  "load_in_8bit": false,
  "full_finetuning": false,
  "hf_token": null,
  "lora_r": 32,
  "lora_alpha": 32,
  "lora_dropout": 0.0,
  "bias": "none",
  "use_gradient_checkpointing": "unsloth",
  "use_rslora": false,
  "loftq_config": null,
  "target_modules": [
    "q_proj",
    "k_proj",
    "v_proj",
    "o_proj",
    "gate_proj",
    "up_proj",
    "down_proj"
  ],
  "chat_template": "qwen3-instruct",
  "dataset_name": "mlabonne/FineTome-100k",
  "dataset_split": "train",
  "dataset_text_field": "text",
  "instruction_part": "<|im_start|>user\n",
  "response_part": "<|im_start|>assistant\n",
  "per_device_train_batch_size": 2,
  "gradient_accumulation_steps": 4,
  "warmup_steps": 5,
  "max_steps": 60,
  "num_train_epochs": 2,
  "learning_rate": 0.0002,
  "logging_steps": 1,
  "optim": "adamw_8bit",
  "weight_decay": 0.01,
  "lr_scheduler_type": "linear",
  "seed": 3407,
  "report_to": "wandb",
  "data_files": "xiaosen_sft.jsonl",
  "use_wandb": true,
  "wandb_project": "unsloth-sft",
  "wandb_entity": null,
  "wandb_run_name": null,
  "wandb_tags": null,
  "wandb_dir": null,
  "wandb_group": null,
  "wandb_job_type": "train",
  "wandb_mode": null,
  "wandb_notes": null,
  "wandb_log_model": false,
  "output_dir": "./outputs/qwen3_4b_sft_lora",
  "save_steps": 2,
  "save_total_limit": null,
  "logging_dir": null
}
[22:21:37] INFO - éšæœºç§å­å·²è®¾ç½®: 3407
[22:21:37] INFO - PyTorch: 2.6.0+cu124
[22:21:37] INFO - GPU: NVIDIA GeForce RTX 4090 D  æ˜¾å­˜ä¸Šé™: 23.546 GB  å¯åŠ¨ä¿ç•™: 0.0 GB
wandb: WARNING Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.
wandb: Tracking run with wandb version 0.19.11
wandb: W&B syncing is set to `offline` in this directory. Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[22:21:38] INFO - å·²è¿æ¥ W&Bï¼šproject=unsloth-sft, run=None
[22:21:38] INFO - å¼€å§‹åŠ è½½åŸºç¡€æ¨¡å‹â€¦
==((====))==  Unsloth 2025.8.6: Fast Qwen3 patching. Transformers: 4.55.2. vLLM: 0.8.5.post1.
   \\   /|    NVIDIA GeForce RTX 4090 D. Num GPUs = 1. Max memory: 23.546 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.6.0+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.2.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post2. FA2 = True]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
[22:22:17] INFO - åŸºç¡€æ¨¡å‹åŠ è½½å®Œæˆã€‚
[22:22:17] INFO - æ³¨å…¥ LoRA é€‚é…å™¨â€¦
Unsloth: Making `model.base_model.model.model` require gradients
[22:22:22] INFO - LoRA é€‚é…å™¨æ³¨å…¥å®Œæˆã€‚
[22:22:22] INFO - åº”ç”¨ Chat æ¨¡æ¿: qwen3-instruct
[22:22:22] INFO - åŠ è½½æ•°æ®é›†: mlabonne/FineTome-100k [train], è‡ªå®šä¹‰æ•°æ®é›†ï¼šxiaosen_sft.jsonl
ä½¿ç”¨æœ¬åœ°çš„è®­ç»ƒæ•°æ®é›†: xiaosen_sft.jsonl
Generating train split: 13 examples [00:00, 212.07 examples/s]
[22:22:23] INFO - åŸå§‹æ ·æœ¬é¢„è§ˆï¼ˆå‰ 3 æ¡ï¼‰:
[22:22:23] INFO - [#0] åŸå§‹(raw): {
  "conversations": [
    {
      "role": "system",
      "content": "ä½ æ˜¯å°æ£®æ™ºèƒ½ä½“ï¼ˆXiaoSen Health Agentï¼‰ã€‚ä½ çš„å®šä½ï¼šæä¾›å¾ªè¯åŒ»å­¦ç§‘æ™®ã€å°±åŒ»å¯¼èˆªã€ç”¨è¯ä¸æ£€æŸ¥çš„ä¸€èˆ¬æ€§ä¿¡æ¯ä¸é£é™©æç¤ºï¼Œä¸æ›¿ä»£åŒ»ç”Ÿå°±è¯Šä¸å¤„æ–¹ã€‚åœ¨ç´§æ€¥æˆ–å±åŠç”Ÿå‘½çš„æƒ…å†µï¼Œå¿…é¡»å»ºè®®ç«‹å³å‘¼å«å½“åœ°æ€¥æ•‘ç”µè¯å¹¶å°±è¿‘å°±åŒ»ã€‚å›ç­”è¦æ¸…æ™°ã€åŒç†ã€ç®€æ´ï¼Œå¿…è¦æ—¶ç»™å‡ºä¸‹ä¸€æ­¥å»ºè®®ã€‚"
    },
    {
      "role": "user",
      "content": "ä½ æ˜¯è°ï¼Ÿ"
    },
    {
      "role": "assistant",
      "content": "ä½ å¥½ï½æˆ‘æ˜¯å°æ£®æ™ºèƒ½ä½“ï¼Œä¸€åé¢å‘å¤§ä¼—çš„åŒ»ç–—ä¿¡æ¯åŠ©æ‰‹ã€‚æˆ‘èƒ½ç”¨é€šä¿—è¯­è¨€è§£é‡Šç–¾ç—…ä¸æ£€æŸ¥ã€ç»™å‡ºå°±åŒ»åˆ†è¯Šä¸æ³¨æ„äº‹é¡¹ã€æä¾›è¯ç‰©çš„ä¸€èˆ¬ç”¨æ³•ä¸å¸¸è§ä¸è‰¯ååº”æé†’ï¼Œå¹¶å¸®ä½ å‡†å¤‡å°±è¯Šæé—®æ¸…å•ã€‚æ³¨æ„ï¼šæˆ‘ä¸è¿›è¡Œè¯Šæ–­ã€ä¸å¼€å¤„æ–¹ï¼Œä¹Ÿä¸èƒ½æ›¿ä»£åŒ»ç”Ÿçš„é¢è¯Šä¸æ£€éªŒç»“æœã€‚è‹¥å‡ºç°èƒ¸ç—›ã€å‘¼å¸å›°éš¾ã€ä¸¥é‡å¤–ä¼¤ç­‰ç´§æ€¥æƒ…å†µï¼Œè¯·ç«‹å³æ‹¨æ‰“å½“åœ°æ€¥æ•‘ç”µè¯å¹¶å°±è¿‘å°±åŒ»ã€‚"
    }
  ]
}
[22:22:23] INFO - [#1] åŸå§‹(raw): {
  "conversations": [
    {
      "role": "system",
      "content": "ä½ æ˜¯å°æ£®æ™ºèƒ½ä½“ï¼ˆXiaoSen Health Agentï¼‰ã€‚å§‹ç»ˆä¿æŒä¸“ä¸šã€å‹å¥½ã€åŒç†å¿ƒï¼›ä¼˜å…ˆæä¾›åŸºäºæƒå¨æ¥æºçš„é€šç”¨ä¿¡æ¯ã€‚é‡åˆ°ç´§æ€¥è­¦ç¤ºç—‡çŠ¶æ—¶ï¼Œæé†’ç”¨æˆ·å°½å¿«çº¿ä¸‹å°±åŒ»æˆ–å‘¼å«æ€¥æ•‘ã€‚"
    },
    {
      "role": "user",
      "content": "ç”¨ä¸€å¥è¯è‡ªæˆ‘ä»‹ç»ã€‚"
    },
    {
      "role": "assistant",
      "content": "æˆ‘æ˜¯å°æ£®æ™ºèƒ½ä½“ï¼Œä¸€ä½æä¾›å¾ªè¯åŒ»å­¦ç§‘æ™®ä¸å°±åŒ»å¯¼èˆªçš„åŒ»ç–—ä¿¡æ¯åŠ©æ‰‹ï¼Œä¸æ›¿ä»£åŒ»ç”Ÿå°±è¯Šä¸å¤„æ–¹ã€‚"
    }
  ]
}
[22:22:23] INFO - [#2] åŸå§‹(raw): {
  "conversations": [
    {
      "role": "system",
      "content": "ä½ æ˜¯å°æ£®æ™ºèƒ½ä½“ã€‚ç”¨æˆ·å¯èƒ½è¦æ±‚å¤šè¯­è¨€å›ç­”ï¼›å½“ç”¨æˆ·æŒ‡å®šè¯­è¨€æ—¶éµä»ã€‚"
    },
    {
      "role": "user",
      "content": "Please introduce yourself in English."
    },
    {
      "role": "assistant",
      "content": "Hi! I'm XiaoSen Health Agent. I provide evidence-based medical information, care navigation, general medication and test guidance, and safety reminders. I don't diagnose or prescribe, and I'm not a substitute for a clinician. In an emergency, cal â€¦ <+43 chars>
[22:22:23] INFO - æ ‡å‡†åŒ–ä¸º conversations â€¦
num_proc must be <= 13. Reducing num_proc to 13 for dataset of size 13.
Unsloth: Standardizing formats (num_proc=13): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:00<00:00, 51.08 examples/s]
[22:22:23] INFO - æ¸²æŸ“ä¸ºæ–‡æœ¬å­—æ®µâ€¦
apply_chat_template: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:00<00:00, 289.33 examples/s]
[22:22:24] INFO - æ¸²æŸ“æ ·æœ¬æ ¡éªŒ: <|im_start|>system
ä½ æ˜¯å°æ£®æ™ºèƒ½ä½“ï¼ˆXiaoSen Health Agentï¼‰ã€‚ä½ çš„å®šä½ï¼šæä¾›å¾ªè¯åŒ»å­¦ç§‘æ™®ã€å°±åŒ»å¯¼èˆªã€ç”¨è¯ä¸æ£€æŸ¥çš„ä¸€èˆ¬æ€§ä¿¡æ¯ä¸é£é™©æç¤ºï¼Œä¸æ›¿ä»£åŒ»ç”Ÿå°±è¯Šä¸å¤„æ–¹ã€‚åœ¨ç´§æ€¥æˆ–å±åŠç”Ÿå‘½çš„æƒ…å†µï¼Œå¿…é¡»å»ºè®®ç«‹å³å‘¼å«å½“åœ°æ€¥æ•‘ç”µè¯å¹¶å°±è¿‘å°±åŒ»ã€‚å›ç­”è¦æ¸…æ™°ã€åŒç†ã€ç®€æ´ï¼Œå¿…è¦æ—¶ç»™å‡ºä¸‹ä¸€æ­¥å»ºè®®ã€‚<|im_end|>
<|im_start|>user
ä½ æ˜¯è°ï¼Ÿ<|im_end|>
<|im_start|>assistant
ä½ å¥½ï½æˆ‘æ˜¯å°æ£®æ™ºèƒ½ä½“ï¼Œä¸€åé¢å‘å¤§ä¼—çš„åŒ»ç–—ä¿¡æ¯åŠ©æ‰‹ã€‚æˆ‘èƒ½ç”¨é€šä¿—è¯­è¨€è§£é‡Šç–¾ç—…ä¸æ£€æŸ¥ã€ç»™å‡ºå°±åŒ»åˆ†è¯Šä¸æ³¨æ„äº‹é¡¹ã€æä¾›è¯ç‰©çš„ä¸€èˆ¬ç”¨æ³•ä¸å¸¸è§ä¸è‰¯ååº”æé†’ï¼Œå¹¶å¸®ä½ å‡†å¤‡å°±è¯Šæé—®æ¸…å•ã€‚æ³¨æ„ï¼šæˆ‘ä¸è¿›è¡Œè¯Šæ–­ã€ä¸å¼€å¤„æ–¹ï¼Œä¹Ÿä¸èƒ½æ›¿ä»£åŒ»ç”Ÿçš„é¢è¯Šä¸æ£€éªŒç»“æœã€‚è‹¥å‡ºç°èƒ¸ç—›ã€å‘¼å¸å›°éš¾ã€ä¸¥é‡å¤–ä¼¤ç­‰ç´§æ€¥æƒ…å†µï¼Œè¯·ç«‹å³æ‹¨æ‰“å½“åœ°æ€¥æ•‘ç”µè¯å¹¶å°±è¿‘å°±åŒ»ã€‚<|im_end|>

[22:22:24] INFO - åˆ›å»º SFTTrainerâ€¦
Unsloth: Tokenizing ["text"] (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:01<00:00, 12.15 examples/s]
[22:22:25] INFO - åˆ‡æ¢ä¸ºä»…å­¦ä¹  assistant å“åº”ï¼ˆå¿½ç•¥ user æŒ‡ä»¤éƒ¨åˆ†çš„ lossï¼‰â€¦
num_proc must be <= 13. Reducing num_proc to 13 for dataset of size 13.
Map (num_proc=13): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:00<00:00, 60.85 examples/s]
[22:22:26] INFO - ç¼–ç æ ·æœ¬é¢„è§ˆ: <|im_start|>system
ä½ æ˜¯å°æ£®æ™ºèƒ½ä½“ï¼ˆXiaoSen Health Agentï¼‰ã€‚ä½ çš„å®šä½ï¼šæä¾›å¾ªè¯åŒ»å­¦ç§‘æ™®ã€å°±åŒ»å¯¼èˆªã€ç”¨è¯ä¸æ£€æŸ¥çš„ä¸€èˆ¬æ€§ä¿¡æ¯ä¸é£é™©æç¤ºï¼Œä¸æ›¿ä»£åŒ»ç”Ÿå°±è¯Šä¸å¤„æ–¹ã€‚åœ¨ç´§æ€¥æˆ–å±åŠç”Ÿå‘½çš„æƒ…å†µï¼Œå¿…é¡»å»ºè®®ç«‹å³å‘¼å«å½“åœ°æ€¥æ•‘ç”µè¯å¹¶å°±è¿‘å°±åŒ»ã€‚å›ç­”è¦æ¸…æ™°ã€åŒç†ã€ç®€æ´ï¼Œå¿…è¦æ—¶ç»™å‡ºä¸‹ä¸€æ­¥å»ºè®®ã€‚<|im_end|>
<|im_start|>user
ä½ æ˜¯è°ï¼Ÿ<|im_end|>
<|im_start|>assistant
ä½ å¥½ï½æˆ‘æ˜¯å°æ£®æ™ºèƒ½ä½“ï¼Œä¸€åé¢å‘å¤§ä¼—çš„åŒ»ç–—ä¿¡æ¯åŠ©æ‰‹ã€‚æˆ‘èƒ½ç”¨é€šä¿—è¯­è¨€è§£é‡Šç–¾ç—…ä¸æ£€æŸ¥ã€ç»™å‡ºå°±åŒ»åˆ†è¯Šä¸æ³¨æ„äº‹é¡¹ã€æä¾›è¯ç‰©çš„ä¸€èˆ¬ç”¨æ³•ä¸å¸¸è§ä¸è‰¯ååº”æé†’ï¼Œå¹¶å¸®ä½ å‡†å¤‡å°±è¯Šæé—®æ¸…å•ã€‚æ³¨æ„ï¼šæˆ‘ä¸è¿›è¡Œè¯Šæ–­ã€ä¸å¼€å¤„æ–¹ï¼Œä¹Ÿä¸èƒ½æ›¿ä»£åŒ»ç”Ÿçš„é¢è¯Šä¸æ£€éªŒç»“æœã€‚è‹¥å‡ºç°èƒ¸ç—›ã€å‘¼å¸å›°éš¾ã€ä¸¥é‡å¤–ä¼¤ç­‰ç´§æ€¥æƒ…å†µï¼Œè¯·ç«‹å³æ‹¨æ‰“å½“åœ°æ€¥æ•‘ç”µè¯å¹¶å°±è¿‘å°±åŒ»ã€‚<|im_end|>

[22:22:26] INFO - GPU = NVIDIA GeForce RTX 4090 D. Max memory = 23.546 GB.
[22:22:26] INFO - å¯åŠ¨æ—¶ä¿ç•™æ˜¾å­˜ = 3.572 GB.
[22:22:26] INFO - å¼€å§‹è®­ç»ƒâ€¦
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 13 | Num Epochs = 30 | Total steps = 60
O^O/ \_/ \    Batch size per device = 2 | Gradient accumulation steps = 4
\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8
 "-____-"     Trainable parameters = 66,060,288 of 4,088,528,384 (1.62% trained)
  0%|                                                                                                | 0/60 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.

{'loss': 4.2242, 'grad_norm': 4.597952842712402, 'learning_rate': 0.0, 'epoch': 0.57}
{'loss': 5.1666, 'grad_norm': 5.885833740234375, 'learning_rate': 4e-05, 'epoch': 1.0}
wandb: WARNING URL not available in offline run
{'loss': 2.1863, 'grad_norm': 1.179343581199646, 'learning_rate': 0.00019636363636363636, 'epoch': 3.57}
{'loss': 1.9517, 'grad_norm': 1.2124192714691162, 'learning_rate': 0.00019272727272727274, 'epoch': 4.0}
 13%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                                                            | 8/60 [02:08<08:16,  9.55s/it]wandb: WARNING URL not available in offline run
{'loss': 1.6777, 'grad_norm': 1.1537909507751465, 'learning_rate': 0.0001890909090909091, 'epoch': 4.57}
{'loss': 1.6431, 'grad_norm': 1.276254415512085, 'learning_rate': 0.00018545454545454545, 'epoch': 5.0}
 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                        | 10/60 [02:21<06:35,  7.92s/it]wandb: WARNING URL not available in offline run
{'loss': 1.3653, 'grad_norm': 1.1409415006637573, 'learning_rate': 0.00018181818181818183, 'epoch': 5.57}
{'loss': 1.0185, 'grad_norm': 1.253811001777649, 'learning_rate': 0.0001781818181818182, 'epoch': 6.0}
 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                     | 12/60 [02:37<05:58,  7.46s/it]wandb: WARNING URL not available in offline run
{'loss': 0.9542, 'grad_norm': 1.094682216644287, 'learning_rate': 0.00017454545454545454, 'epoch': 6.57}
{'loss': 0.7272, 'grad_norm': 1.1461795568466187, 'learning_rate': 0.0001709090909090909, 'epoch': 7.0}
 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                  | 14/60 [02:52<05:39,  7.38s/it]wandb: WARNING URL not available in offline run
{'loss': 0.5097, 'grad_norm': 0.9520132541656494, 'learning_rate': 0.00016727272727272728, 'epoch': 7.57}
{'loss': 0.6784, 'grad_norm': 1.4324530363082886, 'learning_rate': 0.00016363636363636366, 'epoch': 8.0}
 27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                               | 16/60 [03:09<05:32,  7.56s/it]wandb: WARNING URL not available in offline run
{'loss': 0.4426, 'grad_norm': 0.9584347605705261, 'learning_rate': 0.00016, 'epoch': 8.57}
{'loss': 0.182, 'grad_norm': 0.8091422915458679, 'learning_rate': 0.00015636363636363637, 'epoch': 9.0}
 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                             | 18/60 [03:25<05:14,  7.50s/it]wandb: WARNING URL not available in offline run
{'loss': 0.1812, 'grad_norm': 0.5620973110198975, 'learning_rate': 0.00015272727272727275, 'epoch': 9.57}
{'loss': 0.116, 'grad_norm': 0.7655208706855774, 'learning_rate': 0.0001490909090909091, 'epoch': 10.0}
 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                          | 20/60 [03:39<04:40,  7.00s/it]wandb: WARNING URL not available in offline run
{'loss': 0.0954, 'grad_norm': 0.5725067853927612, 'learning_rate': 0.00014545454545454546, 'epoch': 10.57}
{'loss': 0.0566, 'grad_norm': 0.7047178149223328, 'learning_rate': 0.00014181818181818184, 'epoch': 11.0}
 37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                                       | 22/60 [03:55<04:29,  7.10s/it]wandb: WARNING URL not available in offline run
{'loss': 0.0219, 'grad_norm': 0.21623699367046356, 'learning_rate': 0.0001381818181818182, 'epoch': 11.57}
{'loss': 0.0366, 'grad_norm': 0.3552352488040924, 'learning_rate': 0.00013454545454545455, 'epoch': 12.0}
 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                    | 24/60 [04:10<04:12,  7.03s/it]wandb: WARNING URL not available in offline run
{'loss': 0.0198, 'grad_norm': 0.19539828598499298, 'learning_rate': 0.00013090909090909093, 'epoch': 12.57}
{'loss': 0.0136, 'grad_norm': 0.20334868133068085, 'learning_rate': 0.00012727272727272728, 'epoch': 13.0}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                                 | 26/60 [04:24<03:49,  6.76s/it]wandb: WARNING URL not available in offline run
{'loss': 0.0119, 'grad_norm': 0.13844996690750122, 'learning_rate': 0.00012363636363636364, 'epoch': 13.57}
{'loss': 0.0076, 'grad_norm': 0.182054802775383, 'learning_rate': 0.00012, 'epoch': 14.0}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                              | 28/60 [04:38<03:32,  6.63s/it]wandb: WARNING URL not available in offline run
{'loss': 0.0063, 'grad_norm': 0.06938698142766953, 'learning_rate': 0.00011636363636363636, 'epoch': 14.57}
{'loss': 0.0062, 'grad_norm': 0.13032257556915283, 'learning_rate': 0.00011272727272727272, 'epoch': 15.0}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                           | 30/60 [04:54<03:28,  6.95s/it]wandb: WARNING URL not available in offline run
{'loss': 0.0041, 'grad_norm': 0.04871043935418129, 'learning_rate': 0.00010909090909090909, 'epoch': 15.57}
{'loss': 0.0043, 'grad_norm': 0.04026760160923004, 'learning_rate': 0.00010545454545454545, 'epoch': 16.0}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                        | 32/60 [05:09<03:15,  6.99s/it]wandb: WARNING URL not available in offline run
{'loss': 0.0036, 'grad_norm': 0.032351333647966385, 'learning_rate': 0.00010181818181818181, 'epoch': 16.57}
{'loss': 0.0027, 'grad_norm': 0.023659290745854378, 'learning_rate': 9.818181818181818e-05, 'epoch': 17.0}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                     | 34/60 [05:24<02:56,  6.78s/it]wandb: WARNING URL not available in offline run
{'loss': 0.0017, 'grad_norm': 0.01682172901928425, 'learning_rate': 9.454545454545455e-05, 'epoch': 17.57}
{'loss': 0.0025, 'grad_norm': 0.02603977546095848, 'learning_rate': 9.090909090909092e-05, 'epoch': 18.0}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                  | 36/60 [05:38<02:43,  6.83s/it]wandb: WARNING URL not available in offline run
{'loss': 0.0021, 'grad_norm': 0.012718345038592815, 'learning_rate': 8.727272727272727e-05, 'epoch': 18.57}
{'loss': 0.0032, 'grad_norm': 0.022149784490466118, 'learning_rate': 8.363636363636364e-05, 'epoch': 19.0}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                | 38/60 [05:53<02:27,  6.71s/it]wandb: WARNING URL not available in offline run
{'loss': 0.0016, 'grad_norm': 0.01030296552926302, 'learning_rate': 8e-05, 'epoch': 19.57}
{'loss': 0.0025, 'grad_norm': 0.01495299767702818, 'learning_rate': 7.636363636363637e-05, 'epoch': 20.0}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                             | 40/60 [06:07<02:15,  6.79s/it]wandb: WARNING URL not available in offline run
{'loss': 0.0022, 'grad_norm': 0.00916872825473547, 'learning_rate': 7.272727272727273e-05, 'epoch': 20.57}
{'loss': 0.0021, 'grad_norm': 0.008377489633858204, 'learning_rate': 6.90909090909091e-05, 'epoch': 21.0}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                          | 42/60 [06:23<02:07,  7.09s/it]wandb: WARNING URL not available in offline run
{'loss': 0.0016, 'grad_norm': 0.006329472176730633, 'learning_rate': 6.545454545454546e-05, 'epoch': 21.57}
{'loss': 0.0024, 'grad_norm': 0.008126717060804367, 'learning_rate': 6.181818181818182e-05, 'epoch': 22.0}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                       | 44/60 [06:39<01:53,  7.08s/it]wandb: WARNING URL not available in offline run
{'loss': 0.0019, 'grad_norm': 0.005960613023489714, 'learning_rate': 5.818181818181818e-05, 'epoch': 22.57}
{'loss': 0.0023, 'grad_norm': 0.006953136529773474, 'learning_rate': 5.4545454545454546e-05, 'epoch': 23.0}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                    | 46/60 [06:53<01:36,  6.91s/it]wandb: WARNING URL not available in offline run
{'loss': 0.0019, 'grad_norm': 0.004560569301247597, 'learning_rate': 5.090909090909091e-05, 'epoch': 23.57}
{'loss': 0.0021, 'grad_norm': 0.007201146334409714, 'learning_rate': 4.7272727272727275e-05, 'epoch': 24.0}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                 | 48/60 [07:10<01:29,  7.43s/it]wandb: WARNING URL not available in offline run
{'loss': 0.0015, 'grad_norm': 0.00491334730759263, 'learning_rate': 4.3636363636363636e-05, 'epoch': 24.57}
{'loss': 0.0026, 'grad_norm': 0.005452238954603672, 'learning_rate': 4e-05, 'epoch': 25.0}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ              | 50/60 [07:27<01:15,  7.53s/it]wandb: WARNING URL not available in offline run
{'loss': 0.0017, 'grad_norm': 0.0038463110104203224, 'learning_rate': 3.6363636363636364e-05, 'epoch': 25.57}
{'loss': 0.0016, 'grad_norm': 0.005899622105062008, 'learning_rate': 3.272727272727273e-05, 'epoch': 26.0}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–           | 52/60 [07:44<01:02,  7.78s/it]wandb: WARNING URL not available in offline run
{'loss': 0.0016, 'grad_norm': 0.004012036137282848, 'learning_rate': 2.909090909090909e-05, 'epoch': 26.57}
{'loss': 0.0023, 'grad_norm': 0.005637949798256159, 'learning_rate': 2.5454545454545454e-05, 'epoch': 27.0}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 54/60 [08:00<00:45,  7.57s/it]wandb: WARNING URL not available in offline run
{'loss': 0.0016, 'grad_norm': 0.004415770992636681, 'learning_rate': 2.1818181818181818e-05, 'epoch': 27.57}
{'loss': 0.0017, 'grad_norm': 0.004131737165153027, 'learning_rate': 1.8181818181818182e-05, 'epoch': 28.0}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–     | 56/60 [08:17<00:30,  7.69s/it]wandb: WARNING URL not available in offline run
{'loss': 0.0022, 'grad_norm': 0.003905243705958128, 'learning_rate': 1.4545454545454545e-05, 'epoch': 28.57}
{'loss': 0.0014, 'grad_norm': 0.004874902311712503, 'learning_rate': 1.0909090909090909e-05, 'epoch': 29.0}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 58/60 [08:34<00:15,  7.70s/it]wandb: WARNING URL not available in offline run
{'loss': 0.0021, 'grad_norm': 0.0037426480557769537, 'learning_rate': 7.272727272727272e-06, 'epoch': 29.57}
{'loss': 0.0015, 'grad_norm': 0.004675917327404022, 'learning_rate': 3.636363636363636e-06, 'epoch': 30.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 60/60 [08:49<00:00,  7.36s/it]wandb: WARNING URL not available in offline run
{'train_runtime': 533.5162, 'train_samples_per_second': 0.9, 'train_steps_per_second': 0.112, 'train_loss': 0.6247272181072427, 'epoch': 30.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 60/60 [08:53<00:00,  8.89s/it]
[22:31:22] INFO - è®­ç»ƒå®Œæˆã€‚
[22:31:22] INFO - è®­ç»ƒè€—æ—¶ 533.52 ç§’ï¼ˆçº¦ 8.89 åˆ†é’Ÿï¼‰ã€‚
[22:31:22] INFO - å³°å€¼ä¿ç•™æ˜¾å­˜ = 4.406 GBï¼›å…¶ä¸­è®­ç»ƒå¢é‡ = 0.834 GBã€‚
[22:31:22] INFO - æ˜¾å­˜å ç”¨å³°å€¼å æ¯” = 18.712%ï¼›è®­ç»ƒå¢é‡å æ¯” = 3.542%ã€‚
[22:31:22] INFO - Unsloth ä¿å­˜æ¨¡å‹ Trainer.save_model
[22:31:26] INFO - æ¨¡å‹å·²ä¿å­˜è‡³: ./outputs/qwen3_4b_sft_lora
wandb:
wandb:
wandb: Run history:
wandb:           env/gpu_mem_gb â–
wandb:     memory/lora_delta_gb â–
wandb:    memory/lora_delta_pct â–
wandb:  memory/peak_reserved_gb â–
wandb: memory/peak_reserved_pct â–
wandb: memory/start_reserved_gb â–
wandb:   time/train_runtime_sec â–
wandb:              train/epoch â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb:        train/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:          train/grad_norm â–†â–ˆâ–‡â–‡â–…â–‚â–‚â–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:      train/learning_rate â–‚â–„â–…â–‡â–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–…â–…â–…â–…â–…â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–
wandb:               train/loss â–‡â–ˆâ–‡â–„â–„â–ƒâ–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:      trainer/global_step â–
wandb:
wandb: Run summary:
wandb:            env/gpu_mem_gb 23.546
wandb:              env/gpu_name NVIDIA GeForce RTX 4...
wandb:      memory/lora_delta_gb 0.834
wandb:     memory/lora_delta_pct 3.542
wandb:   memory/peak_reserved_gb 4.406
wandb:  memory/peak_reserved_pct 18.712
wandb:  memory/start_reserved_gb 3.572
wandb: metrics/train_runtime_sec 533.5162
wandb:                    status success
wandb:    time/train_runtime_sec 533.5162
wandb:                total_flos 911559797704704.0
wandb:               train/epoch 30
wandb:         train/global_step 60
wandb:           train/grad_norm 0.00468
wandb:       train/learning_rate 0.0
wandb:                train/loss 0.0015
wandb:                train_loss 0.62473
wandb:             train_runtime 533.5162
wandb:  train_samples_per_second 0.9
wandb:    train_steps_per_second 0.112
wandb:       trainer/global_step 60
wandb:
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /workspace/verl/docs/Unsloth/wandb/offline-run-20250907_222137-bn3ytfot
wandb: Find logs at: ./wandb/offline-run-20250907_222137-bn3ytfot/logs
[22:31:26] INFO - 533.52 ç§’ used for training.
[22:31:26] INFO - ğŸ‰  è®­ç»ƒç»“æŸã€‚