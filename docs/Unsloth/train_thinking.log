/opt/conda/lib/python3.11/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
WANDB_AVAILABLEæ˜¯å¯ç”¨çš„ï¼Œå·²ç»å®‰è£…äº†wandb
ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.
/opt/conda/lib/python3.11/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-09-07 09:23:50,444] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
INFO 09-07 09:23:51 [__init__.py:241] Automatically detected platform cuda.
Unsloth: Your Flash Attention 2 installation seems to be broken?
A possible explanation is you have a new CUDA version which isn't
yet compatible with FA2? Please file a ticket to Unsloth or FA2.
We shall now use Xformers instead, which does not have any performance hits!
We found this negligible impact by benchmarking on 1x A100.
ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!
[09:23:56] INFO - æ—¥å¿—å†™å…¥: ./outputs/qwen3_4b_thinking_lora/logs/train_20250907_092356.log
[09:23:56] INFO - ===== è®­ç»ƒé…ç½® =====
[09:23:56] INFO - {
  "model_name": "unsloth/Qwen3-4B-Thinking-2507",
  "max_seq_length": 2048,
  "load_in_4bit": true,
  "load_in_8bit": false,
  "full_finetuning": false,
  "hf_token": null,
  "lora_r": 32,
  "lora_alpha": 32,
  "lora_dropout": 0.0,
  "bias": "none",
  "use_gradient_checkpointing": "unsloth",
  "use_rslora": false,
  "loftq_config": null,
  "target_modules": [
    "q_proj",
    "k_proj",
    "v_proj",
    "o_proj",
    "gate_proj",
    "up_proj",
    "down_proj"
  ],
  "chat_template": "qwen3-thinking",
  "dataset_name": "unsloth/OpenMathReasoning-mini",
  "dataset_split": "train",
  "dataset_text_field": "text",
  "instruction_part": "<|im_start|>user\n",
  "response_part": "<|im_start|>assistant\n",
  "per_device_train_batch_size": 2,
  "gradient_accumulation_steps": 4,
  "warmup_steps": 5,
  "max_steps": 60,
  "num_train_epochs": 2,
  "learning_rate": 0.0002,
  "logging_steps": 1,
  "optim": "adamw_8bit",
  "weight_decay": 0.01,
  "lr_scheduler_type": "linear",
  "seed": 3407,
  "report_to": "wandb",
  "use_wandb": true,
  "wandb_project": "unsloth-sft",
  "wandb_entity": null,
  "wandb_run_name": null,
  "wandb_tags": null,
  "wandb_dir": null,
  "wandb_group": null,
  "wandb_job_type": "train",
  "wandb_mode": null,
  "wandb_notes": null,
  "wandb_log_model": false,
  "output_dir": "./outputs/qwen3_4b_thinking_lora",
  "save_steps": 2,
  "save_total_limit": null,
  "logging_dir": null
}
[09:23:56] INFO - éšæœºç§å­å·²è®¾ç½®: 3407
[09:23:56] INFO - PyTorch: 2.7.1+cu126
[09:23:56] INFO - GPU: NVIDIA GeForce RTX 4090 D  æ˜¾å­˜ä¸Šé™: 23.546 GB  å¯åŠ¨ä¿ç•™: 0.0 GB
wandb: WARNING Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.
wandb: Tracking run with wandb version 0.21.3
wandb: W&B syncing is set to `offline` in this directory. Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
wandb: Run data is saved locally in /workspace/verl/tmp/wandb/offline-run-20250907_092356-9pv4qwic
[09:23:56] INFO - å·²è¿æ¥ W&Bï¼šproject=unsloth-sft, run=None
[09:23:56] INFO - å¼€å§‹åŠ è½½åŸºç¡€æ¨¡å‹â€¦
==((====))==  Unsloth 2025.9.1: Fast Qwen3 patching. Transformers: 4.55.4. vLLM: 0.10.1.1.
   \\   /|    NVIDIA GeForce RTX 4090 D. Num GPUs = 1. Max memory: 23.546 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.7.1+cu126. CUDA: 8.9. CUDA Toolkit: 12.6. Triton: 3.3.1
\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
[09:24:16] INFO - åŸºç¡€æ¨¡å‹åŠ è½½å®Œæˆã€‚
[09:24:16] INFO - æ³¨å…¥ LoRA é€‚é…å™¨â€¦
Unsloth: Making `model.base_model.model.model` require gradients
[09:24:20] INFO - LoRA é€‚é…å™¨æ³¨å…¥å®Œæˆã€‚
[09:24:20] INFO - åº”ç”¨ Chat æ¨¡æ¿: qwen3-thinking
[09:24:20] INFO - åŠ è½½æ•°æ®é›†: unsloth/OpenMathReasoning-mini [train] â€¦
build_conversations: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19252/19252 [00:03<00:00, 5898.48 examples/s]
apply_chat_template: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19252/19252 [00:07<00:00, 2517.09 examples/s]
[09:27:14] INFO - æ ·æœ¬æ ¡éªŒ: <|im_start|>user
Given $\sqrt{x^2+165}-\sqrt{x^2-52}=7$ and $x$ is positive, find all possible values of $x$.<|im_end|>
<|im_start|>assistant
<think>
Okay, let's see. I need to solve the equation âˆš(xÂ² + 165) - âˆš(xÂ² - 52) = 7, and find all positive values of x. Hmm, radicals can be tricky, but maybe if I can eliminate the square roots by squaring both sides. Let me try that.

First, let me write down the equation again to make sure I have it right:

âˆš(xÂ² + 165) - âˆš(xÂ² - 52) = 7.

Okay, so the idea is to isolate one of the radicals and then square both sides. Let me try moving the second radical to the other side:

âˆš(xÂ² + 165) = 7 + âˆš(xÂ² - 52).

Now, if I square both sides, maybe I can get rid of the square roots. Let's do that:

(âˆš(xÂ² + 165))Â² = (7 + âˆš(xÂ² - 52))Â².

Simplifying the left side:

xÂ² + 165 = 49 + 14âˆš(xÂ² - 52) + (âˆš(xÂ² - 52))Â².

The right side is expanded using the formula (a + b)Â² = aÂ² + 2ab + bÂ². So the right side becomes 7Â² + 2*7*âˆš(xÂ² - 52) + (âˆš(xÂ² - 52))Â², which is 49 + 14 â€¦ <+2977 chars>
[09:27:14] INFO - åˆ›å»º SFTTrainerâ€¦
/workspace/verl/tmp/unsloth_compiled_cache/UnslothSFTTrainer.py:581: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
  warnings.warn(
Map (num_proc=92): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19252/19252 [00:19<00:00, 981.31 examples/s]
/workspace/verl/tmp/unsloth_compiled_cache/UnslothSFTTrainer.py:714: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `UnslothSFTTrainer.__init__`. Use `processing_class` instead.
  super().__init__(
[09:27:37] INFO - åˆ‡æ¢ä¸ºä»…å­¦ä¹  assistant å“åº”ï¼ˆå¿½ç•¥ user æŒ‡ä»¤éƒ¨åˆ†çš„ lossï¼‰â€¦
Map (num_proc=88): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19252/19252 [00:03<00:00, 5733.27 examples/s]
[09:27:43] INFO - ç¼–ç æ ·æœ¬é¢„è§ˆ: <|im_start|>user
Given $\sqrt{x^2+165}-\sqrt{x^2-52}=7$ and $x$ is positive, find all possible values of $x$.<|im_end|>
<|im_start|>assistant
<think>
Okay, let's see. I need to solve the equation âˆš(xÂ² + 165) - âˆš(xÂ² - 52) = 7, and find all positive values of x. Hmm, radicals can be tricky, but maybe if I can eliminate the square roots by squaring both sides. Let me try that.

First, let me write down the equation again to make sure I have it right:

âˆš(xÂ² + 165) - âˆš(xÂ² - 52) = 7.

Okay, so the idea is to isolate one of the radicals and then square both sides. Let me try moving the second radical to the other side:

âˆš(xÂ² + 165) = 7 + âˆš(xÂ² - 52).

Now, if I square both sides, maybe I can get rid of the square roots. Let's do that:

(âˆš(xÂ² + 165))Â² = (7 + ï¿½
[09:27:43] INFO - GPU = NVIDIA GeForce RTX 4090 D. Max memory = 23.546 GB.
[09:27:43] INFO - å¯åŠ¨æ—¶ä¿ç•™æ˜¾å­˜ = 3.816 GB.
[09:27:43] INFO - å¼€å§‹è®­ç»ƒâ€¦
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 19,252 | Num Epochs = 1 | Total steps = 60
O^O/ \_/ \    Batch size per device = 2 | Gradient accumulation steps = 4
\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8
 "-____-"     Trainable parameters = 66,060,288 of 4,088,528,384 (1.62% trained)
  0%|                                                                  | 0/60 [00:00<?, ?it/s]Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 0.5885, 'grad_norm': 0.3728635311126709, 'learning_rate': 0.0, 'epoch': 0.0}
{'loss': 0.5195, 'grad_norm': 0.32497748732566833, 'learning_rate': 4e-05, 'epoch': 0.0}
{'loss': 0.6243, 'grad_norm': 0.4116409420967102, 'learning_rate': 8e-05, 'epoch': 0.0}
{'loss': 0.5617, 'grad_norm': 0.3256601393222809, 'learning_rate': 0.00012, 'epoch': 0.0}
{'loss': 0.5198, 'grad_norm': 0.2857953608036041, 'learning_rate': 0.00016, 'epoch': 0.0}
{'loss': 0.4536, 'grad_norm': 0.21673889458179474, 'learning_rate': 0.0002, 'epoch': 0.0}
{'loss': 0.3715, 'grad_norm': 0.13393908739089966, 'learning_rate': 0.00019636363636363636, 'epoch': 0.0}
{'loss': 0.3917, 'grad_norm': 0.1514512002468109, 'learning_rate': 0.00019272727272727274, 'epoch': 0.0}
{'loss': 0.4791, 'grad_norm': 0.21656976640224457, 'learning_rate': 0.0001890909090909091, 'epoch': 0.0}
{'loss': 0.4312, 'grad_norm': 0.1977001279592514, 'learning_rate': 0.00018545454545454545, 'epoch': 0.0}
{'loss': 0.5172, 'grad_norm': 0.19072803854942322, 'learning_rate': 0.00018181818181818183, 'epoch': 0.0}
{'loss': 0.4447, 'grad_norm': 0.24327407777309418, 'learning_rate': 0.0001781818181818182, 'epoch': 0.0}
 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                             | 12/60 [01:56<06:30,  8.14s/it]
âˆš(xÂ² + 165) - âˆš(xÂ² - 52) = 7.

Okay, so the idea is to isolate one of the radicals and then square both sides. Let me try moving the second radical to the other side:

âˆš(xÂ² + 165) = 7 + âˆš(xÂ² - 52).

Now, if I square both sides, maybe I can get rid of the square roots. Let's do that:

(âˆš(xÂ² + 165))Â² = (7 + ï¿½
[09:27:43] INFO - GPU = NVIDIA GeForce RTX 4090 D. Max memory = 23.546 GB.
[09:27:43] INFO - å¯åŠ¨æ—¶ä¿ç•™æ˜¾å­˜ = 3.816 GB.
[09:27:43] INFO - å¼€å§‹è®­ç»ƒâ€¦
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 19,252 | Num Epochs = 1 | Total steps = 60
O^O/ \_/ \    Batch size per device = 2 | Gradient accumulation steps = 4
\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8
 "-____-"     Trainable parameters = 66,060,288 of 4,088,528,384 (1.62% trained)
  0%|                                                                  | 0/60 [00:00<?, ?it/s]Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 0.5885, 'grad_norm': 0.3728635311126709, 'learning_rate': 0.0, 'epoch': 0.0}
{'loss': 0.5195, 'grad_norm': 0.32497748732566833, 'learning_rate': 4e-05, 'epoch': 0.0}
{'loss': 0.6243, 'grad_norm': 0.4116409420967102, 'learning_rate': 8e-05, 'epoch': 0.0}
{'loss': 0.5617, 'grad_norm': 0.3256601393222809, 'learning_rate': 0.00012, 'epoch': 0.0}
{'loss': 0.5198, 'grad_norm': 0.2857953608036041, 'learning_rate': 0.00016, 'epoch': 0.0}
{'loss': 0.4536, 'grad_norm': 0.21673889458179474, 'learning_rate': 0.0002, 'epoch': 0.0}
{'loss': 0.3715, 'grad_norm': 0.13393908739089966, 'learning_rate': 0.00019636363636363636, 'epoch': 0.0}
{'loss': 0.3917, 'grad_norm': 0.1514512002468109, 'learning_rate': 0.00019272727272727274, 'epoch': 0.0}
{'loss': 0.4791, 'grad_norm': 0.21656976640224457, 'learning_rate': 0.0001890909090909091, 'epoch': 0.0}
{'loss': 0.4312, 'grad_norm': 0.1977001279592514, 'learning_rate': 0.00018545454545454545, 'epoch': 0.0}
{'loss': 0.5172, 'grad_norm': 0.19072803854942322, 'learning_rate': 0.00018181818181818183, 'epoch': 0.0}
{'loss': 0.4447, 'grad_norm': 0.24327407777309418, 'learning_rate': 0.0001781818181818182, 'epoch': 0.0}
 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                             | 12/60 [01:56<06:30,  8.14s/it]
First, let me write down the equation again to make sure I have it right:

âˆš(xÂ² + 165) - âˆš(xÂ² - 52) = 7.

Okay, so the idea is to isolate one of the radicals and then square both sides. Let me try moving the second radical to the other side:

âˆš(xÂ² + 165) = 7 + âˆš(xÂ² - 52).

Now, if I square both sides, maybe I can get rid of the square roots. Let's do that:

(âˆš(xÂ² + 165))Â² = (7 + ï¿½
[09:27:43] INFO - GPU = NVIDIA GeForce RTX 4090 D. Max memory = 23.546 GB.
[09:27:43] INFO - å¯åŠ¨æ—¶ä¿ç•™æ˜¾å­˜ = 3.816 GB.
[09:27:43] INFO - å¼€å§‹è®­ç»ƒâ€¦
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 19,252 | Num Epochs = 1 | Total steps = 60
O^O/ \_/ \    Batch size per device = 2 | Gradient accumulation steps = 4
\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8
 "-____-"     Trainable parameters = 66,060,288 of 4,088,528,384 (1.62% trained)
  0%|                                                                  | 0/60 [00:00<?, ?it/s]Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 0.5885, 'grad_norm': 0.3728635311126709, 'learning_rate': 0.0, 'epoch': 0.0}
{'loss': 0.5195, 'grad_norm': 0.32497748732566833, 'learning_rate': 4e-05, 'epoch': 0.0}
{'loss': 0.6243, 'grad_norm': 0.4116409420967102, 'learning_rate': 8e-05, 'epoch': 0.0}
{'loss': 0.5617, 'grad_norm': 0.3256601393222809, 'learning_rate': 0.00012, 'epoch': 0.0}
{'loss': 0.5198, 'grad_norm': 0.2857953608036041, 'learning_rate': 0.00016, 'epoch': 0.0}
{'loss': 0.4536, 'grad_norm': 0.21673889458179474, 'learning_rate': 0.0002, 'epoch': 0.0}
{'loss': 0.3715, 'grad_norm': 0.13393908739089966, 'learning_rate': 0.00019636363636363636, 'epoch': 0.0}
{'loss': 0.3917, 'grad_norm': 0.1514512002468109, 'learning_rate': 0.00019272727272727274, 'epoch': 0.0}
{'loss': 0.4791, 'grad_norm': 0.21656976640224457, 'learning_rate': 0.0001890909090909091, 'epoch': 0.0}
{'loss': 0.4312, 'grad_norm': 0.1977001279592514, 'learning_rate': 0.00018545454545454545, 'epoch': 0.0}
{'loss': 0.5172, 'grad_norm': 0.19072803854942322, 'learning_rate': 0.00018181818181818183, 'epoch': 0.0}
{'loss': 0.4447, 'grad_norm': 0.24327407777309418, 'learning_rate': 0.0001781818181818182, 'epoch': 0.0}
{'loss': 0.4325, 'grad_norm': 0.12044846266508102, 'learning_rate': 0.00017454545454545454, 'epoch': 0.01}
{'loss': 0.3773, 'grad_norm': 0.2367749959230423, 'learning_rate': 0.0001709090909090909, 'epoch': 0.01}
{'loss': 0.4497, 'grad_norm': 0.09639908373355865, 'learning_rate': 0.00016727272727272728, 'epoch': 0.01}
{'loss': 0.4298, 'grad_norm': 0.08616502583026886, 'learning_rate': 0.00016363636363636366, 'epoch': 0.01}
{'loss': 0.3898, 'grad_norm': 0.0893850103020668, 'learning_rate': 0.00016, 'epoch': 0.01}
{'loss': 0.4375, 'grad_norm': 0.10908949375152588, 'learning_rate': 0.00015636363636363637, 'epoch': 0.01}
{'loss': 0.3955, 'grad_norm': 0.10200177878141403, 'learning_rate': 0.00015272727272727275, 'epoch': 0.01}
{'loss': 0.4274, 'grad_norm': 0.1007433757185936, 'learning_rate': 0.0001490909090909091, 'epoch': 0.01}
{'loss': 0.3847, 'grad_norm': 0.09955862909555435, 'learning_rate': 0.00014545454545454546, 'epoch': 0.01}
{'loss': 0.3727, 'grad_norm': 0.08965104818344116, 'learning_rate': 0.00014181818181818184, 'epoch': 0.01}
{'loss': 0.3685, 'grad_norm': 0.08066105842590332, 'learning_rate': 0.0001381818181818182, 'epoch': 0.01}
{'loss': 0.4568, 'grad_norm': 0.08035139739513397, 'learning_rate': 0.00013454545454545455, 'epoch': 0.01}
{'loss': 0.3783, 'grad_norm': 0.3455890119075775, 'learning_rate': 0.00013090909090909093, 'epoch': 0.01}
{'loss': 0.3518, 'grad_norm': 0.07157354056835175, 'learning_rate': 0.00012727272727272728, 'epoch': 0.01}
{'loss': 0.4501, 'grad_norm': 0.08351033926010132, 'learning_rate': 0.00012363636363636364, 'epoch': 0.01}
{'loss': 0.3668, 'grad_norm': 0.07189388573169708, 'learning_rate': 0.00012, 'epoch': 0.01}
{'loss': 0.4511, 'grad_norm': 0.08757680654525757, 'learning_rate': 0.00011636363636363636, 'epoch': 0.01}
{'loss': 0.3804, 'grad_norm': 0.07819052040576935, 'learning_rate': 0.00011272727272727272, 'epoch': 0.01}
{'loss': 0.3667, 'grad_norm': 0.07500045001506805, 'learning_rate': 0.00010909090909090909, 'epoch': 0.01}
{'loss': 0.4059, 'grad_norm': 0.08117512613534927, 'learning_rate': 0.00010545454545454545, 'epoch': 0.01}
{'loss': 0.3501, 'grad_norm': 0.0675412192940712, 'learning_rate': 0.00010181818181818181, 'epoch': 0.01}
{'loss': 0.397, 'grad_norm': 0.07944927364587784, 'learning_rate': 9.818181818181818e-05, 'epoch': 0.01}
{'loss': 0.3728, 'grad_norm': 0.07788346707820892, 'learning_rate': 9.454545454545455e-05, 'epoch': 0.01}
{'loss': 0.3559, 'grad_norm': 0.07076675444841385, 'learning_rate': 9.090909090909092e-05, 'epoch': 0.01}
{'loss': 0.4131, 'grad_norm': 0.07196515798568726, 'learning_rate': 8.727272727272727e-05, 'epoch': 0.02}
{'loss': 0.3555, 'grad_norm': 0.06998749822378159, 'learning_rate': 8.363636363636364e-05, 'epoch': 0.02}
{'loss': 0.379, 'grad_norm': 0.7997954487800598, 'learning_rate': 8e-05, 'epoch': 0.02}
{'loss': 0.432, 'grad_norm': 0.07384613901376724, 'learning_rate': 7.636363636363637e-05, 'epoch': 0.02}
{'loss': 0.4654, 'grad_norm': 0.08194660395383835, 'learning_rate': 7.272727272727273e-05, 'epoch': 0.02}
{'loss': 0.481, 'grad_norm': 0.08362309634685516, 'learning_rate': 6.90909090909091e-05, 'epoch': 0.02}
{'loss': 0.4279, 'grad_norm': 0.0755702555179596, 'learning_rate': 6.545454545454546e-05, 'epoch': 0.02}
{'loss': 0.3572, 'grad_norm': 0.07029201090335846, 'learning_rate': 6.181818181818182e-05, 'epoch': 0.02}
{'loss': 0.4555, 'grad_norm': 0.07548838108778, 'learning_rate': 5.818181818181818e-05, 'epoch': 0.02}
{'loss': 0.3604, 'grad_norm': 0.06944432109594345, 'learning_rate': 5.4545454545454546e-05, 'epoch': 0.02}
{'loss': 0.333, 'grad_norm': 0.0702856108546257, 'learning_rate': 5.090909090909091e-05, 'epoch': 0.02}
{'loss': 0.4162, 'grad_norm': 0.07340552657842636, 'learning_rate': 4.7272727272727275e-05, 'epoch': 0.02}
{'loss': 0.3536, 'grad_norm': 0.06919131428003311, 'learning_rate': 4.3636363636363636e-05, 'epoch': 0.02}
{'loss': 0.3816, 'grad_norm': 0.07057007402181625, 'learning_rate': 4e-05, 'epoch': 0.02}
{'loss': 0.3951, 'grad_norm': 0.07897971570491791, 'learning_rate': 3.6363636363636364e-05, 'epoch': 0.02}
{'loss': 0.3674, 'grad_norm': 0.06707562506198883, 'learning_rate': 3.272727272727273e-05, 'epoch': 0.02}
{'loss': 0.35, 'grad_norm': 0.07267999649047852, 'learning_rate': 2.909090909090909e-05, 'epoch': 0.02}
{'loss': 0.3098, 'grad_norm': 0.06467854231595993, 'learning_rate': 2.5454545454545454e-05, 'epoch': 0.02}
{'loss': 0.4266, 'grad_norm': 0.07776881009340286, 'learning_rate': 2.1818181818181818e-05, 'epoch': 0.02}
{'loss': 0.3716, 'grad_norm': 0.07623150944709778, 'learning_rate': 1.8181818181818182e-05, 'epoch': 0.02}
{'loss': 0.3791, 'grad_norm': 0.07647047936916351, 'learning_rate': 1.4545454545454545e-05, 'epoch': 0.02}
[09:36:47] INFO - è®­ç»ƒå®Œæˆã€‚
[09:36:47] INFO - è®­ç»ƒè€—æ—¶ 543.06 ç§’ï¼ˆçº¦ 9.05 åˆ†é’Ÿï¼‰ã€‚
[09:36:47] INFO - å³°å€¼ä¿ç•™æ˜¾å­˜ = 5.225 GBï¼›å…¶ä¸­è®­ç»ƒå¢é‡ = 1.409 GBã€‚
[09:36:47] INFO - æ˜¾å­˜å ç”¨å³°å€¼å æ¯” = 22.191%ï¼›è®­ç»ƒå¢é‡å æ¯” = 5.984%ã€‚
[09:36:47] INFO - Unsloth ä¿å­˜æ¨¡å‹ Trainer.save_model
[09:36:49] INFO - æ¨¡å‹å·²ä¿å­˜è‡³: ./outputs/qwen3_4b_thinking_lora
wandb:
wandb:
wandb: Run history:
wandb:           env/gpu_mem_gb â–
wandb:     memory/lora_delta_gb â–
wandb:    memory/lora_delta_pct â–
wandb:  memory/peak_reserved_gb â–
wandb: memory/peak_reserved_pct â–
wandb: memory/start_reserved_gb â–
wandb:   time/train_runtime_sec â–
wandb:              train/epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:        train/global_step â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:          train/grad_norm â–„â–„â–ƒâ–‚â–‚â–‚â–ƒâ–‚â–ƒâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                       +3 ...
wandb:
wandb: Run summary:
wandb:            env/gpu_mem_gb 23.546
wandb:              env/gpu_name NVIDIA GeForce RTX 4...
wandb:      memory/lora_delta_gb 1.409
wandb:     memory/lora_delta_pct 5.984
wandb:   memory/peak_reserved_gb 5.225
wandb:  memory/peak_reserved_pct 22.191
wandb:  memory/start_reserved_gb 3.816
wandb: metrics/train_runtime_sec 543.0624
wandb:                    status success
wandb:    time/train_runtime_sec 543.0624
wandb:                       +11 ...
wandb:
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /workspace/verl/tmp/wandb/offline-run-20250907_092625-wbecf2z9
wandb: Find logs at: ./wandb/offline-run-20250907_092625-wbecf2z9/logs
[09:36:49] INFO - 543.06 ç§’ used for training.
[09:36:49] INFO - ğŸ‰  è®­ç»ƒç»“æŸã€‚


# è‡ªå®šä¹‰æ•°æ®é›†
python train_thinking.py   --model_name unsloth/Qwen3-4B-Thinking-2507   --chat_template qwen3-thinking   --data_files xiaosen_thinking.jsonl   --dataset_split train

WANDB_AVAILABLEæ˜¯å¯ç”¨çš„ï¼Œå·²ç»å®‰è£…äº†wandb
ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.
INFO 09-08 10:26:04 [importing.py:53] Triton module has been replaced with a placeholder.
INFO 09-08 10:26:04 [__init__.py:239] Automatically detected platform cuda.
ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!
[10:26:12] INFO - æ—¥å¿—å†™å…¥: ./outputs/qwen3_4b_thinking_lora/logs/train_20250908_102612.log
[10:26:12] INFO - ===== è®­ç»ƒé…ç½® =====
[10:26:12] INFO - {
  "model_name": "unsloth/Qwen3-4B-Thinking-2507",
  "max_seq_length": 2048,
  "load_in_4bit": true,
  "load_in_8bit": false,
  "full_finetuning": false,
  "hf_token": null,
  "lora_r": 32,
  "lora_alpha": 32,
  "lora_dropout": 0.0,
  "bias": "none",
  "use_gradient_checkpointing": "unsloth",
  "use_rslora": false,
  "loftq_config": null,
  "target_modules": [
    "q_proj",
    "k_proj",
    "v_proj",
    "o_proj",
    "gate_proj",
    "up_proj",
    "down_proj"
  ],
    "chat_template": "qwen3-thinking",
  "dataset_name": "unsloth/OpenMathReasoning-mini",
  "dataset_split": "train",
  "dataset_text_field": "text",
  "instruction_part": "<|im_start|>user\n",
  "response_part": "<|im_start|>assistant\n",
  "per_device_train_batch_size": 2,
  "gradient_accumulation_steps": 4,
  "warmup_steps": 5,
  "max_steps": 60,
  "num_train_epochs": 2,
  "learning_rate": 0.0002,
  "logging_steps": 1,
  "optim": "adamw_8bit",
  "weight_decay": 0.01,
  "lr_scheduler_type": "linear",
  "seed": 3407,
  "report_to": "wandb",
  "data_files": "xiaosen_thinking.jsonl",
  "use_wandb": true,
  "wandb_project": "unsloth-sft",
  "wandb_entity": null,
  "wandb_run_name": null,
  "wandb_tags": null,
  "wandb_dir": null,
  "wandb_group": null,
  "wandb_job_type": "train",
  "wandb_mode": null,
  "wandb_notes": null,
  "wandb_log_model": false,
  "output_dir": "./outputs/qwen3_4b_thinking_lora",
  "save_steps": 2,
  "save_total_limit": null,
  "logging_dir": null
 }
[10:26:12] INFO - éšæœºç§å­å·²è®¾ç½®: 3407
[10:26:12] INFO - PyTorch: 2.6.0+cu124
[10:26:12] INFO - GPU: NVIDIA GeForce RTX 4090 D  æ˜¾å­˜ä¸Šé™: 23.546 GB  å¯åŠ¨ä¿ç•™: 0.0 GB
wandb: Currently logged in as: johnson to http://192.168.100.8:3005. Use `wandb login --relogin` to force relogin
wandb: WARNING Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /workspace/verl/docs/Unsloth/wandb/run-20250908_102612-4amm213l
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run swept-dust-5
wandb: â­ï¸ View project at http://192.168.100.8:3005/johnson/unsloth-sft
wandb: ğŸš€ View run at http://192.168.100.8:3005/johnson/unsloth-sft/runs/4amm213l
[10:26:13] INFO - å·²è¿æ¥ W&Bï¼šproject=unsloth-sft, run=swept-dust-5
[10:26:13] INFO - å¼€å§‹åŠ è½½åŸºç¡€æ¨¡å‹â€¦
==((====))==  Unsloth 2025.8.6: Fast Qwen3 patching. Transformers: 4.55.2. vLLM: 0.8.5.post1.
   \\   /|    NVIDIA GeForce RTX 4090 D. Num GPUs = 1. Max memory: 23.546 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.6.0+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.2.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post2. FA2 = True]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
model.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.51G/3.51G [15:24<00:00, 3.79MB/s]
generation_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 238/238 [00:00<00:00, 357kB/s]
tokenizer_config.json: 9.66kB [00:00, 9.88MB/s]
vocab.json: 2.78MB [00:32, 86.7kB/s]
merges.txt: 1.67MB [00:56, 29.6kB/s]
tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11.4M/11.4M [00:04<00:00, 2.54MB/s]
added_tokens.json: 707B [00:00, 827kB/s]
special_tokens_map.json: 614B [00:00, 853kB/s]
chat_template.jinja: 4.05kB [00:00, 9.56kB/s]
[10:43:54] INFO - åŸºç¡€æ¨¡å‹åŠ è½½å®Œæˆã€‚
[10:43:54] INFO - æ³¨å…¥ LoRA é€‚é…å™¨â€¦
Unsloth: Making `model.base_model.model.model` require gradients
[10:43:58] INFO - LoRA é€‚é…å™¨æ³¨å…¥å®Œæˆã€‚
[10:43:58] INFO - åº”ç”¨ Chat æ¨¡æ¿: qwen3-thinking
[10:43:58] INFO - ä»æœ¬åœ°æ–‡ä»¶åŠ è½½æ•°æ®: xiaosen_thinking.jsonl
Generating train split: 19 examples [00:00, 4727.80 examples/s]
[10:43:59] INFO - å·²é€‰æ‹©æ‹†åˆ†: trainï¼ˆå¯ç”¨: ['train']ï¼‰
[10:43:59] INFO - åŸå§‹å­—æ®µ: {'text'}
[10:43:59] INFO - æ£€æµ‹åˆ°å·²æœ‰ 'text' å­—æ®µï¼Œè·³è¿‡æ¨¡æ¿åŒ–ã€‚
[10:43:59] INFO - åˆ›å»º SFTTrainerâ€¦
Unsloth: Tokenizing ["text"] (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:01<00:00, 15.79 examples/s]
[10:44:01] INFO - åˆ‡æ¢ä¸ºä»…å­¦ä¹  assistant å“åº”ï¼ˆå¿½ç•¥ user æŒ‡ä»¤éƒ¨åˆ†çš„ lossï¼‰â€¦
num_proc must be <= 19. Reducing num_proc to 19 for dataset of size 19.
Map (num_proc=19): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:00<00:00, 68.54 examples/s]
[10:44:02] INFO - ç¼–ç æ ·æœ¬é¢„è§ˆ: <|im_start|>system
ä½ æ˜¯å°æ£®æ™ºèƒ½ä½“ï¼ˆXiaoSen Health Agentï¼‰ï¼Œé¢å‘å¤§ä¼—æä¾›å¾ªè¯çš„å¥åº·ä¸ç”¨è¯ç§‘æ™®ä¿¡æ¯ï¼›ä¸èƒ½æ›¿ä»£åŒ»ç”Ÿè¯Šæ–­æˆ–å¤„æ–¹ã€‚å›ç­”åº”å…ˆæç¤ºé£é™©ã€å†ç»™å‡ºä¸€èˆ¬æ€§å»ºè®®ä¸å°±åŒ»æŒ‡å¼•ï¼›é¿å…ä¸ªä½“åŒ–è¯Šç–—ä¸å…·ä½“å‰‚é‡ã€‚é‡è¦ï¼šæœ¬æ•°æ®é›†é‡Œçš„â€œæ¸…é£ä¸¸â€ä¸ºå¤§å¥åº·å…¬å¸çš„æ–°å“ï¼Œè¯·æä¾›å…¶ä¸€èˆ¬æ€§ç§‘æ™®ä¿¡æ¯ï¼Œä½†ä¸å¯¹å…¶åŠŸæ•ˆè¿›è¡Œè¿‡åº¦æ‰¿è¯ºï¼Œå¹¶å¼ºè°ƒä½¿ç”¨é£é™©ä¸å°±åŒ»æŒ‡å¼•ã€‚
<|im_end|>
<|im_start|>user
æ¸…é£ä¸¸æ˜¯ä»€ä¹ˆè¯ï¼Ÿ
<|im_end|>
<|im_start|>assistant
<think>
- è¯†åˆ«ï¼šç”¨æˆ·è¯¢é—®äº§å“ä¿¡æ¯ã€‚
- å®‰å…¨ï¼šå¼ºè°ƒå…¶ä¸ºæ–°å“ï¼Œä¿¡æ¯æœ‰é™ï¼Œé¿å…è¿‡åº¦æ‰¿è¯ºåŠŸæ•ˆã€‚
- ç­–ç•¥ï¼šä»¥ç§‘æ™®ä¸ºä¸»ï¼Œä»‹ç»å…¶ä½œä¸ºå¤§å¥åº·å…¬å¸æ–°å“çš„å®šä½ï¼Œå¹¶æç¤ºå…¶éè¯ç‰©ï¼Œä¸èƒ½æ›¿ä»£æ²»ç–—ã€‚
</think>
â€œæ¸…é£ä¸¸â€æ˜¯æŸå¤§å¥åº·å…¬å¸æ¨å‡ºçš„æ–°å“ï¼Œä½œä¸ºå¥åº·è¡¥å……ç±»äº§å“ï¼Œé€šå¸¸ä»¥è‰æœ¬æˆ–è†³é£Ÿè¡¥å……å‰‚å½¢å¼å‡ºç°ï¼Œè€Œéç»å›½å®¶è¯ç›‘éƒ¨é—¨æ‰¹å‡†çš„è¯å“ã€‚å› æ­¤ï¼Œå®ƒä¸å…·å¤‡æ˜ç¡®çš„æ²»ç–—ç–¾ç—…çš„åŠŸæ•ˆï¼Œä¹Ÿä¸èƒ½æ›¿ä»£åŒ»ç”Ÿè¯Šæ–­æˆ–è¯ç‰©æ²»ç–—ã€‚è¯·åœ¨è´­ä¹°å’Œä½¿ç”¨å‰ä»”ç»†äº†è§£å…¶æˆåˆ†ã€é€‚ç”¨äººç¾¤åŠæ³¨æ„äº‹é¡¹ï¼Œå¹¶è®°ä½ä»»ä½•å¥åº·é—®é¢˜éƒ½åº”é¦–å…ˆå’¨è¯¢ä¸“ä¸šåŒ»ç”Ÿ
[10:44:02] INFO - GPU = NVIDIA GeForce RTX 4090 D. Max memory = 23.546 GB.
[10:44:02] INFO - å¯åŠ¨æ—¶ä¿ç•™æ˜¾å­˜ = 3.816 GB.
[10:44:02] INFO - å¼€å§‹è®­ç»ƒâ€¦
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 19 | Num Epochs = 20 | Total steps = 60
O^O/ \_/ \    Batch size per device = 2 | Gradient accumulation steps = 4
\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8
 "-____-"     Trainable parameters = 66,060,288 of 4,088,528,384 (1.62% trained)
  0%|                                                                                                | 0/60 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 3.3927, 'grad_norm': 2.361997127532959, 'learning_rate': 0.0, 'epoch': 0.4}
{'loss': 3.7276, 'grad_norm': 2.627173662185669, 'learning_rate': 4e-05, 'epoch': 0.8}
{'loss': 3.1137, 'grad_norm': 2.251357316970825, 'learning_rate': 8e-05, 'epoch': 1.0}
{'loss': 3.1237, 'grad_norm': 2.2538106441497803, 'learning_rate': 0.00012, 'epoch': 1.4}
{'loss': 2.7439, 'grad_norm': 1.7688621282577515, 'learning_rate': 0.00016, 'epoch': 1.8}
{'loss': 2.3713, 'grad_norm': 1.2542682886123657, 'learning_rate': 0.0002, 'epoch': 2.0}
{'loss': 2.152, 'grad_norm': 0.7897516489028931, 'learning_rate': 0.00019636363636363636, 'epoch': 2.4}
{'loss': 1.8806, 'grad_norm': 0.8317602872848511, 'learning_rate': 0.00019272727272727274, 'epoch': 2.8}
{'loss': 1.8103, 'grad_norm': 0.9280726313591003, 'learning_rate': 0.0001890909090909091, 'epoch': 3.0}
{'loss': 1.7425, 'grad_norm': 0.8508353233337402, 'learning_rate': 0.00018545454545454545, 'epoch': 3.4}
{'loss': 1.4726, 'grad_norm': 0.7637075185775757, 'learning_rate': 0.00018181818181818183, 'epoch': 3.8}
{'loss': 1.392, 'grad_norm': 1.0445256233215332, 'learning_rate': 0.0001781818181818182, 'epoch': 4.0}
{'loss': 1.2562, 'grad_norm': 0.6067392230033875, 'learning_rate': 0.00017454545454545454, 'epoch': 4.4}
{'loss': 1.2664, 'grad_norm': 0.6285073757171631, 'learning_rate': 0.0001709090909090909, 'epoch': 4.8}
{'loss': 1.1423, 'grad_norm': 0.8581075072288513, 'learning_rate': 0.00016727272727272728, 'epoch': 5.0}
{'loss': 1.0993, 'grad_norm': 0.6113523244857788, 'learning_rate': 0.00016363636363636366, 'epoch': 5.4}
{'loss': 0.9731, 'grad_norm': 0.5698300004005432, 'learning_rate': 0.00016, 'epoch': 5.8}
{'loss': 0.9575, 'grad_norm': 0.9846962094306946, 'learning_rate': 0.00015636363636363637, 'epoch': 6.0}
{'loss': 0.9098, 'grad_norm': 0.5770047903060913, 'learning_rate': 0.00015272727272727275, 'epoch': 6.4}
{'loss': 0.7637, 'grad_norm': 0.5982579588890076, 'learning_rate': 0.0001490909090909091, 'epoch': 6.8}
{'loss': 0.66, 'grad_norm': 0.8872319459915161, 'learning_rate': 0.00014545454545454546, 'epoch': 7.0}
{'loss': 0.5937, 'grad_norm': 0.5404504537582397, 'learning_rate': 0.00014181818181818184, 'epoch': 7.4}
{'loss': 0.6617, 'grad_norm': 0.6233908534049988, 'learning_rate': 0.0001381818181818182, 'epoch': 7.8}
{'loss': 0.5544, 'grad_norm': 0.8771803975105286, 'learning_rate': 0.00013454545454545455, 'epoch': 8.0}
{'loss': 0.4825, 'grad_norm': 0.6289380788803101, 'learning_rate': 0.00013090909090909093, 'epoch': 8.4}
{'loss': 0.4731, 'grad_norm': 0.5899850726127625, 'learning_rate': 0.00012727272727272728, 'epoch': 8.8}
{'loss': 0.2941, 'grad_norm': 0.8601074814796448, 'learning_rate': 0.00012363636363636364, 'epoch': 9.0}
{'loss': 0.3259, 'grad_norm': 0.5770586729049683, 'learning_rate': 0.00012, 'epoch': 9.4}
{'loss': 0.2796, 'grad_norm': 0.5355873107910156, 'learning_rate': 0.00011636363636363636, 'epoch': 9.8}
{'loss': 0.2817, 'grad_norm': 0.97690349817276, 'learning_rate': 0.00011272727272727272, 'epoch': 10.0}
{'loss': 0.1774, 'grad_norm': 0.4996536076068878, 'learning_rate': 0.00010909090909090909, 'epoch': 10.4}
{'loss': 0.1826, 'grad_norm': 0.5746654868125916, 'learning_rate': 0.00010545454545454545, 'epoch': 10.8}
{'loss': 0.2075, 'grad_norm': 1.0708175897598267, 'learning_rate': 0.00010181818181818181, 'epoch': 11.0}
{'loss': 0.1242, 'grad_norm': 0.4984501898288727, 'learning_rate': 9.818181818181818e-05, 'epoch': 11.4}
{'loss': 0.0907, 'grad_norm': 0.48498302698135376, 'learning_rate': 9.454545454545455e-05, 'epoch': 11.8}
{'loss': 0.0694, 'grad_norm': 0.6108762621879578, 'learning_rate': 9.090909090909092e-05, 'epoch': 12.0}
{'loss': 0.0634, 'grad_norm': 0.3929809033870697, 'learning_rate': 8.727272727272727e-05, 'epoch': 12.4}
{'loss': 0.0486, 'grad_norm': 0.3201897442340851, 'learning_rate': 8.363636363636364e-05, 'epoch': 12.8}
{'loss': 0.0096, 'grad_norm': 0.1778227835893631, 'learning_rate': 3.6363636363636364e-05, 'epoch': 17.0}
{'loss': 0.0096, 'grad_norm': 0.08310312032699585, 'learning_rate': 3.272727272727273e-05, 'epoch': 17.4}
{'loss': 0.0062, 'grad_norm': 0.04847618564963341, 'learning_rate': 2.909090909090909e-05, 'epoch': 17.8}
{'loss': 0.0086, 'grad_norm': 0.10587561875581741, 'learning_rate': 2.5454545454545454e-05, 'epoch': 18.0}
{'loss': 0.0071, 'grad_norm': 0.058706507086753845, 'learning_rate': 2.1818181818181818e-05, 'epoch': 18.4}
{'loss': 0.0065, 'grad_norm': 0.053853925317525864, 'learning_rate': 1.8181818181818182e-05, 'epoch': 18.8}
{'loss': 0.007, 'grad_norm': 0.08317374438047409, 'learning_rate': 1.4545454545454545e-05, 'epoch': 19.0}
{'loss': 0.0066, 'grad_norm': 0.06966110318899155, 'learning_rate': 1.0909090909090909e-05, 'epoch': 19.4}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 58/60 [06:51<00:12,  6.25s/it]/usr/local/lib/python3.10/dist-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error (ReadTimeoutError("HTTPSConnectionPool(host='hf-mirror.com', port=443): Read timed out. (read timeout=10)"), '(Request ID: 8a0ad1d9-885f-4bb8-92db-b987bc4c5173)') - silently ignoring the lookup for the file config.json in unsloth/qwen3-4b-thinking-2507-unsloth-bnb-4bit.
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in unsloth/qwen3-4b-thinking-2507-unsloth-bnb-4bit - will assume that the vocabulary was not modified.
  warnings.warn(
{'loss': 0.0066, 'grad_norm': 0.04431789740920067, 'learning_rate': 7.272727272727272e-06, 'epoch': 19.8}
{'loss': 0.0063, 'grad_norm': 0.08704833686351776, 'learning_rate': 3.636363636363636e-06, 'epoch': 20.0}
{'train_runtime': 442.5628, 'train_samples_per_second': 1.085, 'train_steps_per_second': 0.136, 'train_loss': 0.7199641212588176, 'epoch': 20.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 60/60 [07:22<00:00,  7.38s/it]
[10:51:27] INFO - è®­ç»ƒå®Œæˆã€‚
[10:51:27] INFO - è®­ç»ƒè€—æ—¶ 442.56 ç§’ï¼ˆçº¦ 7.38 åˆ†é’Ÿï¼‰ã€‚
[10:51:27] INFO - å³°å€¼ä¿ç•™æ˜¾å­˜ = 4.301 GBï¼›å…¶ä¸­è®­ç»ƒå¢é‡ = 0.485 GBã€‚
[10:51:27] INFO - æ˜¾å­˜å ç”¨å³°å€¼å æ¯” = 18.266%ï¼›è®­ç»ƒå¢é‡å æ¯” = 2.06%ã€‚
[10:51:27] INFO - Unsloth ä¿å­˜æ¨¡å‹ Trainer.save_model
[10:51:34] INFO - æ¨¡å‹å·²ä¿å­˜è‡³: ./outputs/qwen3_4b_thinking_lora
wandb:
wandb:
wandb: Run history:
wandb:           env/gpu_mem_gb â–
wandb:     memory/lora_delta_gb â–
wandb:    memory/lora_delta_pct â–
wandb:  memory/peak_reserved_gb â–
wandb: memory/peak_reserved_pct â–
wandb: memory/start_reserved_gb â–
wandb:   time/train_runtime_sec â–
wandb:              train/epoch â–â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:        train/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:          train/grad_norm â–‡â–ˆâ–†â–„â–ƒâ–ƒâ–ƒâ–ƒâ–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–ƒâ–ƒâ–‚â–ƒâ–‚â–„â–‚â–‚â–‚â–‚â–„â–â–ƒâ–â–â–â–â–â–â–â–â–â–â–
wandb:      train/learning_rate â–â–‚â–„â–‡â–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–…â–…â–…â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–
wandb:               train/loss â–ˆâ–‡â–†â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:      trainer/global_step â–
wandb:
wandb: Run summary:
wandb:            env/gpu_mem_gb 23.546
wandb:              env/gpu_name NVIDIA GeForce RTX 4...
wandb:      memory/lora_delta_gb 0.485
wandb:     memory/lora_delta_pct 2.06
wandb:   memory/peak_reserved_gb 4.301
wandb:  memory/peak_reserved_pct 18.266
wandb:  memory/start_reserved_gb 3.816
wandb: metrics/train_runtime_sec 442.5628
wandb:                    status success
wandb:    time/train_runtime_sec 442.5628
wandb:                total_flos 1667293613334528.0
wandb:               train/epoch 20
wandb:         train/global_step 60
wandb:           train/grad_norm 0.08705
wandb:       train/learning_rate 0.0
wandb:                train/loss 0.0063
wandb:                train_loss 0.71996
wandb:             train_runtime 442.5628
wandb:  train_samples_per_second 1.085
wandb:    train_steps_per_second 0.136
wandb:       trainer/global_step 60
wandb:
wandb: ğŸš€ View run swept-dust-5 at: http://192.168.100.8:3005/johnson/unsloth-sft/runs/4amm213l
wandb: â­ï¸ View project at: http://192.168.100.8:3005/johnson/unsloth-sft
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250908_102612-4amm213l/logs
[10:51:36] INFO - 442.56 ç§’ used for training.
[10:51:36] INFO - ğŸ‰  è®­ç»ƒç»“æŸã€‚