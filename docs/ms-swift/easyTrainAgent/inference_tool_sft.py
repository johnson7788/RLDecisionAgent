#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
è¯„ä¼°ä¸€ä¸ªç»è¿‡å¾®è°ƒçš„å‡½æ•°è°ƒç”¨æ¨¡å‹ï¼ˆå…¼å®¹ Qwen3ï¼Œæ”¯æŒ LoRAï¼‰ï¼Œ
å·¥å…·ç”± MCP æœåŠ¡å™¨åŠ¨æ€å‘ç°å¹¶æ‰§è¡Œã€‚

æ”¯æŒä¸¤ç§æ¨ç†å¼•æ“ï¼š
1) Unsloth è¿è¡Œæ—¶ï¼ˆé»˜è®¤ï¼‰ï¼šå¯åŠ è½½å®Œæ•´æ¨¡å‹ï¼Œæˆ–åœ¨åŸºç¡€æ¨¡å‹ä¸ŠåŠ è½½ LoRA é€‚é…å™¨
2) vLLM è¿è¡Œæ—¶ï¼ˆå¯é€‰ï¼‰ï¼šéœ€è¦åˆå¹¶åçš„å®Œæ•´æ¨¡å‹ï¼ˆä¸èƒ½ç›´æ¥ä½¿ç”¨åŸå§‹ LoRA ç›®å½•ï¼‰

ä¸åŸå§‹ç‰ˆæœ¬ç›¸æ¯”çš„å˜åŒ–ï¼š
- â—å·¥å…·é€šè¿‡ JSON é…ç½®ï¼ˆé»˜è®¤: a2a_agent/mcp_config.jsonï¼‰ä¸­å®šä¹‰çš„ MCP æœåŠ¡å™¨åŠ è½½ï¼Œ
  å¹¶é€šè¿‡è°ƒç”¨å¯¹åº” MCP æœåŠ¡å™¨å·¥å…·æ¥æ‰§è¡Œã€‚
- ğŸ§­ å¦‚æœå¤šä¸ªæœåŠ¡å™¨æš´éœ²äº†ç›¸åŒçš„å·¥å…·åï¼Œå°†ä½¿ç”¨ç¬¬ä¸€ä¸ªå‘ç°çš„ï¼ˆä¼šæ‰“å°è­¦å‘Šï¼‰ã€‚
- ğŸ›¡ï¸ è°ƒç”¨å·¥å…·æ—¶çš„é”™è¯¯ä¼šè¢«æ•è·ï¼Œå¹¶åœ¨ç¬¬äºŒè½®æ¨ç†ä¸­åé¦ˆç»™æ¨¡å‹ã€‚

ä½¿ç”¨ç¤ºä¾‹ï¼š
python inference_tool_sft.py \
  --model ./lora_model \
  --base_model unsloth/Qwen3-4B-Instruct-2507 \
  --engine unsloth \
  --query "ä»Šå¤©çš„æ—¥æœŸæ˜¯ï¼Ÿ" \
  --chat_template qwen-3 \
  --load_in_4bit \
  --mcp_config a2a_agent/mcp_config.json

ä½¿ç”¨ vLLMï¼ˆåˆå¹¶æ¨¡å‹ï¼‰ç¤ºä¾‹ï¼š
python inference_tool_sft.py --model your-hf/merged_model_16bit --engine vllm \
  --query "åŒ—äº¬ç°åœ¨å¤©æ°”å¦‚ä½•ï¼ŸæŠŠ 23 æ‘„æ°åº¦è½¬ä¸ºåæ°åº¦" \
  --mcp_config a2a_agent/mcp_config.json
"""

from __future__ import annotations
import os
import re
import json
import argparse
import hashlib
import random
from typing import Any, Dict, List, Tuple
from datetime import datetime

import torch
from unsloth import FastModel
from transformers import AutoTokenizer, TextStreamer
from peft import PeftModel
from unsloth.chat_templates import get_chat_template as _get_chat_template
from vllm import LLM
from vllm.sampling_params import SamplingParams

# NEW: MCP imports
import asyncio
from fastmcp import Client  # pip install fastmcp
from rl_train.mcp_client import tool_definition_to_dict  # reuse existing util

# =========================
# Utility
# =========================

def parse_bool_env(name: str, default: bool = False) -> bool:
    val = os.environ.get(name)
    if val is None:
        return default
    return val.lower() in {"1", "true", "yes", "y", "on"}


# =========================
# Robust JSON parsing for function calls
# =========================

def parse_function_calls(text: str):
    """Extract a JSON list/object from text.
    - Accepts raw JSON, or fenced code blocks (```json ... ``` / ``` ... ```)
    - Prefers first JSON array; falls back to first JSON object
    Returns a Python object or None.
    """
    if not isinstance(text, str):
        return None

    t = text.strip()
    # Remove surrounding code fences if present
    if t.startswith("```") and t.endswith("```"):
        inner = t.strip("`")
        # remove optional language tag at the start
        inner = re.sub(r"^(json|jsonc|javascript|js|txt)\s+", "", inner, flags=re.IGNORECASE)
        t = inner

    # Try direct JSON first
    try:
        return json.loads(t)
    except Exception:
        pass

    # Find first JSON array
    m = re.search(r"\[(?:.|\s)*\]", t)
    if m:
        try:
            return json.loads(m.group(0))
        except Exception:
            pass

    # Find first JSON object
    m = re.search(r"\{(?:.|\s)*\}", t)
    if m:
        try:
            return json.loads(m.group(0))
        except Exception:
            pass

    return None


# =========================
# MCP: discovery + execution
# =========================

class MCPRegistry:
    """Holds discovered MCP tools (JSON Schemas) and a name->server map."""

    def __init__(self):
        self.tools_schema: List[Dict[str, Any]] = []
        self.name_to_server: Dict[str, str] = {}

    def tools_schema_json(self) -> str:
        return json.dumps(self.tools_schema, ensure_ascii=False, indent=2)

    def find_server(self, tool_name: str) -> str | None:
        return self.name_to_server.get(tool_name)


async def discover_mcp_tools(config_path: str) -> MCPRegistry:
    """Read MCP servers from config and list their tools.

    Config schema expected (subset):
    {
      "mcpServers": {
        "server_key": {"url": "http://localhost:10001", "disabled": false},
        ...
      }
    }
    """
    registry = MCPRegistry()

    try:
        with open(config_path, "r", encoding="utf-8") as f:
            mcp_config = json.load(f)
    except FileNotFoundError:
        raise FileNotFoundError(f"MCPé…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")

    mcp_servers = (mcp_config or {}).get("mcpServers", {})
    if not mcp_servers:
        return registry

    for server_name, server_info in mcp_servers.items():
        if server_info.get("disabled"):
            continue
        url = server_info.get("url")
        if not url:
            continue

        try:
            client = Client(url)
            async with client:
                tools = await client.list_tools()
        except Exception as e:
            print(f"âš ï¸  æ— æ³•ä» '{server_name}' ({url}) è·å–å·¥å…·: {e}")
            continue

        for t in tools:
            try:
                d = tool_definition_to_dict(t)  # {name, description, parameters}
            except Exception:
                # Fallback minimal shape
                d = {"name": getattr(t, "name", ""), "description": getattr(t, "description", None)}

            name = d.get("name")
            if not name:
                continue

            if name in registry.name_to_server:
                # Prefer the first one, but warn once
                # (you can change policy here if needed)
                print(f"âš ï¸  å·¥å…·åå†²çª: '{name}' åœ¨å¤šä¸ªæœåŠ¡å™¨ä¸­å‘ç°ï¼Œä»å°†ä½¿ç”¨å…ˆå‘ç°çš„æœåŠ¡å™¨ {registry.name_to_server[name]}")
            else:
                registry.name_to_server[name] = url

            # Ensure JSON-Schema-like shape
            if "parameters" not in d or d["parameters"] is None:
                d["parameters"] = {"type": "object", "properties": {}, "required": []}

            registry.tools_schema.append(d)

    return registry


async def mcp_call_tool(server_url: str, tool_name: str, arguments: Dict[str, Any] | None) -> Any:
    """Call a tool on a given MCP server and return a JSON-serializable result."""
    arguments = arguments or {}
    client = Client(server_url)
    try:
        async with client:
            try:
                result = await client.call_tool(tool_name, arguments)
            except AttributeError:
                # Older fastmcp versions may use `call` as the method name
                result = await client.call(tool_name, arguments)
    except Exception as e:
        return {"error": f"è°ƒç”¨å·¥å…·å¤±è´¥: {e}"}

    # Normalize to JSON-serializable
    def _normalize(x: Any) -> Any:
        try:
            if hasattr(x, "model_dump"):
                x = x.model_dump()
        except Exception:
            pass
        try:
            json.dumps(x, ensure_ascii=False)
            return x
        except TypeError:
            return repr(x)

    return _normalize(result)


# =========================
# Loading (Unsloth + LoRA)
# =========================

def _file_exists(d: str, name: str) -> bool:
    return os.path.isfile(os.path.join(d, name))


def _is_lora_dir(path: str) -> bool:
    return _file_exists(path, "adapter_config.json") or _file_exists(path, "adapter_model.safetensors")


def _has_full_model(path: str) -> bool:
    if not _file_exists(path, "config.json"):
        return False
    for fname in [
        "model.safetensors", "model.safetensors.index.json",
        "pytorch_model.bin", "pytorch_model.bin.index.json",
    ]:
        if _file_exists(path, fname):
            return True
    return False


def _load_tokenizer(model_dir: str, base_model: str | None, chat_template: str | None):
    tok_dir = model_dir if (_file_exists(model_dir, "tokenizer.json") or _file_exists(model_dir, "tokenizer_config.json")) else (base_model or model_dir)
    tokenizer = AutoTokenizer.from_pretrained(tok_dir, use_fast=True)
    # Optionally force a chat template (e.g., qwen_1_5)
    if chat_template:
        tokenizer = _get_chat_template(tokenizer, chat_template=chat_template)
    return tokenizer


def _ensure_pad_token(tokenizer):
    if getattr(tokenizer, "pad_token_id", None) is None:
        tokenizer.pad_token = tokenizer.eos_token


def _load_model_unsloth(model_path: str, base_model: str | None, max_seq_length: int, load_in_4bit: bool, load_in_8bit: bool):
    if _is_lora_dir(model_path):
        if not base_model:
            raise ValueError("--model points to a LoRA adapter directory; please provide --base_model.")
        base, _ = FastModel.from_pretrained(
            model_name=base_model,
            max_seq_length=max_seq_length,
            load_in_4bit=load_in_4bit,
            load_in_8bit=load_in_8bit,
            full_finetuning=False,
            token=None,
        )
        model = PeftModel.from_pretrained(base, model_path)
    elif _has_full_model(model_path):
        model, _ = FastModel.from_pretrained(
            model_name=model_path,
            max_seq_length=max_seq_length,
            load_in_4bit=load_in_4bit,
            load_in_8bit=load_in_8bit,
            full_finetuning=False,
            token=None,
        )
    else:
        raise ValueError(f"Cannot recognize {model_path} as a full model or a LoRA adapter directory.")

    try:
        model.eval()
    except Exception:
        pass

    # Move to CUDA if available
    if torch is not None and torch.cuda.is_available():
        try:
            model.to("cuda")
        except Exception:
            pass
    return model


# =========================
# Inference passes (now parameterized by MCP tools)
# =========================

def _apply_template(tokenizer, messages: List[Dict[str, str]], add_generation_prompt: bool = True, tokenize: bool = True):
    return tokenizer.apply_chat_template(
        messages,
        tokenize=tokenize,
        add_generation_prompt=add_generation_prompt,
        return_tensors="pt" if tokenize else None,
    )


def _first_pass_system_prompt(tools_json: str) -> str:
    if tools_json.strip() == "[]":
        # No tools â€” still ask for JSON, but make it clear there are none
        return (
            "å½“å‰æ²¡æœ‰å¯ç”¨çš„å·¥å…·ã€‚\n"
            "å¦‚æœéœ€è¦ï¼Œè¯·ç›´æ¥å›ç­”ï¼›è‹¥ä½ ä»è¦è°ƒç”¨å·¥å…·ï¼Œè¯·è¾“å‡ºç©ºåˆ—è¡¨ []ã€‚"
        )
    return (
        "ä½ å¯ä»¥è°ƒç”¨ä¸‹åˆ—å·¥å…·ï¼ˆä»¥ JSON Schema æè¿°ï¼‰ã€‚\n"
        "è¯·å…ˆè¾“å‡ºä¸€æ®µä»…åŒ…å« JSON çš„å†…å®¹ï¼Œæ ¼å¼ä¸ºä¸€ä¸ªåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ï¼š\n"
        '{"name": "<å‡½æ•°å>", "arguments": {<å‚æ•°>}}\n'
        "ä¸è¦è¾“å‡ºè§£é‡Šæ–‡å­—æˆ–é¢å¤–å†…å®¹ã€‚\n"
        "å·¥å…·åˆ—è¡¨å¦‚ä¸‹ï¼š\n" + tools_json
    )


def first_pass_generate_unsloth(model, tokenizer, query: str, max_new_tokens: int, tools_json: str) -> str:
    sys_prompt = _first_pass_system_prompt(tools_json)
    messages = [
        {"role": "system", "content": sys_prompt},
        {"role": "user", "content": query},
    ]

    prompt_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    inputs = tokenizer(prompt_text, return_tensors="pt")
    if hasattr(model, "device"):
        try:
            inputs = {k: v.to(model.device) for k, v in inputs.items()}
        except Exception:
            pass

    gen_kwargs = dict(max_new_tokens=max_new_tokens, use_cache=True, pad_token_id=tokenizer.eos_token_id)
    out = model.generate(**inputs, **gen_kwargs)
    decoded = tokenizer.decode(out[0], skip_special_tokens=False)
    return decoded[len(prompt_text):]


def second_pass_generate_unsloth(model, tokenizer, query: str, tool_results: List[Dict[str, Any]], max_new_tokens: int):
    synthesis_system = (
        "ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„åŠ©ç†ã€‚è¯·ä»…åŸºäºæä¾›çš„å·¥å…·ç»“æœï¼Œç»™å‡ºä¸­æ–‡ç­”æ¡ˆï¼ŒåŒ…å«å…³é”®æ•°å€¼ä¸å•ä½ã€‚"
    )
    synthesis_user = (
        f"ç”¨æˆ·æé—®ï¼š{query}\n\n"
        f"å·¥å…·è°ƒç”¨ä¸è¿”å›ï¼š\n{json.dumps(tool_results, ensure_ascii=False, indent=2)}\n\n"
        "è¯·ç»™å‡ºæœ€ç»ˆå›ç­”ã€‚"
    )
    messages = [
        {"role": "system", "content": synthesis_system},
        {"role": "user", "content": synthesis_user},
    ]

    prompt_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    inputs = tokenizer(prompt_text, return_tensors="pt")
    if hasattr(model, "device"):
        try:
            inputs = {k: v.to(model.device) for k, v in inputs.items()}
        except Exception:
            pass

    streamer = TextStreamer(tokenizer, skip_prompt=True)
    gen_kwargs = dict(max_new_tokens=max_new_tokens, use_cache=True, pad_token_id=tokenizer.eos_token_id)
    _ = model.generate(**inputs, streamer=streamer, **gen_kwargs)


# vLLM versions

def _ensure_tokenizer_for_prompt_only(chat_template: str | None, model_or_repo: str):
    # We only need tokenizer for producing a textual chat-formatted prompt
    tok = AutoTokenizer.from_pretrained(model_or_repo, use_fast=True)
    if chat_template:
        tok = _get_chat_template(tok, chat_template=chat_template)
    return tok


def _vllm_generate_text(llm: LLM, text_prompt: str, max_tokens: int) -> str:
    sp = SamplingParams(max_tokens=max_tokens)
    outputs = llm.generate([text_prompt], sp)
    return outputs[0].outputs[0].text


def first_pass_generate_vllm(model_repo: str, query: str, max_tokens: int, chat_template: str | None, tools_json: str) -> str:
    tok = _ensure_tokenizer_for_prompt_only(chat_template, model_repo)

    sys_prompt = _first_pass_system_prompt(tools_json)
    messages = [
        {"role": "system", "content": sys_prompt},
        {"role": "user", "content": query},
    ]
    text_prompt = tok.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)

    llm = LLM(model=model_repo, max_model_len=2048, tokenizer_mode="auto", tensor_parallel_size=1)
    return _vllm_generate_text(llm, text_prompt, max_tokens)


def second_pass_generate_vllm(model_repo: str, query: str, tool_results: List[Dict[str, Any]], max_tokens: int, chat_template: str | None) -> str:
    tok = _ensure_tokenizer_for_prompt_only(chat_template, model_repo)

    synthesis_system = (
        "ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„åŠ©ç†ã€‚è¯·ä»…åŸºäºæä¾›çš„å·¥å…·ç»“æœï¼Œç»™å‡ºä¸­æ–‡ç­”æ¡ˆï¼ŒåŒ…å«å…³é”®æ•°å€¼ä¸å•ä½ã€‚"
    )
    synthesis_user = (
        f"ç”¨æˆ·æé—®ï¼š{query}\n\n"
        f"å·¥å…·è°ƒç”¨ä¸è¿”å›ï¼š\n{json.dumps(tool_results, ensure_ascii=False, indent=2)}\n\n"
        "è¯·ç»™å‡ºæœ€ç»ˆå›ç­”ã€‚"
    )
    messages = [
        {"role": "system", "content": synthesis_system},
        {"role": "user", "content": synthesis_user},
    ]
    text_prompt = tok.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)

    llm = LLM(model=model_repo, max_model_len=2048, tokenizer_mode="auto", tensor_parallel_size=1)
    return _vllm_generate_text(llm, text_prompt, max_tokens)


# =========================
# Orchestration
# =========================

def run_unsloth(args):
    # 1) Discover MCP tools
    registry = asyncio.run(discover_mcp_tools(args.mcp_config))
    print(f"ğŸ”§ é€šè¿‡ MCP å‘ç° {len(registry.tools_schema)} ä¸ªå·¥å…·ã€‚")

    # 2) Load model/tokenizer
    tokenizer = _load_tokenizer(args.model, args.base_model, args.chat_template)
    _ensure_pad_token(tokenizer)
    model = _load_model_unsloth(
        model_path=args.model,
        base_model=args.base_model,
        max_seq_length=2048,
        load_in_4bit=args.load_in_4bit,
        load_in_8bit=args.load_in_8bit,
    )

    # 3) Pass 1: ask for function calls
    tools_json = registry.tools_schema_json() if registry.tools_schema else "[]"
    first = first_pass_generate_unsloth(model, tokenizer, args.query, args.max_new_tokens, tools_json)
    print("=== Raw function-call block ===\n", first)
    calls = parse_function_calls(first)
    print("=== Parsed function calls ===\n", calls)

    if not calls:
        print("æœªæ£€æµ‹åˆ°å‡½æ•°è°ƒç”¨ JSONï¼Œç›´æ¥è¾“å‡ºï¼š\n" + first)
        return

    # 4) Execute tools via MCP
    items = calls if isinstance(calls, list) else [calls]
    tool_results = []
    for item in items:
        name = (item or {}).get("name")
        args_ = (item or {}).get("arguments") or {}
        if not name:
            res = {"error": "Missing 'name' in function call"}
        else:
            server_url = registry.find_server(name)
            if not server_url:
                res = {"error": f"Unknown tool: {name}"}
            else:
                res = asyncio.run(mcp_call_tool(server_url, name, args_))
        print(f"[Tool:{name}] -> {res}")
        tool_results.append({"name": name, "arguments": args_, "result": res})

    # 5) Pass 2: synthesis
    second_pass_generate_unsloth(model, tokenizer, args.query, tool_results, max_new_tokens=min(512, args.max_new_tokens))


def run_vllm(args):
    # 1) Discover MCP tools
    registry = asyncio.run(discover_mcp_tools(args.mcp_config))
    print(f"ğŸ”§ é€šè¿‡ MCP å‘ç° {len(registry.tools_schema)} ä¸ªå·¥å…·ã€‚")

    # 2) vLLM requires merged model
    if _is_lora_dir(args.model):
        raise ValueError("vLLM ä¸æ”¯æŒç›´æ¥åŠ è½½ LoRA é€‚é…å™¨ç›®å½•ï¼›è¯·å…ˆ merge å¾—åˆ°å®Œæ•´æƒé‡åå†ç”¨ --engine vllm")

    # 3) Pass 1
    tools_json = registry.tools_schema_json() if registry.tools_schema else "[]"
    text1 = first_pass_generate_vllm(args.model, args.query, max_tokens=min(768, args.max_new_tokens), chat_template=args.chat_template, tools_json=tools_json)
    print("=== vLLM raw output (function calls) ===\n", repr(text1))
    calls = parse_function_calls(text1)
    print("=== Parsed function calls ===\n", calls)

    if not calls:
        print("æœªæ£€æµ‹åˆ°å‡½æ•°è°ƒç”¨ JSONï¼Œç›´æ¥è¾“å‡ºï¼š\n" + text1)
        return

    # 4) Execute tools via MCP
    items = calls if isinstance(calls, list) else [calls]
    tool_results = []
    for item in items:
        name = (item or {}).get("name")
        args_ = (item or {}).get("arguments") or {}
        if not name:
            res = {"error": "Missing 'name' in function call"}
        else:
            server_url = registry.find_server(name)
            if not server_url:
                res = {"error": f"Unknown tool: {name}"}
            else:
                res = asyncio.run(mcp_call_tool(server_url, name, args_))
        print(f"[Tool:{name}] -> {res}")
        tool_results.append({"name": name, "arguments": args_, "result": res})

    # 5) Pass 2
    text2 = second_pass_generate_vllm(args.model, args.query, tool_results, max_tokens=min(512, args.max_new_tokens), chat_template=args.chat_template)
    print("=== vLLM final answer ===\n", text2)


# =========================
# CLI
# =========================

def main():
    ap = argparse.ArgumentParser(description="ä½¿ç”¨ MCP å·¥å…·è¯„ä¼°ä¸€ä¸ªæ”¯æŒå‡½æ•°è°ƒç”¨çš„å¾®è°ƒæ¨¡å‹ï¼ˆQwen3/Unsloth/LoRAï¼‰ã€‚é¦–å…ˆå¯åŠ¨MCPæœåŠ¡å™¨")
    ap.add_argument("--model", required=True, help="æ¨¡å‹è·¯å¾„ï¼ˆå®Œæ•´æ¨¡å‹ç›®å½•ã€LoRA é€‚é…å™¨ç›®å½•ï¼Œæˆ– HuggingFace ä»“åº“åï¼‰ã€‚")
    ap.add_argument("--base_model", default=None,
                    help="å½“ --model æ˜¯ LoRA ç›®å½•æ—¶éœ€æŒ‡å®šåŸºç¡€æ¨¡å‹ï¼ˆå¦‚ unsloth/Qwen3-4B-Instruct-2507ï¼‰")
    ap.add_argument("--engine", choices=["unsloth", "vllm"], default="unsloth",
                    help="æ¨ç†å¼•æ“ï¼Œé»˜è®¤ä¸º unslothï¼Œå¯é€‰ vllm")
    ap.add_argument("--query", default="æ—§é‡‘å±±çš„å¤©æ°”å¦‚ä½•ï¼Ÿ", help="è¾“å…¥æŸ¥è¯¢æ–‡æœ¬")
    ap.add_argument("--max_new_tokens", type=int, default=1024, help="æœ€å¤§ç”Ÿæˆ token æ•°")
    ap.add_argument("--chat_template", default=None, help="å¼ºåˆ¶æŒ‡å®š chat æ¨¡æ¿ï¼ˆå¦‚ qwen_1_5ï¼‰ã€‚è‹¥ä¸æŒ‡å®šåˆ™ä½¿ç”¨é»˜è®¤ã€‚")
    ap.add_argument("--load_in_4bit", action="store_true", default=False, help="æ˜¯å¦ä»¥ 4bit åŠ è½½æ¨¡å‹")
    ap.add_argument("--load_in_8bit", action="store_true", default=False,
                    help="æ˜¯å¦ä»¥ 8bit åŠ è½½æ¨¡å‹ï¼ˆè‹¥åŒæ—¶æŒ‡å®š 4bit å’Œ 8bitï¼Œå°†ä¼˜å…ˆä½¿ç”¨ 4bitï¼‰")
    ap.add_argument("--mcp_config", default="a2a_agent/mcp_config.json", help="MCP æœåŠ¡å™¨é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆJSON æ ¼å¼ï¼‰")
    args = ap.parse_args()

    # Boolean adjust: if both are set, prioritize 4bit
    if args.load_in_4bit and args.load_in_8bit:
        args.load_in_8bit = False

    if args.engine == "unsloth":
        run_unsloth(args)
    else:
        run_vllm(args)


if __name__ == "__main__":
    main()
