swift infer \
    --adapters ./output/v0-20250926-114832/checkpoint-94 \
    --stream true \
    --merge_lora true \
    --infer_backend vllm \
    --vllm_max_model_len 8192 \
    --temperature 0 \
    --max_new_tokens 2048

/usr/local/lib/python3.11/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[INFO:swift] Successfully registered `/usr/local/lib/python3.11/site-packages/swift/llm/dataset/data/dataset_info.json`.
[INFO:swift] Loading the model using model_dir: ./output/v0-20250926-114832/checkpoint-94
[INFO:swift] Successfully loaded /workspace/verl/output/v0-20250926-114832/checkpoint-94/args.json.
[INFO:swift] rank: -1, local_rank: -1, world_size: 1, local_world_size: 1
[INFO:swift] Downloading the model from ModelScope Hub, model_id: Qwen/Qwen2.5-7B-Instruct
Downloading Model from https://www.modelscope.cn to directory: /mnt/workspace/.cache/modelscope/hub/models/Qwen/Qwen2.5-7B-Instruct
[INFO:modelscope] Target directory already exists, skipping creation.
[INFO:swift] Loading the model using model_dir: /mnt/workspace/.cache/modelscope/hub/models/Qwen/Qwen2___5-7B-Instruct
`torch_dtype` is deprecated! Use `dtype` instead!
[INFO:swift] Setting args.lazy_tokenize: False
[INFO:swift] args.result_path: /workspace/verl/output/v0-20250926-114832/checkpoint-94/infer_result/20250926-150317.jsonl
[INFO:swift] Setting args.eval_human: True
[INFO:swift] Global seed set to 42
[INFO:swift] args: InferArguments(model='Qwen/Qwen2.5-7B-Instruct', model_type='qwen2_5', model_revision=None, task_type='causal_lm', torch_dtype=torch.bfloat16, attn_impl=None, new_special_tokens=[], num_labels=None, problem_type=None, rope_scaling=None, device_map=None, max_memory={}, max_model_len=None, local_repo_path=None, init_strategy=None, template='qwen2_5', system='You are a helpful assistant.', max_length=32768, truncation_strategy='delete', max_pixels=None, agent_template=None, norm_bbox=None, use_chat_template=True, padding_free=False, padding_side='right', loss_scale='default', sequence_parallel_size=1, response_prefix=None, template_backend='swift', dataset=[], val_dataset=[], split_dataset_ratio=0.0, data_seed=42, dataset_num_proc=1, load_from_cache_file=True, dataset_shuffle=True, val_dataset_shuffle=False, streaming=False, interleave_prob=None, stopping_strategy='first_exhausted', shuffle_buffer_size=1000, download_mode='reuse_dataset_if_exists', columns={}, strict=False, remove_unused_columns=True, model_name=None, model_author=None, custom_dataset_info=[], quant_method=None, quant_bits=None, hqq_axis=None, bnb_4bit_compute_dtype=torch.bfloat16, bnb_4bit_quant_type='nf4', bnb_4bit_use_double_quant=True, bnb_4bit_quant_storage=None, max_new_tokens=2048, temperature=0.0, top_k=None, top_p=None, repetition_penalty=None, num_beams=1, stream=True, stop_words=[], logprobs=False, top_logprobs=None, ckpt_dir='/workspace/verl/output/v0-20250926-114832/checkpoint-94', lora_modules=[], tuner_backend='peft', train_type='lora', adapters=['/workspace/verl/output/v0-20250926-114832/checkpoint-94'], external_plugins=[], seed=42, model_kwargs={}, load_args=True, load_data_args=False, packing=False, packing_length=None, lazy_tokenize=False, cached_dataset=[], custom_register_path=[], use_hf=False, hub_token=None, ddp_timeout=18000000, ddp_backend=None, ignore_args_error=False, use_swift_lora=False, vllm_gpu_memory_utilization=0.9, vllm_tensor_parallel_size=1, vllm_pipeline_parallel_size=1, vllm_enable_expert_parallel=False, vllm_max_num_seqs=256, vllm_max_model_len=8192, vllm_disable_custom_all_reduce=True, vllm_enforce_eager=False, vllm_limit_mm_per_prompt={}, vllm_max_lora_rank=16, vllm_enable_prefix_caching=False, vllm_use_async_engine=False, vllm_quantization=None, vllm_reasoning_parser=None, vllm_disable_cascade_attn=False, vllm_data_parallel_size=1, gpu_memory_utilization=None, tensor_parallel_size=None, limit_mm_per_prompt=None, data_parallel_size=None, use_async_engine=None, sglang_tp_size=1, sglang_pp_size=1, sglang_dp_size=1, sglang_ep_size=1, sglang_enable_ep_moe=False, sglang_mem_fraction_static=None, sglang_context_length=None, sglang_disable_cuda_graph=False, sglang_quantization=None, sglang_kv_cache_dtype='auto', sglang_enable_dp_attention=False, sglang_disable_custom_all_reduce=True, lmdeploy_tp=1, lmdeploy_session_len=None, lmdeploy_cache_max_entry_count=0.8, lmdeploy_quant_policy=0, lmdeploy_vision_batch_size=1, merge_lora=True, safe_serialization=True, max_shard_size='5GB', infer_backend='vllm', result_path='/workspace/verl/output/v0-20250926-114832/checkpoint-94/infer_result/20250926-150317.jsonl', write_batch_size=1000, metric=None, max_batch_size=1, val_dataset_sample=None)
[INFO:swift] merge_device_map: cpu
[INFO:swift] Downloading the model from ModelScope Hub, model_id: Qwen/Qwen2.5-7B-Instruct
Downloading Model from https://www.modelscope.cn to directory: /mnt/workspace/.cache/modelscope/hub/models/Qwen/Qwen2.5-7B-Instruct
[INFO:modelscope] Target directory already exists, skipping creation.
[INFO:swift] Loading the model using model_dir: /mnt/workspace/.cache/modelscope/hub/models/Qwen/Qwen2___5-7B-Instruct
[INFO:swift] model_kwargs: {'device_map': 'cpu'}
`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 41.04it/s]
[INFO:swift] default_system: 'You are a helpful assistant.'
[INFO:swift] max_length: 32768
[INFO:swift] response_prefix: ''
[INFO:swift] agent_template: hermes
/usr/local/lib/python3.11/site-packages/awq/__init__.py:21: DeprecationWarning:
I have left this message as the final dev message to help you transition.

Important Notice:
- AutoAWQ is officially deprecated and will no longer be maintained.
- The last tested configuration used Torch 2.6.0 and Transformers 4.51.3.
- If future versions of Transformers break AutoAWQ compatibility, please report the issue to the Transformers project.

Alternative:
- AutoAWQ has been adopted by the vLLM Project: https://github.com/vllm-project/llm-compressor

For further inquiries, feel free to reach out:
- X: https://x.com/casper_hansen_
- LinkedIn: https://www.linkedin.com/in/casper-hansen-804005170/

  warnings.warn(_FINAL_DEV_MESSAGE, category=DeprecationWarning, stacklevel=1)
[INFO:swift] Merge LoRA...
[INFO:swift] Saving merged weights...
/usr/local/lib/python3.11/site-packages/deepspeed/ops/op_builder/builder.py:16: DeprecationWarning: The distutils package is deprecated and slated for removal in Python 3.12. Use setuptools or check PEP 632 for potential alternatives
  import distutils.ccompiler
/usr/local/lib/python3.11/site-packages/deepspeed/ops/op_builder/builder.py:18: DeprecationWarning: The distutils.sysconfig module is deprecated, use sysconfig instead
  import distutils.sysconfig
[2025-09-26 15:03:37,568] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-09-26 15:03:39,350] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
/usr/local/lib/python3.11/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[INFO:swift] Successfully merged LoRA and saved in /workspace/verl/output/v0-20250926-114832/checkpoint-94-merged.
<frozen importlib._bootstrap>:241: DeprecationWarning: builtin type SwigPyPacked has no __module__ attribute
<frozen importlib._bootstrap>:241: DeprecationWarning: builtin type SwigPyObject has no __module__ attribute
/usr/local/lib/python3.11/site-packages/triton/runtime/autotuner.py:108: DeprecationWarning: warmup, rep, and use_cuda_graph parameters are deprecated. See https://github.com/triton-lang/triton/pull/4496 for details.
  warnings.warn(("warmup, rep, and use_cuda_graph parameters are deprecated. See "
Modular Diffusers is currently an experimental feature under active development. The API is subject to breaking changes in future releases.
INFO 09-26 15:04:05 [__init__.py:241] Automatically detected platform cuda.
[INFO:swift] Loading the model using model_dir: /workspace/verl/output/v0-20250926-114832/checkpoint-94-merged
[INFO:swift] default_system: 'You are a helpful assistant.'
[INFO:swift] max_length: 32768
[INFO:swift] response_prefix: ''
[INFO:swift] agent_template: hermes
[INFO:swift] Setting max_model_len: 8192
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
INFO 09-26 15:04:18 [__init__.py:711] Resolved architecture: Qwen2ForCausalLM
INFO 09-26 15:04:18 [__init__.py:1750] Using max model len 8192
INFO 09-26 15:04:18 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=2048.
/usr/local/lib/python3.11/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[INFO:swift] Successfully registered `/usr/local/lib/python3.11/site-packages/swift/llm/dataset/data/dataset_info.json`.
Modular Diffusers is currently an experimental feature under active development. The API is subject to breaking changes in future releases.
INFO 09-26 15:04:29 [__init__.py:241] Automatically detected platform cuda.
(EngineCore_0 pid=1613) INFO 09-26 15:04:30 [core.py:636] Waiting for init message from front-end.
(EngineCore_0 pid=1613) INFO 09-26 15:04:30 [core.py:74] Initializing a V1 LLM engine (v0.10.1.1) with config: model='/workspace/verl/output/v0-20250926-114832/checkpoint-94-merged', speculative_config=None, tokenizer='/workspace/verl/output/v0-20250926-114832/checkpoint-94-merged', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=/workspace/verl/output/v0-20250926-114832/checkpoint-94-merged, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.mamba_mixer2"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":1,"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"pass_config":{},"max_capture_size":512,"local_cache_dir":null}
(EngineCore_0 pid=1613) INFO 09-26 15:04:32 [parallel_state.py:1134] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
(EngineCore_0 pid=1613) INFO 09-26 15:04:32 [topk_topp_sampler.py:50] Using FlashInfer for top-p & top-k sampling.
(EngineCore_0 pid=1613) INFO 09-26 15:04:32 [gpu_model_runner.py:1953] Starting to load model /workspace/verl/output/v0-20250926-114832/checkpoint-94-merged...
(EngineCore_0 pid=1613) INFO 09-26 15:04:33 [gpu_model_runner.py:1985] Loading model from scratch...
(EngineCore_0 pid=1613) INFO 09-26 15:04:33 [cuda.py:328] Using Flash Attention backend on V1 engine.
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:02,  1.19it/s]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.06it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.01it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.40it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.25it/s]
(EngineCore_0 pid=1613)
(EngineCore_0 pid=1613) INFO 09-26 15:04:36 [default_loader.py:262] Loading weights took 3.21 seconds
(EngineCore_0 pid=1613) INFO 09-26 15:04:37 [gpu_model_runner.py:2007] Model loading took 14.2488 GiB and 3.415556 seconds
(EngineCore_0 pid=1613) INFO 09-26 15:04:45 [backends.py:548] Using cache directory: /root/.cache/vllm/torch_compile_cache/304f5897e9/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_0 pid=1613) INFO 09-26 15:04:45 [backends.py:559] Dynamo bytecode transform time: 8.28 s
(EngineCore_0 pid=1613) INFO 09-26 15:04:49 [backends.py:194] Cache the graph for dynamic shape for later use
(EngineCore_0 pid=1613) INFO 09-26 15:05:17 [backends.py:215] Compiling a graph for dynamic shape takes 31.31 s
(EngineCore_0 pid=1613) INFO 09-26 15:05:24 [monitor.py:34] torch.compile takes 39.59 s in total
(EngineCore_0 pid=1613) /usr/local/lib/python3.11/site-packages/torch/utils/cpp_extension.py:2356: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation.
(EngineCore_0 pid=1613) If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
(EngineCore_0 pid=1613)   warnings.warn(

(EngineCore_0 pid=1613) INFO 09-26 15:06:23 [gpu_worker.py:276] Available KV cache memory: 6.39 GiB
(EngineCore_0 pid=1613) INFO 09-26 15:06:24 [kv_cache_utils.py:849] GPU KV cache size: 119,712 tokens
(EngineCore_0 pid=1613) INFO 09-26 15:06:24 [kv_cache_utils.py:853] Maximum concurrency for 8,192 tokens per request: 14.61x
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████████████████████████████████████████████| 67/67 [00:03<00:00, 18.20it/s]
(EngineCore_0 pid=1613) INFO 09-26 15:06:28 [gpu_model_runner.py:2708] Graph capturing finished in 4 secs, took 0.50 GiB
(EngineCore_0 pid=1613) INFO 09-26 15:06:28 [core.py:214] init engine (profile, create kv cache, warmup model) took 111.37 seconds
[INFO:swift] Start time of running main: 2025-09-26 15:06:29.139961
[INFO:swift] swift.__version__: 3.8.1
[INFO:swift] request_config: RequestConfig(max_tokens=2048, temperature=0.0, top_k=None, top_p=None, repetition_penalty=None, num_beams=1, stop=[], seed=None, stream=True, logprobs=False, top_logprobs=None, n=1, best_of=None, presence_penalty=0.0, frequency_penalty=0.0, length_penalty=1.0, return_details=False)
[INFO:swift] Input `exit` or `quit` to exit the conversation.
[INFO:swift] Input `multi-line` to switch to multi-line input mode.
[INFO:swift] Input `reset-system` to reset the system and clear the history.
[INFO:swift] Input `clear` to clear the history.
<<< <<< <<< 你好，叫什么名字
我是swift-robot，由swift开发的人工智能聊天机器人。有什么我可以帮助您的吗？
--------------------------------------------------