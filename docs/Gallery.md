# Parquet
Parquet æ˜¯ä¸€ç§åˆ—å¼å­˜å‚¨æ ¼å¼ï¼ˆColumnar Storage Formatï¼‰ï¼Œä¸“ä¸ºé«˜æ•ˆå­˜å‚¨å’Œå¤„ç†å¤§è§„æ¨¡æ•°æ®è€Œè®¾è®¡ï¼Œç‰¹åˆ«é€‚åˆå¤§æ•°æ®å¤„ç†å’Œåˆ†æåœºæ™¯ã€‚å®ƒæ˜¯ Apache å¼€å‘çš„å¼€æºé¡¹ç›®ï¼Œå¸¸ç”¨äº Hadoopã€Sparkã€Hiveã€Prestoã€Pandas ç­‰æ•°æ®å¤„ç†ç³»ç»Ÿä¸­ã€‚
```python
import pandas as pd

# ä¿å­˜ DataFrame ä¸º Parquet
df = pd.DataFrame({'a': [1, 2], 'b': ['x', 'y']})
df.to_parquet('train.parquet', engine='pyarrow')

# è¯»å– Parquet æ–‡ä»¶
df2 = pd.read_parquet('train.parquet', engine='pyarrow')
print(df2)
```

# è®­ç»ƒå‘½ä»¤è§£æ
PYTHONUNBUFFERED=1 python3 -m verl.trainer.main_ppo ...
PYTHONUNBUFFERED=1ï¼šè¾“å‡ºä¸ç¼“å†²ï¼Œç¡®ä¿è®­ç»ƒæ—¥å¿—å®æ—¶æ˜¾ç¤ºã€‚

python3 -m verl.trainer.main_ppoï¼šè¿è¡Œ PPO å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ¨¡å—ã€‚

æ•°æ®é…ç½®
data.train_files=/home/.../train.parquet
data.val_files=/home/.../test.parquet
data.train_batch_size=16
data.max_prompt_length=512
data.max_response_length=256
æŒ‡å®šè®­ç»ƒæ•°æ®ä¸éªŒè¯æ•°æ®çš„ Parquet æ–‡ä»¶è·¯å¾„ã€‚

è®¾ç½®è®­ç»ƒ batch å¤§å°ä¸º 16ã€‚

é™åˆ¶æ¨¡å‹è¾“å…¥ï¼ˆpromptï¼‰æœ€å¤§é•¿åº¦ä¸º 512ï¼Œè¾“å‡ºï¼ˆresponseï¼‰æœ€å¤§é•¿åº¦ä¸º 256ã€‚

Actor æ¨¡å‹é…ç½®
actor_rollout_ref.model.path=Qwen/Qwen2.5-0.5B-Instruct
actor_rollout_ref.actor.optim.lr=1e-6
actor_rollout_ref.actor.ppo_mini_batch_size=4
actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=4

actor_rollout_ref.model.pathï¼šä½¿ç”¨ Qwen2.5-0.5B-Instruct ä½œä¸º Actor æ¨¡å‹ã€‚

lr=1e-6ï¼šActor çš„å­¦ä¹ ç‡è®¾ä¸º 1e-6ã€‚

mini_batch æ˜¯ PPO æ¯è½®è¿­ä»£ä½¿ç”¨çš„å°æ‰¹é‡å¤§å°ï¼›micro_batch æ˜¯åœ¨ GPU ä¸Šå®é™…æ‰§è¡Œçš„ batch å¤§å°ï¼ˆç”¨äºæ˜¾å­˜æ§åˆ¶ï¼‰ã€‚


Rollout é…ç½®ï¼ˆç”ŸæˆåŠ¨ä½œ/é‡‡æ ·ï¼‰
actor_rollout_ref.rollout.log_prob_micro_batch_size_per_gpu=8
actor_rollout_ref.rollout.tensor_model_parallel_size=1
actor_rollout_ref.rollout.gpu_memory_utilization=0.4
æ§åˆ¶ rollout é˜¶æ®µæ¯ GPU çš„ batch å¤§å°ï¼ˆç”¨äºè®¡ç®— log probabilityï¼‰ã€‚

æŒ‡å®šå¹¶è¡Œåº¦ä¸º 1ï¼ˆä¸å¼€ tensor/model å¹¶è¡Œï¼‰ã€‚

ä½¿ç”¨ GPU æ˜¾å­˜çš„æœ€å¤§æ¯”ä¾‹ä¸º 0.4ï¼ˆé˜²æ­¢ OOMï¼‰

Reference æ¨¡å‹é…ç½®
actor_rollout_ref.ref.log_prob_micro_batch_size_per_gpu=4
reference model è´Ÿè´£å¯¹è¾“å‡ºè¿›è¡Œæ‰“åˆ†ï¼ˆlog_probï¼‰ï¼Œç”¨äºè®¡ç®—å¥–åŠ±å’Œ KL lossã€‚

Critic æ¨¡å‹é…ç½®
critic.model.path=Qwen/Qwen2.5-0.5B-Instruct
critic.optim.lr=1e-5
critic.ppo_micro_batch_size_per_gpu=4
ä½¿ç”¨åŒæ ·çš„æ¨¡å‹ä½œä¸º criticã€‚

critic å­¦ä¹ ç‡ä¸º 1e-5ã€‚

critic åœ¨æ¯ä¸ª GPU ä¸Šä½¿ç”¨çš„ micro batch sizeã€‚

algorithm.kl_ctrl.kl_coef=0.001
PPO ä¸­ç”¨äºæ§åˆ¶ Actor å’Œ Reference ä¹‹é—´çš„å·®å¼‚çš„ KL æƒ©ç½šç³»æ•°ã€‚è¶Šå¤§ï¼ŒActor è¶Šä¸æ•¢åç¦» Referenceã€‚


Traineré…ç½®
bash
å¤åˆ¶
ç¼–è¾‘
trainer.logger=console
trainer.val_before_train=False
trainer.n_gpus_per_node=1
trainer.nnodes=1
trainer.save_freq=10
trainer.test_freq=10
trainer.total_epochs=3
ä½¿ç”¨ console æ—¥å¿—å™¨ï¼ˆè€Œé wandb ç­‰ï¼‰ã€‚

val_before_train=Falseï¼šè·³è¿‡è®­ç»ƒå‰çš„éªŒè¯ã€‚

ä½¿ç”¨ 1 å¼  GPUï¼Œå•èŠ‚ç‚¹è®­ç»ƒã€‚

æ¯è®­ç»ƒ 10 ä¸ª epoch è¿›è¡Œä¸€æ¬¡ä¿å­˜å’Œæµ‹è¯•ã€‚

æ€»å…±è®­ç»ƒ 3 ä¸ª epochã€‚


# Actorï¼ˆæ¼”å‘˜ï¼‰
æ ¸å¿ƒæ¨¡å‹ï¼Œåœ¨è®­ç»ƒä¸­ä¸æ–­è°ƒæ•´æƒé‡ä»¥ä¼˜åŒ–å›ç­”è´¨é‡ã€‚

åœ¨æ¯ä¸ªè®­ç»ƒæ­¥éª¤ä¸­ï¼Œæ¥å—ä¸€ä¸ª promptï¼ˆè¾“å…¥ï¼‰ï¼Œç„¶åç”Ÿæˆ responseï¼ˆè¾“å‡ºï¼‰ã€‚

æ¨¡å‹çš„è¡Œä¸ºä¼šæ ¹æ® rewardï¼ˆç”± critic ç»™å‡ºï¼‰æ¥æ›´æ–°ï¼Œä½¿å…¶æ›´ç¬¦åˆäººç±»åå¥½æˆ–ç›®æ ‡ä»»åŠ¡ã€‚

Actor ä¼šå°è¯•ç”Ÿæˆç­–ç•¥ Ï€_Î¸ï¼Œå¹¶é€šè¿‡ PPO ç®—æ³•è¿›è¡Œç¨³å®šæ›´æ–°ï¼Œä½¿å…¶æ¯” Reference æ›´å¥½ï¼ˆæ›´é«˜ rewardï¼‰ã€‚

# Criticï¼ˆè¯„è®ºå®¶ï¼‰
è´Ÿè´£ç»™ Actor çš„è¾“å‡ºæ‰“åˆ†ï¼ˆå³ rewardï¼‰
å¯¹æ¯ä¸ª prompt å’Œ Actor è¾“å‡ºçš„ responseï¼Œé¢„æµ‹ä¸€ä¸ª reward åˆ†æ•°ï¼ˆç”¨ä½œ PPO ä¸­çš„ä»·å€¼å‡½æ•° V(s)ï¼‰ã€‚

Critic æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ª å›å½’æ¨¡å‹ï¼Œè¾“å‡º reward æœŸæœ›ã€‚

å®ƒæ˜¯ç›‘ç£ Actor å­¦ä¼šä»€ä¹ˆæ ·çš„å›ç­”æ˜¯â€œå¥½çš„â€çš„ä¾æ®ã€‚

# Referenceï¼ˆå‚è€ƒæ¨¡å‹ï¼‰
 Reference æ¨¡å‹ï¼šç”¨æ¥é™åˆ¶ Actor åç¦»å¤ªè¿œ
å†»ç»“ä¸æ›´æ–°ï¼Œä½œä¸º Actor çš„æ—§ç‰ˆæœ¬ã€‚

ç”¨æ¥è®¡ç®— KL æ•£åº¦ï¼ˆActor çš„è¾“å‡ºä¸ Reference çš„å·®è·ï¼‰ï¼Œé˜²æ­¢æ¨¡å‹è®­ç»ƒè¿‡åº¦åç¦»åˆå§‹èƒ½åŠ›ï¼ˆå³ä¿æŒâ€œåŸæ ·æ€§â€ï¼‰ã€‚

è¶Šåç¦»ï¼Œæƒ©ç½šè¶Šå¤§ï¼ˆé€šè¿‡ KL lossï¼‰ï¼Œæ§åˆ¶æ¨¡å‹å­¦ä¹ çš„â€œæ¿€è¿›ç¨‹åº¦â€ã€‚

Prompt â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Actor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Response
                    â”‚                        â”‚
                    â–¼                        â–¼
             Reference Model          Criticï¼ˆæ‰“åˆ†ï¼‰
                    â”‚                        â”‚
                    â–¼                        â–¼
             KL divergence             Reward Value
                    â”‚                        â”‚
                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
                           â”‚            â”‚
                    PPO æ›´æ–°ç­–ç•¥       PPO æ›´æ–°Critic


Maxwell-Jia/AIME_2024


# verlé…ç½®æ–‡ä»¶è·¯å¾„
[evaluation.yaml](..%2Fverl%2Fverl%2Ftrainer%2Fconfig%2Fevaluation.yaml)
[generation.yaml](..%2Fverl%2Fverl%2Ftrainer%2Fconfig%2Fgeneration.yaml)
[ppo_megatron_trainer.yaml](..%2Fverl%2Fverl%2Ftrainer%2Fconfig%2Fppo_megatron_trainer.yaml)
[ppo_trainer.yaml](..%2Fverl%2Fverl%2Ftrainer%2Fconfig%2Fppo_trainer.yaml)
[sft_trainer.yaml](..%2Fverl%2Fverl%2Ftrainer%2Fconfig%2Fsft_trainer.yaml)

## evaluation.yaml
data:
  path: /tmp/math_Qwen2-7B-Instruct.parquet
  prompt_key: prompt
  response_key: responses
  data_source_key: data_source
  reward_model_key: reward_model
pathï¼šæŒ‡å®šè¯„ä¼°æ•°æ®æ–‡ä»¶è·¯å¾„ï¼Œåº”ä¸º .parquet æ–‡ä»¶ã€‚

prompt_keyï¼šç”¨äºä»æ•°æ®ä¸­æå–è¾“å…¥æç¤ºçš„å­—æ®µåã€‚

response_keyï¼šè¡¨ç¤ºæ¨¡å‹ç”Ÿæˆå“åº”ï¼ˆç”Ÿæˆçš„ç­”æ¡ˆï¼‰å­—æ®µï¼Œé€šå¸¸æ˜¯å­—ç¬¦ä¸²åˆ—è¡¨ã€‚

data_source_keyï¼šç”¨äºåŒºåˆ†ä¸åŒæ•°æ®æ¥æºï¼Œåœ¨è¯„ä¼°æ—¶å¯åˆ†åˆ«è®¡ç®—å„æ¥æºçš„æŒ‡æ ‡ã€‚

reward_model_keyï¼šä»£è¡¨â€œå‚è€ƒç­”æ¡ˆâ€æˆ–è¯„åˆ†æ¨¡å‹è¾“å‡ºçš„å­—æ®µåï¼Œç”¨äºä¸ç”Ÿæˆè¾“å‡ºåšå¯¹æ¯”ã€‚


custom_reward_function:
  path: null
  name: compute_score
pathï¼šæŒ‡å®šåŒ…å«è‡ªå®šä¹‰å¥–åŠ±å‡½æ•°çš„ Python æ–‡ä»¶è·¯å¾„ã€‚å¦‚æœä¸º nullï¼Œå°†ä½¿ç”¨å†…ç½®çš„é¢„è®¾å‡½æ•°ã€‚
nameï¼šå‡½æ•°åï¼Œé»˜è®¤æ˜¯ compute_scoreã€‚å¦‚æœä½ åªå†™ä¸€ä¸ª compute_score å‡½æ•°ï¼Œå¯ä»¥ç®€å•ä½¿ç”¨é»˜è®¤å³å¯ã€‚

ray_initï¼ˆä¸ Ray åˆå§‹åŒ–ç›¸å…³ï¼‰
ray_init:
  num_cpus: null
  timeline_json_file: null
num_cpusï¼šç”¨äºæ§åˆ¶ Ray é›†ç¾¤ä½¿ç”¨çš„ CPU æ ¸å¿ƒæ•°ã€‚è‹¥ä¸º nullï¼ˆæˆ– Noneï¼‰ï¼Œé»˜è®¤ä½¿ç”¨ç³»ç»Ÿæ‰€æœ‰ CPUï¼Œä½†åœ¨ä¸€äº›é›†ç¾¤ç¯å¢ƒï¼ˆå¦‚ SLURMï¼‰å¯èƒ½ä¼šå¯¼è‡´å¡ä½ã€‚å»ºè®®æ˜ç¡®è®¾ç½®ä¸€ä¸ªå…è®¸ä½¿ç”¨çš„æ•°å­—ã€‚
timeline_json_fileï¼šå¯é€‰è·¯å¾„ï¼Œç”¨äºè¾“å‡º Ray Timeline çš„ JSON æ–‡ä»¶ï¼Œä¾¿äºè°ƒè¯•æ€§èƒ½é—®é¢˜ã€‚è‹¥ä¸éœ€è¦åˆ™è®¾ä¸º nullã€‚ã€

# generation.yaml
ä¸‹é¢æ˜¯å¯¹ä½ æä¾›çš„ `generation.yaml` é…ç½®é€é¡¹è§£é‡Šï¼Œç»“åˆ Verl å®˜æ–¹æ–‡æ¡£ä¸­çš„è¯´æ˜ï¼ˆæˆªè‡³ 2025 å¹´æ›´æ–°ï¼‰([Verl][1])ã€‚


## ğŸ§  Trainer è®¾ç½®

```yaml
trainer:
  nnodes: 1
  n_gpus_per_node: 8
  device: cuda
```

* **`nnodes`** ä¸ **`n_gpus_per_node`**ï¼šé…ç½®ç”¨äºç”Ÿæˆçš„èŠ‚ç‚¹æ•°å’Œæ¯èŠ‚ç‚¹ GPU æ•°é‡ï¼Œæ”¯æŒå¤šèŠ‚ç‚¹è®­ç»ƒï¼generationã€‚
* **`device`**ï¼šä½¿ç”¨ GPU (`cuda`) è€Œé CPUã€‚

---

## ğŸ“‚ Data éƒ¨åˆ†

```yaml
data:
  path: ~/data/rlhf/math/test.parquet
  prompt_key: prompt
  n_samples: 5
  output_path: /opt/tiger/math_Qwen2-7B-Instruct.parquet
  batch_size: 128
```

* **`path`**ï¼šè¾“å…¥æ•°æ®æºï¼Œé€šå¸¸æ˜¯ `.parquet` æ ¼å¼ã€‚
* **`prompt_key`**ï¼šæ•°æ®ä¸­çš„æç¤ºå­—æ®µåï¼ˆpromptï¼‰ã€‚
* **`n_samples`**ï¼šæ¯ä¸ª prompt ç”Ÿæˆå¤šå°‘ä¸ªç­”æ¡ˆæ ·æœ¬ï¼ˆè¿™é‡Œä¸º 5ï¼‰ã€‚
* **`output_path`**ï¼šç”Ÿæˆçš„æ ·æœ¬å°†ä¿å­˜åˆ°è¯¥è·¯å¾„ã€‚
* **`batch_size`**ï¼šä¸€æ¬¡å¤„ç†å¤šå°‘ promptï¼Œä»¥æé«˜å¹¶è¡Œååã€‚

---

## ğŸ§± Model é…ç½®

```yaml
model:
  path: ~/models/Qwen2-7B-Instruct
  external_lib: null
```

* **`path`**ï¼šæŒ‡å®šç”¨äºæ¨ç†æˆ–ç”Ÿæˆçš„æ¨¡å‹è·¯å¾„ã€‚
* **`external_lib`**ï¼šå¦‚ä¸º null ä½¿ç”¨é»˜è®¤åº“ï¼Œä¹Ÿå¯ä»¥æŒ‡å®šè‡ªå®šä¹‰åº“ï¼ˆä¾‹å¦‚æœ‰ç‰¹æ®Š tokenizer æˆ–åå¤„ç†ï¼‰ã€‚

---

## ğŸ”„ Rollout æ¨¡å—

```yaml
rollout:
  name: vllm
  mode: sync
  temperature: 1.0
  top_k: 50
  top_p: 0.7
  prompt_length: 1536
  response_length: 512
  dtype: bfloat16
  gpu_memory_utilization: 0.5
  ignore_eos: False
  enforce_eager: True
  free_cache_engine: True
  load_format: dummy_dtensor
  tensor_model_parallel_size: 1
  max_num_batched_tokens: 8192
  max_num_seqs: 1024
  log_prob_micro_batch_size_per_gpu: 8
  do_sample: True
  disable_log_stats: True
  enable_chunked_prefill: True
  n: 1
  calculate_log_probs: False
```

### ğŸ¯ åŸºç¡€é‡‡æ ·å‚æ•°

* **`name: vllm`**ï¼šé€‰æ‹© vLLM ä½œä¸º rollout engineã€‚
* **`mode: sync`**ï¼šåŒæ­¥æ¨¡å¼ï¼ˆasync è¡¨ç¤ºä½¿ç”¨ AsyncLLMï¼‰ã€‚
* **`temperature`ã€`top_k`ã€`top_p`**ï¼šæ§åˆ¶é‡‡æ ·ç­–ç•¥çš„éšæœºæ€§ä¸å¤šæ ·æ€§([Verl][1], [vLLM Forums][2])ã€‚

### âš™ï¸ vLLM ç‰¹å®šè®¾ç½®

* **`dtype: bfloat16`**ï¼šæŒ‡å®šç”Ÿæˆä½¿ç”¨çš„æµ®ç‚¹ç±»å‹ï¼Œä¸è®­ç»ƒ actor æ¨¡å‹ä¿æŒä¸€è‡´([Verl][1])ã€‚
* **`gpu_memory_utilization: 0.5`**ï¼švLLM å ç”¨ GPU æ€»å†…å­˜çš„æ¯”ä¾‹ï¼Œé€šå¸¸è®¾ 0.5â€“0.7 ä»¥å¹³è¡¡ååä¸ OOM é£é™©([Verl][1])ã€‚
* **`ignore_eos`**ï¼šä¸åœ¨ç”Ÿæˆç»“æŸæ—¶å› é‡ EOS token è€Œåœæ­¢ã€‚
* **`enforce_eager`**ï¼šå…³é—­ CUDA å›¾ï¼ˆCUDAGraphï¼‰ï¼Œé¿å… vLLM æŸäº›ç‰ˆæœ¬åœ¨ç¼“å­˜é‡Šæ”¾è¿‡ç¨‹ä¸­å´©æºƒ([Verl][1])ã€‚
* **`free_cache_engine`**ï¼šç”Ÿæˆåé‡Šæ”¾ KV cacheï¼Œé…åˆ `enforce_eager=True` ä»¥é™ä½å†…å­˜ã€‚
* **`load_format`**ï¼š`dummy_dtensor` ç”¨äº FSDP åç«¯çš„è™šæ‹Ÿåˆå§‹åŒ–æ–¹å¼ï¼Œå»¶è¿Ÿå®ç°æƒé‡åŒæ­¥([Verl][1])ã€‚
* **`tensor_model_parallel_size: 1`**ï¼šTP sizeï¼Œåªä½¿ç”¨ 1 ä»½ vLLM å‰¯æœ¬ã€‚

### æ‰¹é‡è°ƒä¼˜å‚æ•°

* **`max_num_batched_tokens: 8192`** å’Œ **`max_num_seqs: 1024`**ï¼šæ§åˆ¶æ¯æ¬¡ç”Ÿæˆæ—¶å¤„ç†çš„ token å’Œåºåˆ—æ•°ã€‚å¢å¤§å¯æé«˜ååæ€§èƒ½([Verl][1], [Verl][3])ã€‚
* **`log_prob_micro_batch_size_per_gpu: 8`**ï¼šæ¯ä¸ª GPU ç”¨äº logâ€‘prob è®¡ç®—çš„å°æ‰¹é‡å¤§å°ï¼ˆæ›¿ä»£è¿‡æ—¶ `log_prob_micro_batch_size`ï¼‰([Verl][1])ã€‚

### HF Rollout æ¨¡å¼å‚æ•°ï¼ˆå…¼å®¹æ€§ï¼‰

* **`do_sample`**ï¼šæ˜¯å¦é‡‡æ ·ï¼ˆä¸ greedyï¼‰ã€‚
* **`disable_log_stats`**ã€**`enable_chunked_prefill`**ï¼šå¯ç”¨äºç»Ÿè®¡æ—¥å¿—æ§åˆ¶ä¸å†…å­˜åˆ†å—é¢„å¡«å……ä¼˜åŒ–ã€‚
* **`n`**ï¼šæ¯ä¸ª prompt è¾“å‡ºæ¡æ•°ï¼Œé€šå¸¸ä¸º 1ã€‚

### è°ƒè¯•é€‰é¡¹

* **`calculate_log_probs: False`**ï¼šæ˜¯å¦åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­è®°å½• logâ€‘probï¼Œæ–¹ä¾¿è°ƒè¯•ä½†ä¼šå½±å“æ€§èƒ½ã€‚

---

## ğŸ¬ Actor æ¨¡å‹ç»†èŠ‚ï¼ˆActor Rolloutï¼‰

```yaml
actor:
  strategy: fsdp
  ulysses_sequence_parallel_size: 1
  entropy_from_logits_with_chunking: False
  entropy_checkpointing: False
  fsdp_config:
    fsdp_size: -1
    forward_prefetch: False
```

* **`strategy: fsdp`**ï¼šä½¿ç”¨ PyTorch FSDP åç«¯è®­ç»ƒ actorã€‚
* **å¹¶è¡Œä¸ç†µè®¡ç®—ç›¸å…³è®¾ç½®**ï¼ˆå¦‚ Ulysses parallel, checkpointingï¼‰æ˜¯ç”¨äºå†…å­˜ä¼˜åŒ–å’Œååæå‡çš„ç»†èŠ‚é€‰é¡¹ã€‚

---

## â˜ï¸ Ray åˆå§‹åŒ–ï¼ˆä¸ evaluation.yaml ä¸€è‡´ï¼‰

```yaml
ray_init:
  num_cpus: null
  timeline_json_file: null
```

* **`num_cpus: null`**ï¼šé»˜è®¤ä½¿ç”¨ç³»ç»Ÿå…¨éƒ¨ CPUï¼Œå»ºè®®åœ¨ SLURM ç­‰é›†ç¾¤ç¯å¢ƒä¸­è®¾ç½®ä¸ºç‰¹å®šå€¼ä»¥é¿å… hangã€‚
* **`timeline_json_file`**ï¼šå¯å†™å…¥ Ray æ€§èƒ½ timeline JSONï¼Œä»¥ç”¨äºè°ƒè¯•ã€‚

---

## âœ… æ€»ç»“ä¸€è§ˆ

| æ¨¡å—            | é…ç½®é¡¹                             | è¯´æ˜              |
| ------------- | ------------------------------- | --------------- |
| **trainer**   | nnodes / n\_gpus\_per\_node     | å¤šèŠ‚ç‚¹ä¸ GPU æ•°è®¾ç½®    |
|               | device                          | ä½¿ç”¨ GPU æˆ– CPU    |
| **data**      | n\_samples / batch\_size        | å¤šæ ·æœ¬ç”ŸæˆåŠå¹¶è¡Œåå      |
| **model**     | path / external\_lib            | æ¨¡å‹è·¯å¾„ä¸è‡ªå®šä¹‰åº“       |
| **rollout**   | name, mode                      | rollout å¼•æ“ä¸è°ƒç”¨æ¨¡å¼ |
|               | temperature, top\_k, top\_p     | ç”Ÿæˆç­–ç•¥æ§åˆ¶          |
|               | dtype, gpu\_memory\_utilization | å†…å­˜ç±»å‹åŠå ç”¨æ¯”ä¾‹       |
|               | load\_format                    | æƒé‡åŠ è½½æ–¹å¼åŒ¹é…è®­ç»ƒåç«¯    |
|               | batched\_tokens, seqs           | æ‰¹å¤„ç†è§„æ¨¡æ§åˆ¶         |
| **actor**     | fsdp / Ulysses parallel ç­‰       | actor è®­ç»ƒè¡Œä¸ºä¸ä¼˜åŒ–å¼€å…³ |
| **ray\_init** | num\_cpus, timeline\_json\_file | Ray èµ„æºæ§åˆ¶ä¸è°ƒè¯•è¾…åŠ©   |





# RLçš„æ•°æ®é›†ä¸­çš„interaction_kwargså­—æ®µçš„æ„æ€
https://verl.readthedocs.io/en/latest/sglang_multiturn/interaction_system.html?utm_source=chatgpt.com
ä¸ç‰¹å®šæ ·æœ¬å¯¹åº”çš„äº¤äº’é€»è¾‘å‚æ•°ï¼Œ Rollout é˜¶æ®µï¼ˆsglang_rollout.pyï¼‰ ï¼Œåœ¨å®é™… rollout è¿‡ç¨‹ä¸­ï¼Œå½“è¯·æ±‚çŠ¶æ€ä¸º INTERACTING æ—¶ï¼Œç³»ç»Ÿä¼šè¯»å– _req.interaction_kwargs ä¸­çš„ "name" å­—æ®µæ¥é€‰æ‹©äº¤äº’ agentï¼š
ç„¶åè°ƒç”¨å¯¹åº”çš„äº¤äº’ç±»å®ä¾‹å¼•å¯¼å¤šè½®å¯¹è¯ã€æä¾›åé¦ˆã€è®¡ç®—å¥–åŠ±ç­‰ï¼Œverl äº¤äº’ç³»ç»Ÿåœ¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒæœŸé—´æ”¯æŒåŠ¨æ€ã€å¤šè½®å¯¹è¯åé¦ˆã€‚è¯¥ç³»ç»Ÿå…è®¸æ¨¡å‹å‚ä¸è¿­ä»£é—®é¢˜è§£å†³åœºæ™¯ï¼Œäº¤äº’ä»£ç†å¯ä»¥æ ¹æ®æ¨¡å‹çš„å“åº”æä¾›çº æ­£åé¦ˆã€æŒ‡å¯¼æˆ–è¯„ä¼°ã€‚
å‚è€ƒï¼š verl/interactions/gsm8k_interaction.py
Verl æ¡†æ¶ä¸­ç”¨äº GSM8K ä»»åŠ¡çš„äº¤äº’ä»£ç†ç±» Gsm8kInteractionï¼Œå®ƒç»§æ‰¿è‡ª BaseInteractionï¼Œç”¨äºæŒ‡å¯¼è®­ç»ƒæ¨¡å‹åœ¨ RLHF æˆ– DPO è¿‡ç¨‹ä¸­é€šè¿‡å¤šè½®äº¤äº’æ–¹å¼æå‡æ•°å­¦é¢˜çš„è§£ç­”èƒ½åŠ›ã€‚
ğŸ’¡ interaction_kwargs åœ¨å“ªä½“ç°ï¼Ÿ
äº¤äº’æµç¨‹æ˜¯å›´ç»•æ ·æœ¬æºå¸¦çš„ interaction_kwargs æ¥é…ç½®çš„ï¼Œä¾‹å¦‚ï¼š
{
  "name": "gsm8k",
  "query": "Samantha has 12 apples, eats 3...",
  "ground_truth": "The correct answer is 9."
}
Verl åœ¨ rollout é˜¶æ®µï¼š
è°ƒç”¨ interaction = interaction_map["gsm8k"]
ç”¨ start_interaction(ground_truth="The correct answer is 9.") å¯åŠ¨çŠ¶æ€
æ¨¡å‹è¾“å‡ºåï¼Œgenerate_response() åˆ¤æ–­ç­”é¢˜å¯¹é”™
ç»™å‡ºå¥–åŠ±å’Œç¯å¢ƒåé¦ˆï¼ˆç”¨äºä¸‹ä¸€æ­¥è®­ç»ƒæˆ– samplingï¼‰


# verlä¸­çš„multiturnå¤šè½®å¯¹è¯æ˜¯å¦‚ä½•è®¡ç®—å¥–åŠ±çš„ï¼Ÿ
å¤šè½®å¯¹è¯ï¼ˆmultiâ€‘turn dialogueï¼‰çš„å¥–åŠ±æœºåˆ¶é€šå¸¸æ˜¯åŸºäº æ¯ä¸ªå¯¹è¯å›åˆï¼ˆturn-levelï¼‰ æˆ– æ•´ä¸ªå¯¹è¯è½¨è¿¹ï¼ˆtrajectory-levelï¼‰ 
| æ–¹æ³•ç±»åˆ«                      | å¥–åŠ±æ—¶æœº           | å¥–åŠ±æ¥æº                                   | é€‚ç”¨åœºæ™¯                          |
| ------------------------- | -------------- | -------------------------------------- | ----------------------------- |
| **å›åˆçº§ï¼ˆturn-levelï¼‰**       | æ¯ä¸ªå›åˆå          | tool æ‰“åˆ†ï¼Œå¦‚ correctness                  | GSM8Kã€QA æ¯æ­¥æ‰“åˆ†å‹å¯¹è¯              |
| **è½¨è¿¹çº§ï¼ˆtrajectory-levelï¼‰** | å¯¹è¯ç»“æŸå          | æœ€ç»ˆæ˜¯å¦æ­£ç¡®ã€judge æ¨¡å‹è¾“å‡º                      | MGPOã€ARTISTã€agent agents ç±» RL |
| **æ··åˆæ¨¡å‹**                  | å…¼å…·å›åˆå†…è¯„åˆ†ä¸æœ€ç»ˆè½¨è¿¹å¥–åŠ± | proxy æˆ– correctness + information gain | ä¿¡æ¯ç¨€ç–ã€å¤šå›åˆæ¨ç†ä»»åŠ¡                  |

# RLè®­ç»ƒé›†ä¸­çš„agent_nameå­—æ®µ
https://verl.readthedocs.io/en/latest/start/agentic_rl.html#overview
Tool Agent Loop è¦æ±‚å‘æ•°æ®é›†æ·»åŠ â€œagent_nameâ€å­—æ®µã€‚åœ¨è½¬å‡ºæœŸé—´ï¼Œå®ƒå°†æ ¹æ®æ­¤å­—æ®µé€‰æ‹©ä½¿ç”¨ tool_agent_loop æˆ– single_turn_agentï¼ˆé»˜è®¤ï¼‰ã€‚
RL æ•°æ®é›†ä¸­çš„ agent_name å­—æ®µä¸»è¦ç”¨äºæŒ‡å®šè®­ç»ƒæ—¶æ‰€ä½¿ç”¨çš„ä»£ç†å¾ªç¯ç­–ç•¥ï¼ˆagent loopï¼‰,åœ¨å¤šè½®äº¤äº’ï¼ˆmulti-turn conversationï¼‰å’Œå·¥å…·è°ƒç”¨ï¼ˆtool callsï¼‰åœºæ™¯ä¸­ï¼ŒTool Agent Loop è¦æ±‚æ•°æ®é›†é‡Œå¿…é¡»åŒ…å« "agent_name" å­—æ®µã€‚ç³»ç»Ÿä¼šä¾æ®è¯¥å­—æ®µå€¼ å†³å®šä½¿ç”¨ tool_agent_loop è¿˜æ˜¯ single_turn_agentï¼ˆé»˜è®¤ï¼‰ è¿›è¡Œåç»­ rollout å¤„ç†
| æ–‡ä»¶ / ç”¨ä¾‹                                                | agent\_name å€¼         | ä½œç”¨                           |
| ------------------------------------------------------ | --------------------- | ---------------------------- |
| GSM8K å·¥å…·è°ƒç”¨è®­ç»ƒè„šæœ¬ (`gsm8k_tool_agent_loop.py`)            | `"tool_agent"`        | ä½¿ç”¨å·¥å…·ä»£ç†å¾ªç¯ï¼Œå¼€å¯å·¥å…·è°ƒç”¨æ”¯æŒä¸è®¡ç®—å¥–åŠ±èåˆ     |
| Multiâ€‘turn React Agent æµ‹è¯• (`test_react_agent_loop.py`) | `"react_agent"`       | æµ‹è¯•å¸¦æœ‰å¤šè½® React è¡Œä¸ºçš„ agent loop  |
| å•è½®æµ‹è¯• (`test_basic_agent_loop.py`)                      | `"single_turn_agent"` | ä½¿ç”¨å•å›åˆä»£ç†é€»è¾‘ï¼Œæ— å·¥å…·è°ƒç”¨å‚ä¸            |
| æ•°å­¦è¡¨è¾¾å¼æ•°æ®ç”Ÿæˆ (`create_dataset.py`)                        | `"math_expression"`   | è‡ªå®šä¹‰ agent loop ç±»å‹ï¼Œç”¨äºç‰¹å®šæ•°å­¦è¡¨è¾¾ä»»åŠ¡ |

tool_agentï¼šç”¨äº GSM8K æ•°å­¦é¢˜ + å·¥å…·è°ƒç”¨è®­ç»ƒæµç¨‹ï¼Œä¼šä½¿ç”¨ ToolAgentLoop ç±»æ¥æ”¯æŒ stepâ€‘byâ€‘step æ¨ç†ã€è°ƒç”¨ calc_gsm8k_reward ç­‰å·¥å…·é€»è¾‘ã€‚

react_agentï¼šåœ¨ React Agentï¼ˆLangGraph é£æ ¼ï¼‰ä¸­ä½¿ç”¨ï¼Œç”¨äº multiâ€‘turn å·¥å…·è°ƒç”¨ + è‡ªåæ€æ¨ç†ç±»å‹æµç¨‹ã€‚

single_turn_agentï¼šé»˜è®¤å•è½®å“åº” agent loopï¼Œæ— å·¥å…·è°ƒç”¨èƒ½åŠ›ï¼Œé€‚ç”¨äºç®€å•ä¸€æ¬¡æ€§å›ç­”æµç¨‹ã€‚

math_expressionï¼šè‡ªå®šä¹‰ agent loopï¼Œä¾‹å¦‚ç”¨äºç”Ÿæˆæ•°å­¦è¡¨è¾¾å¼ä»»åŠ¡çš„æµç¨‹é€»è¾‘ã€‚

cat ./verl/verl/experimental/agent_loop/__init__.py

./verl/verl/experimental/agent_loop/tool_agent_loop.py
verl ä¸­çš„ä¸€ä¸ªæ ¸å¿ƒ agent loop runner ç±»å‹ï¼šToolAgentLoopã€‚å®ƒæ˜¯ä¸€ä¸ªåŸºäºå·¥å…·è°ƒç”¨çš„å¤šè½®å¯¹è¯ Agentï¼Œç”¨äºåœ¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒæˆ–æ¨ç†æ—¶ï¼Œè‡ªåŠ¨å¤„ç†å·¥å…·è°ƒç”¨ã€ç”Ÿæˆå“åº”ã€ä»¥åŠä¸ç¯å¢ƒäº¤äº’ã€‚

æ¯è½®å¾ªç¯åŒ…æ‹¬ï¼š

è°ƒç”¨ LLM ç”Ÿæˆå“åº”ï¼ˆassistant å›åˆï¼‰ã€‚

è§£æå“åº”ä¸­æ˜¯å¦åŒ…å«å·¥å…·è°ƒç”¨ã€‚

å¦‚æœæœ‰å·¥å…·è°ƒç”¨ï¼Œå°±æ‰§è¡Œå·¥å…·å¹¶ç”Ÿæˆ tool responseã€‚

å°† tool response æ‹¼æ¥è¿› promptï¼Œä½œä¸ºä¸‹ä¸€è½®è¾“å…¥ï¼ˆuser å›åˆï¼‰ã€‚

ç›´åˆ°è¾¾åˆ°è¿™äº›ç»ˆæ­¢æ¡ä»¶ä¹‹ä¸€ï¼š

è¾¾åˆ°æœ€å¤§ token é™åˆ¶ï¼ˆresponse_lengthï¼‰

è¾¾åˆ°æœ€å¤§ assistant turn æˆ– user turn

æ²¡æœ‰å·¥å…·è°ƒç”¨äº†

å·¥å…·è°ƒç”¨å¼‚å¸¸ï¼ˆå¦‚æŠ¥é”™


## ppo_trainer.yaml
è¿™ä¸ª `ppo_trainer.yaml` æ˜¯ **VERL**ï¼ˆVersatile RLHF Libraryï¼‰æ¡†æ¶ä¸­ç”¨äºé…ç½® **PPOï¼ˆProximal Policy Optimizationï¼‰è®­ç»ƒå™¨** çš„æ ¸å¿ƒé…ç½®æ–‡ä»¶ï¼Œé…ç½®å†…å®¹è¾ƒå¤šï¼Œä¸‹é¢æˆ‘ä¼šä»æ•´ä½“ç»“æ„å’Œå…³é”®å­—æ®µè§£é‡Šå…¶ä½œç”¨ä¸è®¾è®¡æ€è·¯ã€‚

---

## ğŸ§­ æ–‡ä»¶æ•´ä½“ç»“æ„æ¦‚è§ˆ

```yaml
defaults:
  - actor@actor_rollout_ref.actor: dp_actor
  ...
  - _self_
```

VERL ä½¿ç”¨ Hydra/OmegaConf çš„é…ç½®ç»§æ‰¿ç³»ç»Ÿï¼Œ`defaults` å—ç”¨æ¥å®šä¹‰é…ç½®ç»„åˆæ–¹å¼ï¼Œå³ï¼š

* æ¯ä¸€é¡¹ `<å­æ¨¡å—>@<è·¯å¾„>` è¡¨ç¤ºå°†ä¸€ä¸ª yaml æ–‡ä»¶ç»‘å®šåˆ°å½“å‰é…ç½®çš„å­æ¨¡å—ä¸Šï¼›
* `_self_` è¡¨ç¤ºå½“å‰è¿™ä¸ª `ppo_trainer.yaml` å¯ä»¥è¦†ç›–å‰é¢çš„é»˜è®¤è®¾ç½®ã€‚

---

## ğŸ§  æ ¸å¿ƒæ¨¡å—åˆ†è§£

---

### 1. `actor_rollout_ref`: Actorã€Rolloutã€Referenceæ¨¡å‹é…ç½®

è¿™ä¸ªæ¨¡å—ç»Ÿä¸€ç®¡ç†ï¼šactor æ¨¡å‹ï¼ˆè®­ç»ƒç”¨ï¼‰ã€rollout æ¨¡å‹ï¼ˆç”Ÿæˆç”¨ï¼‰ã€reference æ¨¡å‹ï¼ˆç”¨æ¥ç®—KLï¼‰ã€‚

```yaml
actor_rollout_ref:
  hybrid_engine: true  # ä½¿ç”¨æ··åˆå¼•æ“ï¼ˆactorã€rolloutã€refå…±å­˜ï¼‰
  model: {...}         # æ¨¡å‹åŠ è½½æ–¹å¼ã€LoRAã€æ˜¯å¦å¼€å¯ gradient checkpointing
  rollout: {...}       # rolloutä¸“ç”¨çš„ä¼˜åŒ–é…ç½®
  profiler: {...}      # profilerè®¾ç½®
```

ä¸»è¦å­—æ®µè§£æï¼š

#### `model.path`

æ¨¡å‹è·¯å¾„ï¼Œå¯ä»¥æ˜¯æœ¬åœ°æˆ–è¿œç¨‹ HuggingFace æ¨¡å‹ã€‚

#### `lora_rank`, `lora_alpha`, `target_modules`

LoRA çš„é…ç½®ï¼Œæ§åˆ¶æ˜¯å¦ä½ç§©å¾®è°ƒï¼Œå“ªäº›æ¨¡å—åº”ç”¨ LoRAã€‚

#### `enable_gradient_checkpointing`

èŠ‚çœå†…å­˜ï¼Œæé«˜ batch sizeï¼ˆä½†ä¼šå‡æ…¢è®­ç»ƒé€Ÿåº¦ï¼‰ã€‚

#### `rollout.enable_chunked_prefill`

æ˜¯å¦å¼€å¯ chunked prefillï¼ˆå¤§æ¨¡å‹æ—¶èƒ½æ˜¾è‘—æé«˜ååé‡ï¼‰ã€‚

---

### 2. `custom_reward_function`: è‡ªå®šä¹‰å¥–åŠ±å‡½æ•°è®¾ç½®

```yaml
custom_reward_function:
  path: null
  name: compute_score
```

* `path`: å¯å¡«å…¥ä¸€ä¸ª Python æ–‡ä»¶è·¯å¾„ï¼Œè‡ªå®šä¹‰å¥–åŠ±å‡½æ•°ï¼›
* `name`: å¥–åŠ±å‡½æ•°åç§°ï¼Œé»˜è®¤ä½¿ç”¨ `compute_score`ã€‚

---

### 3. `algorithm`: PPOç®—æ³•çš„å…³é”®è¶…å‚æ•°é…ç½®

```yaml
algorithm:
  gamma: 1.0
  lam: 1.0
  adv_estimator: gae
  use_kl_in_reward: False
  kl_ctrl:
    type: fixed
    kl_coef: 0.001
```

è§£é‡Šå‡ ä¸ªå…³é”®å‚æ•°ï¼š

| å‚æ•°å                | è¯´æ˜                                                    |
| ------------------ | ----------------------------------------------------- |
| `gamma`            | æŠ˜æ‰£å› å­ï¼Œè¶Šä½è¶ŠçŸ­è§†ï¼›1.0 ä»£è¡¨ä¸æŠ˜æ‰£æœªæ¥å¥–åŠ±ã€‚                             |
| `lam`              | GAE (Generalized Advantage Estimator) çš„æƒè¡¡ç³»æ•°ã€‚          |
| `adv_estimator`    | ä¼˜åŠ¿å‡½æ•°ä¼°è®¡æ–¹å¼ï¼Œæ¯”å¦‚ `"gae"`ã€`"reinforce_plus_plus"` ç­‰ã€‚        |
| `use_kl_in_reward` | æ˜¯å¦å°†KLæ•£åº¦ä½œä¸ºå¥–åŠ±æƒ©ç½šé¡¹ã€‚                                       |
| `kl_ctrl`          | KLæ§åˆ¶ç­–ç•¥ï¼šå¯ä»¥æ˜¯ `fixed` æˆ– `adaptive`ï¼›adaptive å¯ç”¨äºåŠ¨æ€è°ƒèŠ‚KLç³»æ•°ã€‚ |

---

### 4. `trainer`: PPOä¸»è®­ç»ƒå™¨é…ç½®

```yaml
trainer:
  total_epochs: 30
  n_gpus_per_node: 8
  logger: ['console', 'wandb']
  project_name: verl_examples
  experiment_name: gsm8k
  resume_mode: auto
  val_before_train: True
  test_freq: -1
```

é‡ç‚¹å­—æ®µè¯´æ˜ï¼š

| å­—æ®µ                                        | å«ä¹‰                 |
| ----------------------------------------- | ------------------ |
| `total_epochs`                            | æ€»å…±è®­ç»ƒ epoch æ•°       |
| `save_freq`                               | å¤šä¹…ä¿å­˜ä¸€æ¬¡æ¨¡å‹ï¼ˆæŒ‰ stepï¼‰   |
| `logger`                                  | æ—¥å¿—è¾“å‡ºåç«¯ï¼Œå¦‚æ§åˆ¶å°æˆ– wandb |
| `rollout_data_dir`, `validation_data_dir` | rollout/valçš„ç”Ÿæˆè¾“å‡ºç›®å½• |
| `resume_mode`                             | è‡ªåŠ¨æ¢å¤è®­ç»ƒ             |
| `val_only` / `val_before_train`           | æ§åˆ¶éªŒè¯è¡Œä¸º             |

å…³äº **Nsight GPU profiling**ï¼š

```yaml
profile_steps: null
controller_nsight_options:
  trace: "cuda,nvtx,cublas,ucx"
  cuda-memory-usage: "true"
```

åªæœ‰åœ¨ `profile_steps` æŒ‡å®šæ—¶æ‰ä¼šç”Ÿæ•ˆï¼Œç”¨äº GPU æ€§èƒ½è¯Šæ–­åˆ†æã€‚

---

### 5. `ray_init`: Rayåˆ†å¸ƒå¼åˆå§‹åŒ–é…ç½®

```yaml
ray_init:
  num_cpus: null
  timeline_json_file: null
```

é€šå¸¸ç”¨äºåˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒï¼Œè‹¥ä½¿ç”¨ SLURM å»ºè®®æ˜¾å¼æŒ‡å®š `num_cpus`ã€‚

---

## ğŸ’¡ å®æˆ˜å»ºè®®

| éœ€æ±‚            | æ¨èè®¾ç½®                                       |
| ------------- | ------------------------------------------ |
| **èŠ‚çœæ˜¾å­˜**      | å¼€å¯ `enable_gradient_checkpointing: true`   |
| **è®­ç»ƒå¤šè½®å¯¹è¯ä»»åŠ¡**  | è®¾ç½® `use_kl_in_reward: true`ï¼Œå¼€å¯å‚è€ƒæ¨¡å‹KLæƒ©ç½š     |
| **è°ƒè¯•**        | åªå¯ç”¨ console æ—¥å¿— + è®¾ç½®è¾ƒå° batch                |
| **LoRAå¾®è°ƒ**    | è®¾ç½® `lora_rank > 0` å¹¶é…ç½®åˆé€‚çš„ `target_modules` |
| **Profiling** | å¼€å¯ `profile_steps` å¹¶é…ç½® `nsight_options`    |


# HybridFlowåŸç†

## ğŸ§  æ•´ä½“ç†è§£ï¼šHybridFlow åœ¨ä»£ç ä¸­çš„ä½“ç°

HybridFlow çš„ç›®æ ‡æ˜¯å°† RLHF è®­ç»ƒæµç¨‹ï¼ˆå¦‚ PPOï¼‰è¡¨ç¤ºä¸ºçµæ´»çš„æ•°æ®æµå›¾ï¼Œå†å°†æ¯ä¸ªå­æ¨¡å—ï¼ˆActorã€RewardModelã€Critic ç­‰ï¼‰å°è£…ä¸ºå¹¶è¡Œè®¡ç®—èŠ‚ç‚¹ï¼Œå¹¶é€šè¿‡æ··åˆæ§åˆ¶å™¨è°ƒåº¦å®ƒä»¬ã€‚Verl çš„ä»£ç ç»“æ„å®Œç¾åæ˜ äº†è¿™ä¸€è®¾è®¡ã€‚

---

## ğŸ”§ æ ¸å¿ƒæ¨¡å—è®²è§£

### 1. **è®­ç»ƒå…¥å£ä¸è°ƒåº¦æ§åˆ¶å™¨**

```
verl/trainer/main_ppo.py
verl/trainer/ppo/ray_trainer.py
```

* **main\_ppo.py**ï¼šè®­ç»ƒå…¥å£ï¼ŒåŠ è½½é…ç½®ã€åˆå§‹åŒ–æ¨¡å‹ã€è®­ç»ƒå™¨ã€‚
* **ray\_trainer.py**ï¼š

  * å°è£… PPO ç­‰ç®—æ³•çš„ä¸»è®­ç»ƒå¾ªç¯ï¼›
  * è°ƒç”¨ rolloutã€rewardã€criticã€update ç­‰æ­¥éª¤ï¼›
  * æ¯ä¸€æ­¥éƒ½æ˜¯ä¸€ä¸ªã€Œflow stepã€ï¼Œè¢«é€å…¥ HybridFlow çš„è°ƒåº¦å›¾ä¸­ã€‚

âš™ï¸ è°ƒç”¨æ–¹å¼å¤§è‡´æ˜¯ï¼š

```python
rollouts = actor.generate()
rewards = reward_model.score(rollouts)
values = critic.evaluate(rollouts)
updated_actor = ppo.update(actor, rewards, values)
```

è¿™äº›æ“ä½œå®é™…æ˜¯ **ç”± HybridFlow DAG ç»“æ„ç»„ç»‡èµ·æ¥çš„ä»»åŠ¡æµ**ã€‚

---

### 2. **Workers æ¨¡å—ï¼šæ‰§è¡Œå•å…ƒå®ç°**

```
verl/workers/
```

è¿™æ˜¯æ‰§è¡Œç‰©ç†è®¡ç®—ä»»åŠ¡çš„â€œå·¥äººå±‚â€æ¨¡å—ï¼š

* `actor/`ï¼šè´Ÿè´£æ–‡æœ¬ç”Ÿæˆï¼ˆActor æ¨¡å‹çš„ rolloutï¼‰ã€‚

  * `dp_actor.py`: ä½¿ç”¨ FSDP çš„æ•°æ®å¹¶è¡Œç­–ç•¥ã€‚
  * `megatron_actor.py`: ä½¿ç”¨ Megatron çš„ nD å¹¶è¡Œç­–ç•¥ã€‚

* `critic/`ï¼šç”¨äºå€¼å‡½æ•°ä¼°è®¡ã€‚

* `reward_model/`ï¼šæ‰“åˆ†æ¨¡å—ã€‚

* `rollout/`ï¼šæ¥å…¥ä¸åŒæ¨ç†åç«¯ï¼ˆå¦‚ vllm, TGIï¼‰çš„æ¥å£ã€‚

â›“ï¸ å®ƒä»¬é€šè¿‡ `protocol.py` ä¸­çš„æ¥å£æ ‡å‡†åŒ–ä¸º HybridFlow å¯è°ƒåº¦ä»»åŠ¡ï¼Œæˆä¸º â€œæ•°æ®æµèŠ‚ç‚¹â€ã€‚

---

### 3. **Sharding Managerï¼šé«˜æ•ˆåˆ‡æ¢ä¸æ¨¡å‹é‡åˆ†å¸ƒ**

```
verl/workers/sharding_manager/
```

ç”¨äºæ”¯æŒ **Actor æ¨¡å‹åœ¨ rollout â†” è®­ç»ƒé˜¶æ®µä¹‹é—´çš„é«˜æ•ˆåˆ‡æ¢**ï¼Œæ˜¯ HybridFlow çš„æ•ˆç‡æ ¸å¿ƒï¼š

* `fsdp_ulysses.py`: åœ¨ FSDP ä¸‹è¿›è¡Œæƒé‡é‡åˆ†å¸ƒï¼›
* `megatron_vllm.py`: Megatron + vllm åœºæ™¯ä¸‹çš„ shard æ“ä½œã€‚

ğŸ“Œ ä¾‹å¦‚ï¼Œrollout é˜¶æ®µæˆ‘ä»¬éœ€è¦ fp16ã€æ— æ¢¯åº¦çš„æ¨ç†æ¨¡å‹ï¼›
è®­ç»ƒé˜¶æ®µéœ€è¦å…¨ç²¾åº¦æ¢¯åº¦æ¨¡å‹ï¼Œé‡ shard å°±å‘ç”Ÿåœ¨è¿™ä¸¤è€…ä¹‹é—´ã€‚

---

### 4. **é…ç½®ç³»ç»Ÿï¼ˆç”Ÿæˆ + è®­ç»ƒï¼‰**

```
verl/config/
  generation.yaml
  ppo_trainer.yaml
```

ç»Ÿä¸€å‚æ•°å…¥å£ï¼Œæè¿°ï¼š

* ä½¿ç”¨å“ªä¸ªæ¨¡å‹
* ä½¿ç”¨å“ªä¸ª worker backend
* rollout çš„ batch sizeã€è®¾å¤‡æ˜ å°„ç­‰
* PPO æˆ– GRPO çš„è®­ç»ƒå‚æ•°

æ‰€æœ‰è¿™äº›ä¼šå½±å“ HybridFlow DAG çš„å½¢çŠ¶ä¸è°ƒåº¦ç­–ç•¥ã€‚

---

### 5. **utils + reward\_score + dataset**

```
verl/utils/
verl/utils/reward_score/
verl/utils/dataset/
```

è¿™äº›æ¨¡å—æä¾›ï¼š

* Dataset åŠ è½½ä¸é¢„å¤„ç†ï¼›
* è‡ªå®šä¹‰å¥–åŠ±å‡½æ•°ï¼ˆå¦‚ mathã€gsm8kï¼‰ï¼›
* Sequence length balancing ç­‰ä¼˜åŒ–æŠ€å·§ã€‚

è¿™äº›ä¼šå‚ä¸ `rollout â†’ reward` è¿™ä¸€å­æµç¨‹çš„æ•°æ®å˜æ¢ã€‚

---

## âœ… HybridFlow çš„å…³é”®åˆ›æ–°åœ¨æ­¤ä½“ç°ï¼š

| æ¨¡å—        | å¯¹åº”åŸç†                        | å®ç°ä½ç½®                                          |
| --------- | --------------------------- | --------------------------------------------- |
| ä»»åŠ¡å›¾æŠ½è±¡     | RLHF è®­ç»ƒæµå»ºæ¨¡ä¸ºæ•°æ®æµå›¾             | `ray_trainer.py`, `protocol.py`               |
| æ··åˆæ§åˆ¶è°ƒåº¦    | æ§åˆ¶å™¨å†³å®šä½•æ—¶ä½¿ç”¨ sync / async æ‰§è¡ŒèŠ‚ç‚¹ | ä¸»è®­ç»ƒå™¨ã€é…ç½®ä¸­æ§åˆ¶                                    |
| 3D é‡åˆ†å¸ƒå¼•æ“  | é«˜æ•ˆåˆ‡æ¢ rollout / è®­ç»ƒ æ¨¡å‹çŠ¶æ€      | `sharding_manager/*.py`                       |
| åç«¯è§£è€¦      | Actor / Critic / RM æ”¯æŒå¤šç§æ¡†æ¶  | `dp_*.py`, `megatron_*.py`, `vllm_rollout.py` |
| æ¨¡å‹ / æ•°æ®å¹¶è¡Œ | ç”¨ Megatron / FSDP åšé€æ˜å¹¶è¡Œ     | `fsdp_workers.py`, `megatron_workers.py`      |

# WorkerGroup
## 1. ğŸ— WorkerGroup çš„æ„å»ºæ–¹å¼

* æ¯ä¸ª `WorkerGroup` ç®¡ç†ä¸€ç»„è¿œç¨‹è¿è¡Œçš„ `Worker` å®ä¾‹ï¼Œæ¯ä¸ª Worker é€šå¸¸éƒ¨ç½²åœ¨ä¸€å¼  GPU ä¸Šï¼Œè´Ÿè´£å…·ä½“è®¡ç®—ä»»åŠ¡ï¼Œæ„é€ è¿‡ç¨‹å³åœ¨ controller è¿›ç¨‹ä¸­å¯åŠ¨æ•´ä¸ªç»„ ([verl.readthedocs.io][1])ã€‚
* `WorkerGroup` æœ¬è´¨ä¸Šæ˜¯ controller ä¸å¤š GPU worker çš„ä»£ç†ï¼Œå®ƒè´Ÿè´£å°† controller è°ƒç”¨æ‹†è§£ä¸ºå¤š GPU å¹¶è¡Œè¿œç¨‹è°ƒç”¨ï¼ŒåŒ…æ‹¬ï¼š**åˆ†å‰²è¾“å…¥ã€åˆ†å‘ã€å¼‚æ­¥æ‰§è¡Œã€ç»“æœæ”¶é›†ã€æ‹¼æ¥** ([verl.readthedocs.io][2])ã€‚
* PPO ä¸­å®šä¹‰äº†ä¸‰ç§ä¸»è¦ WorkerGroupï¼š

  * **ActorRolloutRef**ï¼šå°è£… actor æ¨¡å‹ã€rollout æ¨ç†ã€reference ç­–ç•¥è®¡ç®—ï¼Œå¯ç»„åˆæˆä¸åŒè§’è‰²ä»¥æœ€å¤§å¤ç”¨ä»£ç ï¼Œå€ŸåŠ© create\_colocated\_worker\_cls åˆ›å»ºå…±äº« GPU çš„ Ray actor ç±»ï¼Œä¾¿äºå¿«é€Ÿæƒé‡è¿ç§»ã€å®ç° LoRA PPO æ¶æ„ ([verl.readthedocs.io][1])ã€‚
  * **Critic**ï¼šè´Ÿè´£å€¼å‡½æ•°ä¼°è®¡ã€‚
  * **Reward**ï¼šè´Ÿè´£å¥–åŠ±æ¨¡å‹è®¡ç®—ã€‚

è¿™äº›ç»„çš„æ„å»ºéƒ½é€šè¿‡ Ray çš„èµ„æºæ± ï¼ˆResourcePoolï¼‰å®Œæˆï¼Œå°† worker å’Œ GPU èµ„æºç»‘å®šåœ¨ä¸€èµ· ([verl.readthedocs.io][1])ã€‚

---

## 2. ğŸ§© Worker æ–¹æ³•å®šä¹‰ä¸è£…é¥°å™¨æœºåˆ¶

ä»¥ `ActorRolloutRefWorker` ä¸ºä¾‹ï¼š

* å®ƒå®šä¹‰äº†ä¸€ç³»åˆ—å¯¹ controller å¯è§çš„æ¥å£ï¼ˆremote RPC æ–¹æ³•ï¼‰ï¼Œå¦‚ï¼š

  * `init_model()`
  * `generate_sequences(DataProto)`
  * `compute_log_prob(...)`
  * `compute_ref_log_prob(...)`
  * `update_actor(...)`
  * `save_checkpoint()` ç­‰ ([verl.readthedocs.io][1])ã€‚

* è¿™äº›æ–¹æ³•é€šè¿‡ `@register(dispatch_mode=Dispatch.DP_COMPUTE_PROTO)` è£…é¥°å™¨æ ‡è®°ï¼Œæ˜ç¡®æŒ‡å‡ºï¼š

  * å¦‚ä½•åˆ‡åˆ†æ•°æ®ï¼ˆdispatch\_modeï¼‰
  * é˜»å¡è¡Œä¸ºã€æ‰§è¡Œæ¨¡å¼ç­‰ metadata
  * è£…é¥°å™¨æœ¬èº«å°è£…è¾“å…¥è¾“å‡ºå…ƒæ•°æ®ï¼Œä½†ä¸æ”¹å˜æœ¬ä½“é€»è¾‘ ([verl.readthedocs.io][2])ã€‚

* åœ¨ `WorkerGroup` åˆå§‹åŒ–æ—¶ï¼Œä¼šè‡ªåŠ¨æ‰«æ Worker ç±»ä¸­æ‰€æœ‰å¸¦æœ‰ `@register` çš„æ–¹æ³•ï¼Œå¹¶ç»‘å®šä¸º `WorkerGroup` çš„æ–¹æ³•ï¼š

  * è¯»å– dispatch\_modeï¼ŒæŸ¥è¡¨è·å–å¯¹åº”çš„ dispatch\_fn å’Œ collect\_fnï¼ˆä¾‹å¦‚ DP\_COMPUTE\_PROTO æ˜¾å¼åˆ†ç‰‡ã€æ”¶é›†é€»è¾‘ï¼‰ï¼›
  * è·å– execute\_fnï¼Œæ ¹æ® execute\_mode å†³å®šåŒæ­¥å¼‚æ­¥è¡Œä¸ºï¼›
  * æœ€ç»ˆç”Ÿæˆå¸¦åˆ†å‘ã€æ”¶é›†ã€å¹¶å‘æ‰§è¡Œå°è£…çš„ Group æ–¹æ³• ([verl.readthedocs.io][2])ã€‚

è¿™æ · controller åªéœ€è°ƒç”¨ï¼š

```python
output = actor_rollout_ref_wg.generate_sequences(data)
```

å°±è§¦å‘ï¼šæ•°æ®åˆ†å‰² â†’ åˆ†å‘è‡³å„ `Worker` â†’ å¹¶å‘ remote è°ƒç”¨ â†’ ç»“æœ ray.getã€æ‹¼æ¥è¿”å›ã€‚æ— éœ€æ‰‹åŠ¨å†™å¾ªç¯é€»è¾‘ ([verl.readthedocs.io][1])ã€‚

---

# PPO ä¸»å¾ªç¯

å€ŸåŠ©ä¸Šè¿° WorkerGroupï¼ŒPPO ä¸»æ§æµç¨‹å¯ä»¥åƒå•è¿›ç¨‹é‚£æ ·ç®€æ´ä¹¦å†™å„é˜¶æ®µï¼š

```python
for prompt in dataloader:
    output = actor_rollout_ref_wg.generate_sequences(prompt)
    old_lp = actor_rollout_ref_wg.compute_log_prob(output)
    ref_lp = actor_rollout_ref_wg.compute_ref_log_prob(output)
    values = critic_wg.compute_values(output)
    rewards = reward_wg.compute_scores(output)
    advantages = compute_advantages(values, rewards)
    merged = output.union(old_lp, ref_lp, values, rewards, advantages)
    actor_rollout_ref_wg.update_actor(merged)
    critic_wg.update_critic(merged)
```
generate_sequencesã€compute_log_probã€compute_ref_log_probã€compute_valuesã€compute_scoresã€update_actorã€update_critic å‡æ˜¯ WorkerGroup çš„æ–¹æ³•ï¼Œè¿™äº›ç”±è£…é¥°å™¨ï¼ˆå¦‚ @register(dispatch_mode=Dispatch.DP_COMPUTE_PROTO)ï¼‰å°è£…åï¼Œè‡ªåŠ¨æ‰§è¡Œï¼šè¾“å…¥åˆ‡ç‰‡ã€æ•°æ®åˆ†å‘åˆ° GPU workerã€è¿œç¨‹è°ƒç”¨å¹¶å‘è®¡ç®—ã€ç»“æœæ”¶é›†æ‹¼æ¥è¿”å›ç­‰æµç¨‹
control loop çœ‹èµ·æ¥åƒæ™®é€šçš„æµç¨‹æ§åˆ¶ï¼Œä½†å®é™…æ‰§è¡Œæ—¶æ¯ä¸€æ­¥éƒ½è°ƒç”¨è¿œç¨‹ GPU workerï¼Œcontroller è‡ªèº«å¹¶ä¸åšè¿™äº›è®¡ç®—ï¼Œåªè´Ÿè´£è°ƒåº¦å’Œæ¥æ”¶è¿”å›ã€‚
compute_advantages æ˜¯åœ¨ controller æœ¬åœ°æ‰§è¡Œçš„å°è§„æ¨¡è®¡ç®—ï¼Œä¸ä¼šæ¶‰åŠ GPUï¼Œåªæ˜¯ç®€å•åœ°ç”¨è¿”å›çš„ values å’Œ rewards è®¡ç®— advantageã€‚
è¿™äº›è°ƒç”¨èƒŒåéƒ½ç”± WorkerGroup åˆ†å‘æ‰§è¡Œç»†èŠ‚å°è£…ï¼Œcontroller æ— éœ€æ˜¾å¼å¤„ç†å¤š GPU çš„åˆ†å‘ä¸æ”¶é›†é€»è¾‘ ([verl.readthedocs.io][1])ã€‚

## 4. ä¸ºä»€ä¹ˆå®ƒâ€œåƒå•è¿›ç¨‹â€â€”â€”HybridFlow çš„ç²¾é«“
HybridFlow çš„æ ¸å¿ƒè®¾è®¡ç†å¿µæ˜¯ï¼šç”¨å•æ§åˆ¶å™¨ï¼ˆSingleâ€‘Controllerï¼‰å½¢å¼ç¼–å†™è®­ç»ƒæ§åˆ¶æµç¨‹ï¼Œè€Œå®é™…æ‰§è¡Œè¿‡ç¨‹åˆ™ç”±å¤šè¿›ç¨‹ï¼ˆå¤š GPU workersï¼‰å®Œæˆã€‚
è¿™æ ·ä¸€æ¥ï¼Œç”¨æˆ·åªéœ€ç¼–å†™åƒå•æœºè®­ç»ƒé‚£æ ·çš„ PPO æ§åˆ¶æµç¨‹ï¼›
æ¡†æ¶è´Ÿè´£è°ƒåº¦åˆ†å¸ƒå¼æ‰§è¡Œï¼Œä½¿å¾— backend å¯åœ¨ FSDPã€Megatronã€vLLM ç­‰ä¹‹é—´è‡ªç”±åˆ‡æ¢ï¼›
æ§åˆ¶é€»è¾‘å’Œè®¡ç®—é€»è¾‘å®Œå…¨è§£è€¦ï¼Œæ§åˆ¶æµç¨‹å¯¹ backend å®Œå…¨é€æ˜

---

## âœ… æ ¸å¿ƒè¦ç‚¹æ€»ç»“

| æ¦‚å¿µ                          | è¯´æ˜                                                                                |
| --------------------------- | --------------------------------------------------------------------------------- |
| `WorkerGroup` æ„é€             | åˆå§‹åŒ–æœŸé—´ç»‘å®šå¤šä¸ªè¿œç¨‹ Workerï¼Œå¹¶ä¸ºå…¶æ³¨å…¥å¸¦ dispatch/collect é€»è¾‘çš„æ–¹æ³•æ¥å£                                |
| `@register`                 | è£…é¥° Worker ä¸­çš„æ–¹æ³•ï¼ŒæŒ‡å®š dispatch æ¨¡å¼ï¼ˆå¦‚ DP\_COMPUTE\_PROTOï¼‰å’Œæ‰§è¡Œç‰¹æ€§                          |
| `Dispatch.DP_COMPUTE_PROTO` | æŒ‰æ•°æ®å¹¶è¡Œåˆ†ç‰‡è¾“å…¥ï¼Œè¿œç¨‹è°ƒç”¨å„ Workerï¼Œå†æ”¶é›†æ‹¼æ¥è¾“å‡º                                                    |
| æ§åˆ¶å™¨æµç¨‹                       | ä¸»è®­ç»ƒå¾ªç¯ä¿æŒç®€æ´ï¼Œå°†å¤æ‚çš„å¤šå¡å¼‚æ„æ‰§è¡Œéšè—äº WorkerGroup å°è£…ä¸­                                           |
| çµæ´»éƒ¨ç½²                        | ä¿®æ”¹ ResourcePool æˆ– WorkerGroup æ„æˆå³å¯åˆ‡æ¢ backendï¼ˆMegatron / FSDP / vLLM ç­‰ï¼‰ï¼Œæ— éœ€è§¦åŠæ§åˆ¶æµç¨‹é€»è¾‘ |

[1]: https://verl.readthedocs.io/en/latest/hybrid_flow.html?utm_source=chatgpt.com "HybridFlow Programming Guide - verl documentation - Read the Docs"
[2]: https://verl.readthedocs.io/en/latest/single_controller.html?utm_source=chatgpt.com "The Design of verl.single_controller - verl documentation"


# Reward
https://github.com/volcengine/verl/tree/main/verl/utils/reward_score
https://verl.readthedocs.io/en/latest/preparation/reward_function.html
å¯¹äºæ¯ä¸ªæ•°æ®é›†ï¼Œæˆ‘ä»¬éœ€è¦å®ç°å¥–åŠ±å‡½æ•°æˆ–åˆ©ç”¨å¥–åŠ±æ¨¡å‹æ¥è®¡ç®—ç”Ÿæˆå“åº”çš„å¥–åŠ±
æˆ‘ä»¬æ”¯æŒ GSM8k å’Œ MATH æ•°æ®é›†çš„å¥–åŠ±å‡½æ•°ã€‚å¯¹äº RLHF æ•°æ®é›†ï¼ˆä¾‹å¦‚ full_hh_rlhfï¼‰å’Œä»£ç ç”Ÿæˆï¼ˆä¾‹å¦‚ APPSï¼‰ï¼Œæˆ‘ä»¬åˆ†åˆ«ä½¿ç”¨å¥–åŠ±æ¨¡å‹å’Œ SandBoxï¼ˆå³å°†å¼€æºï¼‰è¿›è¡Œè¯„ä¼°ã€‚
åœ¨ PPO è®­ç»ƒåè„šæœ¬ main_ppo.py çš„å…¥å£ç‚¹ä¸­ï¼Œæˆ‘ä»¬å®ç°äº†ä¸€ä¸ª RewardManagerï¼Œå®ƒåˆ©ç”¨é¢„å…ˆå®ç°çš„å¥–åŠ±å‡½æ•°æ¥è®¡ç®—æ¯ä¸ªå“åº”çš„åˆ†æ•°ã€‚
åœ¨ RewardManager ä¸­ï¼Œæˆ‘ä»¬å®ç°äº†ä¸€ä¸ª __call__ å‡½æ•°æ¥è®¡ç®—æ¯ä¸ªå“åº”çš„åˆ†æ•°ã€‚æ‰€æœ‰å¥–åŠ±å‡½æ•°å‡ç”± compute_score_fn æ‰§è¡Œã€‚è¾“å…¥æ˜¯ä¸€ä¸ª DataProtoï¼Œå…¶ä¸­åŒ…æ‹¬ï¼š
input_idsã€attention_maskï¼šåº”ç”¨ chat_template åçš„ input_ids å’Œ attention_maskï¼ŒåŒ…æ‹¬æç¤ºå’Œå“åº”
å“åº” ï¼šå“åº”æ ‡è®°
ground_truthï¼šå½“å‰æç¤ºçš„åŸºæœ¬äº‹å®å­—ç¬¦ä¸²ã€‚å­˜å‚¨åœ¨ DataProto ä¸­çš„ non_tensor_batchï¼Œåº”åœ¨ parquet æ–‡ä»¶ä¸­è¿›è¡Œé¢„å¤„ç†ã€‚
data_sourceï¼šå½“å‰æç¤ºçš„æ•°æ®é›†åç§°ã€‚å­˜å‚¨åœ¨ non_tensor_batch DataProto ä¸­ï¼Œåº”åœ¨ parquet æ–‡ä»¶ä¸­è¿›è¡Œé¢„å¤„ç†ã€‚ å¯¹å“åº”è¿›è¡Œdetokenizeåï¼Œå“åº”å­—ç¬¦ä¸²å’ŒåŸºæœ¬äº‹å®å­—ç¬¦ä¸²å°†è¾“å…¥åˆ° compute_score_fn ä¸­ï¼Œä»¥è®¡ç®—æ¯ä¸ªå“åº”çš„åˆ†æ•°ã€‚

## ä¸€ã€ä¸»è¦æ¨¡å—å¯¹æ¯”

### âœ… `default_compute_score`ï¼ˆå·²æ›¿ä»£ `_default_compute_score`ï¼‰

* **åŠŸèƒ½**ï¼šé€šç”¨ç­–ç•¥ï¼Œç”¨äºå¸¸è§„ RLHF æ•°æ®é›†ï¼Œå°¤å…¶å½“æ²¡æœ‰ä¸“ç”¨å‡½æ•°æ—¶çš„é»˜è®¤å¤„ç†æ–¹å¼ã€‚
* **å¤‡æ³¨**ï¼šæ—§ç‰ˆå‡½æ•° `_default_compute_score` å·²è¢«å¼ƒç”¨ï¼Œå®˜æ–¹å»ºè®®ä½¿ç”¨ `verl.utils.reward_score.default_compute_score` ([GitHub][1])ã€‚

### ğŸ“Š `gsm8k.py`

* **ç”¨é€”**ï¼šé’ˆå¯¹ GSM8k æ•°å­¦é¢˜æ•°æ®é›†çš„è§„åˆ™è®¡ç®—æ–¹å¼ã€‚
* **å®ç°æ–¹å¼**ï¼š

  * è¦æ±‚æ¨¡å‹å°†ç­”æ¡ˆæ”¾åœ¨æ ¼å¼ `#### ç­”æ¡ˆ` ä¸­ï¼›
  * **å®Œå…¨æ­£ç¡®** â†’ reward = 1ï¼›
  * **æ ¼å¼æ­£ç¡®ä½†ç­”æ¡ˆé”™è¯¯** â†’ reward = 0.1ï¼›
  * **æ ¼å¼é”™è¯¯** â†’ reward = 0 ([verl.readthedocs.io][2], [Hugging Face][3])ã€‚

### ğŸ“ `math.py`

* **ç”¨é€”**ï¼šå¤„ç† MATH æ•°æ®é›†ï¼ˆå¤æ‚æ•°å­¦æ¨ç†ï¼‰ã€‚
* **å®ç°æ–¹å¼**ï¼šå‚è€ƒ lm-evaluation-harness çš„ Hendrycks æ–¹æ³•ï¼Œå®ç°åˆ†æ­¥æ¨ç†æ­£ç¡®åº¦è¡¡é‡å’Œå¾—åˆ†è¯„ä¼° ([verl.readthedocs.io][2], [Hugging Face][3])ã€‚

### ğŸ’» `prime_code`ï¼ˆä»£ç ç”Ÿæˆä»»åŠ¡ï¼‰

* **ç”¨é€”**ï¼šé’ˆå¯¹ç”Ÿæˆä»£ç æ­£ç¡®æ€§çš„è¯„ä¼°ã€‚
* **å®ç°æ–¹å¼**ï¼š

  * é¦–å…ˆå¯¹æ‰€æœ‰æµ‹è¯•ç”¨ä¾‹è¿è¡Œ correctness æ£€æŸ¥ã€‚
  * å¦‚æœå…¨éƒ¨é€šè¿‡ï¼Œåˆ™ç›´æ¥è¿”å›æˆåŠŸï¼›
  * è‹¥å¤±è´¥ï¼Œåˆ™å¯¹å‰ 10 æµ‹è¯•ç”¨ä¾‹é€ä¸ªåˆ¤æ–­æ­£ç¡®ç‡ï¼ˆcontinuous æ¨¡å¼ä¸‹å§‹ç»ˆæ£€æŸ¥å‰ 10ï¼‰ ([GitHub][4])ã€‚

---

## äºŒã€ä¸»è¦åŒºåˆ«ä¸€è§ˆ

| æ¨¡å—                      | åº”ç”¨åœºæ™¯           | è®¡ç®—æ–¹å¼            | å¾—åˆ†ç­–ç•¥         |
| ----------------------- | -------------- | --------------- | ------------ |
| `default_compute_score` | é€šç”¨ RLHF æˆ–è‡ªå®šä¹‰ä»»åŠ¡ | ä½¿ç”¨é»˜è®¤åˆ†å€¼ç­–ç•¥        | è¾ƒé€šç”¨ï¼Œä½†è¾ƒé€šç”¨åŒ–    |
| `gsm8k.py`              | GSM8k æ•°å­¦é¢˜      | å­—ç¬¦ä¸²åŒ¹é…æ ¼å¼ + ç­”æ¡ˆå¯¹æ¯”  | 1ï¼0.1ï¼0 ä¸‰æ¡£è¯„åˆ† |
| `math.py`               | MATH æ•°æ®é›†       | æŒ‰è§£é¢˜æ­¥éª¤é€»è¾‘è¯„ä¼°æ­£ç¡®æ€§    | è¿ç»­åˆ†å€¼ï¼Œçœ‹æ¨ç†è´¨é‡   |
| `prime_code`            | ä»£ç ç”Ÿæˆï¼ç¼–ç¨‹ä»»åŠ¡      | æµ‹è¯•ç”¨ä¾‹æ­£ç¡®æ€§å’Œè¿ç»­æ€§åˆ†æ­¥æ£€æµ‹ | å…¨å¯¹å³æ»¡åˆ†ï¼Œéƒ¨åˆ†å¯¹æŒ‰æ¯”ä¾‹ |
