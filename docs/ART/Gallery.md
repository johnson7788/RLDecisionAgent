# ModelConfig
ModelConfig å¹¶ä¸æ˜¯ä¸€ä¸ªå…·ä½“çš„ç±»å®šä¹‰ï¼Œè€Œæ˜¯ä¸€ä¸ªç±»å‹å˜é‡ï¼ˆTypeVarï¼‰ã€‚
ModelConfig = TypeVar("ModelConfig", bound=BaseModel | None)
è¿™é‡Œçš„ bound=BaseModel | None æ„å‘³ç€ ModelConfig å¯ä»¥æ˜¯ä»»ä½•ç»§æ‰¿è‡ª pydantic.BaseModel çš„ç±»ï¼Œæˆ–è€… Noneã€‚
1. çµæ´»æ€§: Model æˆ– TrainableModel
   çš„å®ä¾‹å¯ä»¥æºå¸¦ä¸€ä¸ªå…·ä½“çš„é…ç½®å¯¹è±¡ï¼Œè¿™ä¸ªå¯¹è±¡çš„ç±»å‹ä¸æ˜¯å†™æ­»çš„ã€‚ä½ å¯ä»¥å®šä¹‰è‡ªå·±çš„é…ç½®ç±»ï¼ˆåªè¦å®ƒç»§æ‰¿è‡ª
   pydantic.BaseModelï¼‰ï¼Œç„¶åå°†å®ƒç”¨äº Model çš„ config å­—æ®µã€‚
2. ç±»å‹å®‰å…¨: ä½¿ç”¨æ³›å‹å’Œç±»å‹å˜é‡ï¼Œé™æ€ç±»å‹æ£€æŸ¥å·¥å…·ï¼ˆå¦‚ MyPyï¼‰å¯ä»¥çŸ¥é“å½“ä½ åˆ›å»ºä¸€ä¸ª Model å®ä¾‹æ—¶ï¼Œå®ƒçš„ config å±æ€§åº”è¯¥æ˜¯ä»€ä¹ˆç±»å‹ã€‚
3. æ— å…·ä½“å®ç°: art åº“æœ¬èº«ä¸æä¾›ä¸€ä¸ªåä¸º ModelConfig çš„å…·ä½“é…ç½®ç±»ã€‚å®ƒåªæ˜¯å®šä¹‰äº†ä¸€ä¸ªâ€œæ’æ§½â€æˆ–â€œæ¨¡æ¿â€ï¼Œè®©ä½¿ç”¨è€…æ¥å¡«å……å…·ä½“çš„é…ç½®ã€‚

  æ€»ç»“ä¸€ä¸‹ï¼š

* å®ƒæ˜¯ä»€ä¹ˆï¼Ÿ ModelConfig æ˜¯ä¸€ä¸ªç±»å‹å ä½ç¬¦ï¼ˆæ³›å‹ç±»å‹å˜é‡ï¼‰ï¼Œä»£è¡¨ä»»ä½•ç”¨äºæ¨¡å‹é…ç½®ã€ä¸”ç»§æ‰¿è‡ª pydantic.BaseModel çš„ç±»ã€‚
* é‡Œé¢æœ‰ä»€ä¹ˆï¼Ÿ å®ƒçš„å†…å®¹å–å†³äºä½ æˆ–åº“çš„å…¶ä»–éƒ¨åˆ†å¦‚ä½•å®šä¹‰å¹¶ä¼ å…¥ä¸€ä¸ªå…·ä½“çš„é…ç½®ç±»ã€‚å®ƒæœ¬èº«æ²¡æœ‰å­—æ®µã€‚

# Unsloth
Unsloth æ˜¯ä¸€ç§ä¼˜åŒ–åçš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒåŸºç¡€è®¾æ–½ï¼Œå®ç°äº† GRPO ç­‰ç®—æ³•ã€‚
ART æ˜¯åŸºäº Unsloth çš„æ›´é«˜å±‚æ„å»ºï¼Œæä¾›ä»£ç†è®­ç»ƒæ•´ä¸ª pipelineï¼ˆè½¨è¿¹é‡‡é›†ã€å¥–åŠ±è¯„ä¼°ã€è®­ç»ƒå¾ªç¯ç­‰ï¼‰çš„ä¸€ä½“åŒ–å·¥å…·åº“ã€‚

# await model.register(backend)ï¼Œæ˜¯è¿è¡Œåˆ°è¿™é‡Œ
ART/src/art/model.py
async def register(
        self,
        model: "Model",
    ) -> None:
        """

# weave 
å…¶å®æ˜¯ä¸€ä¸ª LLM è®­ç»ƒä¸æ¨ç†çš„å¯è§‚æµ‹æ€§ / è¿½è¸ª (observability & tracing) åº“ï¼Œä¸»è¦ç”¨äºè®°å½•æ¨¡å‹çš„è¿è¡Œä¿¡æ¯ã€æ—¥å¿—ã€æŒ‡æ ‡å’Œè°ƒç”¨é“¾ã€‚
å®ƒæ˜¯ Weights & Biases (wandb) æ——ä¸‹çš„ä¸€ä¸ªé¡¹ç›®ï¼Œå®šä½ç±»ä¼¼äºï¼š
ç»™ LLM åº”ç”¨ åŠ ä¸Šè‡ªåŠ¨åŒ–çš„ logging / tracing / metrics æ”¶é›†ï¼›
åœ¨ æ¨ç† / è®­ç»ƒ / agent rollout æ—¶ï¼ŒæŠŠè°ƒç”¨é“¾ã€promptã€responseã€latencyã€é”™è¯¯ä¿¡æ¯ç­‰è¿½è¸ªä¸‹æ¥ï¼›
åœ¨ web dashboard ä¸Šå¯è§†åŒ–è°ƒç”¨è¿‡ç¨‹ï¼Œæ–¹ä¾¿è°ƒè¯•ã€ç›‘æ§å’Œå¤ç°ã€‚


# æµ‹è¯•MCPå·¥å…·
è¿è¡Œserverç«¯
cd backend/ART_mcp-rl/servers/python/mcp_caculator
python server.py --transport sse

é…ç½®config.json
cat config.json
{
  "mcpServers": {
    "everything": {
      "type": "sse",
      "url": "http://localhost:8001/sse"
    },
    "my-server": {
      "command": "node",
      "args": ["build/index.js", "arg1", "arg2"],
      "env": {
        "key": "value",
        "key2": "value2"
      }
    }
  }
}
å¯åŠ¨æµ‹è¯•å·¥å…·
npx @modelcontextprotocol/inspector --config ./config.json --server everything

# LocalBackendä¸­çš„in_processå‚æ•°
in_process æ˜¯ä¸€ä¸ªå¼€å…³å‚æ•°ï¼Œç”¨æ¥å†³å®š æ¨¡å‹æœåŠ¡ï¼ˆmodel-serviceï¼‰æ˜¯ç›´æ¥åœ¨å½“å‰ Python è¿›ç¨‹é‡Œè¿è¡Œï¼Œè¿˜æ˜¯è¦ fork / spawn æˆä¸€ä¸ªç‹¬ç«‹çš„å­è¿›ç¨‹è¿è¡Œã€‚


# UnslothåŠ è½½åŒç­‰æ¨¡å‹æ—¶ï¼Œä¼šè‡ªåŠ¨åŠ è½½èŠ‚çœæ˜¾å­˜çš„æ›´å°ç‰ˆæœ¬
Unsloth åœ¨â€œå·å·å¸®ä½ çœæ˜¾å­˜/æé€Ÿâ€ï¼š

ä½ åœ¨ vLLM çš„ engine_args é‡ŒæŠŠ model è®¾æˆäº† Qwen/Qwen2.5-0.5B-Instructï¼Œæ‰€ä»¥ã€Œå¯¹å¤–å®£ç§°ã€çš„ base_model å°±æ˜¯å®ƒã€‚

ä½†ä½ é¡¹ç›®é‡Œ å¼•å…¥äº† Unslothï¼ˆæ—¥å¿—é‡Œæœ‰ Unsloth: Will patch...ï¼‰ï¼ŒUnsloth ä¼šå¯¹å¸¸è§æ¨¡å‹åš è‡ªåŠ¨ä¼˜åŒ–ä¸æ›¿æ¢ï¼šå½“æ£€æµ‹åˆ°å¯ç”¨çš„åŒç­‰æ¨¡å‹çš„ 4bit é¢„é‡åŒ–æƒé‡ æ—¶ï¼Œä¼šæŠŠå®é™…åŠ è½½çš„æƒé‡æ¢æˆè‡ªå·±åœ¨ HF ä¸Šçš„é•œåƒï¼Œä¾‹å¦‚
unsloth/qwen2.5-0.5b-instruct-unsloth-bnb-4bitï¼Œå¹¶åŒæ—¶æŠŠ vLLM çš„ quantization=bitsandbytesã€load_format=bitsandbytes ç­‰å‚æ•°ä¸€å¹¶è®¾ç½®å¥½ã€‚
è¿™å°±æ˜¯ä¸ºå•¥åé¢ vLLM çš„åˆå§‹åŒ–ä¸ä¸‹è½½æ—¥å¿—æ˜¾ç¤ºçš„æ˜¯ unsloth/...-bnb-4bitã€‚

ç®€å•è¯´ï¼šåä¹‰ä¸Šä»æ˜¯ Qwen å®˜æ–¹æ¨¡å‹ï¼›å®é™…åŠ è½½çš„æ˜¯ Unsloth çš„ 4bit ç­‰ä»·æƒé‡ï¼Œè¿™æ ·æ›´çœæ˜¾å­˜ã€æ›´å¿«ï¼Œä½†è¡Œä¸ºï¼ˆé™¤äº†é‡åŒ–è¯¯å·®ï¼‰ä¸åŸæ¨¡å‹å¯¹é½ã€‚


# qwençš„chat_template.jinjaï¼Œ  chatçš„jinjaæ¨¡ç‰ˆ
{%- if tools %}
    {{- '<|im_start|>system\n' }}
    {%- if messages[0]['role'] == 'system' %}
        {{- messages[0]['content'] }}
    {%- else %}
        {{- 'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.' }}
    {%- endif %}
    {{- "\n\n# Tools\n\nYou may call one or more functions to assist with the user query.\n\nYou are provided with function signatures within <tools></tools> XML tags:\n<tools>" }}
    {%- for tool in tools %}
        {{- "\n" }}
        {{- tool | tojson }}
    {%- endfor %}
    {{- "\n</tools>\n\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\n<tool_call>\n{\"name\": <function-name>, \"arguments\": <args-json-object>}\n</tool_call><|im_end|>\n" }}
{%- else %}
    {%- if messages[0]['role'] == 'system' %}
        {{- '<|im_start|>system\n' + messages[0]['content'] + '<|im_end|>\n' }}
    {%- else %}
        {{- '<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n' }}
    {%- endif %}
{%- endif %}
{%- for message in messages %}
    {%- if (message.role == "user") or (message.role == "system" and not loop.first) or (message.role == "assistant" and not message.tool_calls) %}
        {{- '<|im_start|>' + message.role + '\n' + message.content + '<|im_end|>' + '\n' }}
    {%- elif message.role == "assistant" %}
        {{- '<|im_start|>' + message.role }}
        {%- if message.content %}
            {{- '\n' + message.content }}
        {%- endif %}
        {%- for tool_call in message.tool_calls %}
            {%- if tool_call.function is defined %}
                {%- set tool_call = tool_call.function %}
            {%- endif %}
            {{- '\n<tool_call>\n{"name": "' }}
            {{- tool_call.name }}
            {{- '", "arguments": ' }}
            {{- tool_call.arguments | tojson }}
            {{- '}\n</tool_call>' }}
        {%- endfor %}
        {{- '<|im_end|>\n' }}
    {%- elif message.role == "tool" %}
        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != "tool") %}
            {{- '<|im_start|>user' }}
        {%- endif %}
        {{- '\n<tool_response>\n' }}
        {{- message.content }}
        {{- '\n</tool_response>' }}
        {%- if loop.last or (messages[loop.index0 + 1].role != "tool") %}
            {{- '<|im_end|>\n' }}
        {%- endif %}
    {%- endif %}
{%- endfor %}
{%- if add_generation_prompt %}
    {{- '<|im_start|>assistant\n' }}
{%- endif %}


# å¤šå¡è®­ç»ƒ
LocalBackend ç±»ä¼šåœ¨ä½ æ™ºèƒ½ä½“è¿è¡Œçš„åŒä¸€å°æœºå™¨ä¸Šå¯åŠ¨ä¸€ä¸ª vLLM æœåŠ¡å™¨å’Œä¸€ä¸ª Unsloth æˆ– torchtune å®ä¾‹ã€‚
https://github.com/OpenPipe/ART/pull/163/commits
å¤š GPU æ”¯æŒæ˜¯è¿‘æœŸåŠ åˆ° torchtune serviceï¼›åŒæ—¶æœ‰ç»´æŠ¤è€…è¯´æ˜â€œtorchtune åš full finetuneï¼Œé€šå¸¸ç”¨äºå•èŠ‚ç‚¹å¤š GPUï¼›Unsloth é€‚åˆå• GPU çš„ LoRAâ€ã€‚
è®­ç»ƒåç«¯å‡çº§æˆå¯åˆ†å¸ƒå¼çš„ torchtune serviceï¼Œå¹¶åœ¨æ¨ç†ä¾§é…åˆ vLLMï¼Œä»è€Œè®©ä½ åœ¨ä¸€å°å¤š GPU æœºå™¨ï¼Œæˆ–é€šè¿‡ SkyPilot èµ·çš„å¤š GPU èŠ‚ç‚¹ä¸Šåš å•æœºå¤šå¡ è®­ç»ƒ


# æµ‹è¯•å…¼å®¹çš„openai
python -m ART.src.art.openai_patch


# å¯ä»¥è€ƒè™‘åœ¨ç¯å¢ƒå˜é‡é‡Œé¢æ·»åŠ IMPORT_UNSLOTHå’ŒIMPORT_PEFTï¼Œå› ä¸ºART/src/art/local/backend.pyé‡Œè®¾ç½®äº†ï¼Œè®©å®ƒä»¬æå‰åŠ è½½ï¼Œæå‡æ€§èƒ½
IMPORT_UNSLOTH=1
IMPORT_PEFT=1


# metricså€¼
exception_rateï¼š è½¨è¿¹å¼‚å¸¸çš„æ•°é‡
reward_std_devï¼šreward çš„å¹³å‡æ ‡å‡†å·®

# Scenario
ç®€çŸ­è¯´ï¼š**Scenario å°±æ˜¯ä¸€æ¬¡â€œä»»åŠ¡+ç¯å¢ƒé…ç½®â€çš„æ‰“åŒ…**ã€‚å®ƒæŠŠâ€œè¦åšä»€ä¹ˆâ€ï¼ˆtaskï¼‰å’Œâ€œåœ¨å“ªå„¿/æ€ä¹ˆåšâ€ï¼ˆç¯å¢ƒæˆ–æœåŠ¡å™¨å‚æ•°ã€åˆå§‹çŠ¶æ€ã€æ­¥æ•°ä¸Šé™ç­‰ï¼‰è£…è¿›ä¸€ä¸ªå¯åºåˆ—åŒ–çš„å°å¯¹è±¡é‡Œï¼Œäº¤ç»™ `rollout(...)` å»è¿è¡Œå¹¶é‡‡æ ·è½¨è¿¹ï¼ˆtrajectoryï¼‰ã€‚ä¸åŒå­é¡¹ç›®é‡Œæœ‰å„è‡ªçš„ Scenario ç»“æ„ï¼Œä½†æœ¬è´¨éƒ½æ˜¯â€œè®© `rollout` çŸ¥é“å¦‚ä½•å¼€å±€å’Œä½•æ—¶ç»“æŸâ€çš„æ•°æ®å®¹å™¨ã€‚

## ä»£ç é‡Œå·²ç»æœ‰å“ªäº› Scenarioï¼Ÿ

* **MCP ä»£ç†è®­ç»ƒï¼ˆmcp-rlï¼‰**
  `McpScenario`ï¼šä¸€ä¸ª dataclassï¼Œå­—æ®µå¾ˆç²¾ç®€

  * `task_description: str` â€”â€” è¦æ±‚ä»£ç†å®Œæˆçš„è‡ªç„¶è¯­è¨€ä»»åŠ¡
  * `server_params: StdioServerParameters` â€”â€” è¦è¿æ¥çš„ MCP æœåŠ¡å™¨é…ç½®
  * `max_turns: int = 10` â€”â€” å›åˆä¸Šé™
    ç”¨æ³•ï¼š`rollout(model, scenario: McpScenario)` åœ¨ç»™å®š MCP æœåŠ¡å™¨é‡Œæ‰§è¡Œè¿™é¡¹ä»»åŠ¡ï¼ˆè§ `mcp_rl/rollout.py` ç¬¬ 33â€“45 è¡Œä¸ç¬¬ 215â€“221 è¡Œç¤ºä¾‹ï¼‰ã€‚

* **é‚®ä»¶é—®ç­”å®éªŒï¼ˆart-e.pyï¼‰**
  ä¸¤å±‚ç»“æ„ï¼š

  * `Scenario(BaseModel)`ï¼šæ•°æ®é›†é‡Œçš„â€œé—®ç­”åœºæ™¯â€ï¼Œå« `id / question / answer / message_ids / how_realistic / inbox_address / query_date / split` ç­‰ï¼Œç”¨æ¥**å®šä¹‰é—®é¢˜ä¸å‚è€ƒç­”æ¡ˆ**ã€ä»¥åŠåˆ¤åˆ†éœ€è¦çš„å…ƒä¿¡æ¯ï¼ˆè§ `art-e.py` ç¬¬ 118â€“127 è¡Œï¼›åŠ è½½å‡½æ•°åœ¨ç¬¬ 466â€“483 è¡ŒæŠŠæ¯æ¡æ•°æ®è½¬æˆ `Scenario`ï¼‰ã€‚
  * `EmailScenario(BaseModel)`ï¼šè®­ç»ƒ/æ¨ç†æ—¶çš„â€œæ‰§è¡ŒåŒ…è£…â€ï¼Œå« `step` ä¸ `scenario: Scenario`ï¼Œäº¤ç»™ `rollout(model, email_scenario)` ä½¿ç”¨ï¼ˆè§ `art-e.py` ç¬¬ 641â€“648ã€942â€“946 è¡Œï¼‰ã€‚

* **äº•å­—æ£‹è‡ªåšå¼ˆï¼ˆtic\_tac\_toe*ï¼‰*\*
  `TicTacToeScenario(BaseModel)`ï¼šæè¿°åšå¼ˆåˆå§‹æ¡ä»¶å’Œè®­ç»ƒ/éªŒè¯åˆ†å‰²

  * è½»é‡ç‰ˆä»…æœ‰ `step`ï¼ˆ`tic_tac_toe/rollout.py` ç¬¬ 27â€“33 è¡Œï¼‰ã€‚
  * è‡ªåšå¼ˆç‰ˆå¢åŠ  `split / x_teacher / o_teacher / initial_move` ç­‰ï¼ˆ`tic_tac_toe_self_play/rollout.py` ç¬¬ 103â€“113 è¡Œï¼‰ã€‚
    ç”¨æ³•ï¼š`rollout(..., scenario=TicTacToeScenario(...))`ï¼ˆå¤šå¤„ç¤ºä¾‹ï¼Œå¦‚ `train.py`/`tic-tac-toe.py`ï¼‰ã€‚

## æŠŠå®ƒæŠ½è±¡å‡ºæ¥ï¼šScenario çš„é€šç”¨ç»„æˆ

1. **ä»»åŠ¡**ï¼šè‡ªç„¶è¯­è¨€ç›®æ ‡æˆ–æ¸¸æˆ/é—®é¢˜å®šä¹‰ï¼ˆ`task_description` / `question`ï¼‰ã€‚
2. **ç¯å¢ƒ**ï¼šæ€ä¹ˆæ¥å…¥å¤–éƒ¨ç³»ç»Ÿæˆ–å¦‚ä½•åˆå§‹åŒ–çŠ¶æ€ï¼ˆå¦‚ `server_params`ã€æ£‹å±€åˆå§‹è½å­ï¼‰ã€‚
3. **çº¦æŸ**ï¼šå›åˆ/æ­¥æ•°ä¸Šé™ã€éš¾åº¦ã€split ç­‰ï¼ˆ`max_turns`ã€`split`ã€`difficulty`ï¼‰ã€‚
4. **è¯„æµ‹ä¿¡æ¯ï¼ˆå¯é€‰ï¼‰**ï¼šå‚è€ƒç­”æ¡ˆã€åˆ¤åˆ†æ‰€éœ€å…ƒæ•°æ®ï¼ˆ`answer`ã€`message_ids` ç­‰ï¼‰ã€‚

## æˆ‘è¯¥å¦‚ä½•å®šä¹‰è‡ªå·±çš„ Scenarioï¼Ÿ

å…³é”®æ˜¯**å…ˆçœ‹ä½ çš„ `rollout` éœ€è¦ä»€ä¹ˆ**ã€‚`rollout` çš„å‡½æ•°ç­¾åå†³å®šäº† Scenario çš„å­—æ®µã€‚ä¸¾ä¸‰ä¸ªæœ€å°æ¨¡æ¿ï¼š

* **é’ˆå¯¹ MCP å·¥å…·è°ƒç”¨ç±»ä»»åŠ¡**ï¼ˆå¤ç”¨ç°æœ‰ `McpScenario` è¶³å¤Ÿï¼‰ï¼š

  ```python
  from mcp_rl.mcp_rl.rollout import McpScenario, rollout

  scenario = McpScenario(
      task_description="ç”¨ search_symbol æœ biotech ç›¸å…³å…¬å¸å¹¶æ•´ç†ç»“æœ",
      server_params=server_params,  # ä½ å·²æœ‰çš„ MCP StdioServerParameters
      max_turns=8,
  )
  traj = await rollout(model, scenario)
  ```

* **é’ˆå¯¹â€œæœ‰å‚è€ƒç­”æ¡ˆâ€çš„ä¿¡æ¯æ£€ç´¢/é—®ç­”**ï¼ˆä»¿ç…§ `art-e.py`ï¼‰ï¼š

  ```python
  from pydantic import BaseModel
  from typing import List, Literal

  class QARefScenario(BaseModel):
      id: int
      question: str
      answer: str                   # è¯„æµ‹ç”¨çš„å‚è€ƒç­”æ¡ˆ
      evidence_ids: List[str] = []  # å¯é€‰ï¼šå¼•ç”¨åˆ°çš„æ•°æ®/æ–‡æ¡£é”®
      split: Literal["train", "test"] = "train"

  class QARunScenario(BaseModel):
      step: int
      scenario: QARefScenario

  # rollout(model, qa_run_scenario) å†…éƒ¨è¯»å– questionã€äº§ç”Ÿç­”æ¡ˆï¼Œå†å¯¹æ¯” reference
  ```

* **é’ˆå¯¹â€œæœ‰æ˜ç¡®åˆå§‹çŠ¶æ€â€çš„äº¤äº’/åšå¼ˆ**ï¼ˆä»¿ç…§äº•å­—æ£‹ï¼‰ï¼š

  ```python
  from pydantic import BaseModel
  from typing import Optional

  class GameScenario(BaseModel):
      step: int
      split: str = "train"
      seed: Optional[int] = None
      initial_state: Optional[dict] = None
  ```

## è®¾è®¡å°å»ºè®®

* **è¶Šå°è¶Šå¥½**ï¼šåªæ”¾ `rollout` çœŸæ­£éœ€è¦çš„å­—æ®µï¼Œä¾¿äºåºåˆ—åŒ–/è®°å½•/å›æ”¾ã€‚
* **å¯åºåˆ—åŒ–**ï¼šç”¨ `dataclass` æˆ– Pydantic `BaseModel`ï¼ˆæ–¹ä¾¿æ ¡éªŒä¸ä¿å­˜ï¼‰ã€‚
* **æ˜ç¡®è¯„æµ‹æ¥å£**ï¼šå¦‚æœè¦è‡ªåŠ¨æ‰“åˆ†ï¼ŒScenario é‡Œåº”åŒ…å«å‚è€ƒç­”æ¡ˆæˆ–å¯æ®æ­¤å¾—åˆ†çš„çº¿ç´¢ã€‚
* **ä¸æ•°æ®ç”Ÿæˆå¯¹é½**ï¼šä½ è‹¥ç”¨ `scenario_generator.py` è‡ªåŠ¨ç”Ÿæˆä»»åŠ¡ï¼Œå®ƒäº§å‡ºå½¢å¦‚ `{"task": "...", "difficulty": ...}` çš„ JSONï¼›è®­ç»ƒè„šæœ¬å†æŠŠå®ƒè½¬æˆ `McpScenario`ï¼ˆè§ `mcp_rl/train.py` 118â€“137 è¡Œï¼‰ã€‚


# model.get_step()
get_step() ç”¨æ¥æ‹¿â€œè¿™ä¸ªå¯è®­ç»ƒæ¨¡å‹ç›®å‰å¤„åœ¨ç¬¬å‡ æ­¥ï¼ˆglobal training stepï¼‰â€ã€‚å®ƒè¿”å›ä¸€ä¸ªæ•´æ•°ï¼Œæ¯”å¦‚ 0ã€1ã€2â€¦â€¦ï¼Œè¡¨ç¤ºä½ å·²ç»å®Œæˆå¹¶è½ç›˜çš„æœ€æ–°è®­ç»ƒæ­¥ï¼Œä»è€Œè®©è®­ç»ƒ/è¯„æµ‹/ä¿å­˜éƒ½èƒ½æ¥ç€ä¸Šæ¬¡çš„è¿›åº¦ç»§ç»­ï¼Œè€Œä¸æ˜¯ä»å¤´æ¥ã€‚
model.get_step()ï¼ˆsrc/art/model.pyï¼‰æ˜¯ä¸ª async æ–¹æ³• â†’ è°ƒç”¨åç«¯çš„ backend._get_step(model)ã€‚
æœ¬åœ°åç«¯é‡Œï¼ˆsrc/art/local/backend.pyï¼‰å®é™…å®ç°ä¸ºï¼š
è‹¥æ˜¯ TrainableModelï¼Œå°±ä»æ¨¡å‹çš„è¾“å‡ºç›®å½•é‡Œæ‰¾æœ€æ–°çš„ checkpoint ç›®å½•ï¼Œè·¯å¾„çº¦ä¸º
.../<project>/models/<name>/checkpoints/<step:04d>
å¹¶å–å…¶ä¸­æœ€å¤§çš„ <step> ä½œä¸ºå½“å‰ stepï¼ˆè§ src/art/utils/get_model_step.pyï¼‰ã€‚
å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œåˆ™è¿”å› 0ã€‚æ³¨å†Œåä¼šæœ‰ 0000 è¿™ä¸ªåˆå§‹ checkpointã€‚
ä¹‹æ‰€ä»¥æ˜¯ asyncï¼Œæ˜¯å› ä¸ºä¹Ÿå¯èƒ½é€šè¿‡ REST æ¥å£æŸ¥è¯¢è¿œç«¯/æœ¬åœ°æœåŠ¡ï¼ˆè§ src/art/backend.py ä¸ src/art/cli.pyï¼‰ã€‚


#  FastLanguageModel å‚æ•°
ä¸‹é¢æŠŠä½ è´´çš„ `FastLanguageModel.from_pretrained(...)` / `FastModel.from_pretrained(...)` é‡Œå¸¸è§å‚æ•°é€ä¸ªç”¨ä¸­æ–‡è¯´æ˜ï¼Œå¹¶ç»™å‡ºâ€œä»€ä¹ˆæ—¶å€™ç”¨/æ€ä¹ˆé€‰â€çš„ç®€è¦å»ºè®®ã€‚æ‹¬å·é‡Œæ˜¯é»˜è®¤å€¼ï¼ˆæ¥è‡ªä½ ç»™çš„æºç ï¼‰ã€‚

# FastLanguageModel.from\_pretrained(...)ï¼ˆçº¯æ–‡æœ¬/å¸¸è§ LLMï¼‰

* **model\_name** (`"unsloth/Llama-3.2-1B-Instruct"`): è¦åŠ è½½çš„æ¨¡å‹æˆ–è·¯å¾„ã€‚æ—¢å¯æŒ‡å‘åŸºç¡€æ¨¡å‹ï¼Œä¹Ÿå¯æŒ‡å‘åªå« LoRA çš„ä»“åº“ï¼›å¦‚æœæ£€æµ‹åˆ°æ˜¯ LoRA é€‚é…å™¨ï¼Œä¼šè‡ªåŠ¨å›æº¯å¹¶åŠ è½½å…¶ `base_model` å†å¥—ä¸Šé€‚é…å™¨ã€‚
  ç”¨æ³•ï¼šä¼  HF Hub åç§°æˆ–æœ¬åœ°ç›®å½•ã€‚è‹¥ä»“åº“åŒ…å« `adapter_config.json` å°±è§†ä¸º PEFT é€‚é…å™¨ã€‚([Hugging Face][1])
* **max\_seq\_length** (`2048`): ç”Ÿæˆ/è®­ç»ƒæ—¶æ”¯æŒçš„æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦ï¼ˆUnsloth ä¼šæ®æ­¤åˆ›å»º/è¡¥ä¸ç¼“å­˜ï¼‰ã€‚
  ç»éªŒï¼šè®¾ä¸ºä½ å‡†å¤‡è®­ç»ƒ/æ¨ç†æ‰€éœ€çš„æœ€å¤§å€¼ã€‚
* **dtype** (`None`): `torch.float16` / `torch.bfloat16`ï¼ˆæˆ–ç•™ç©ºè‡ªåŠ¨é€‰ bfloat16 æ”¯æŒåˆ™ç”¨ bf16ï¼Œå¦åˆ™ fp16ï¼‰ã€‚
  ç»éªŒï¼šA100/H100/4090 ç­‰ä¼˜å…ˆ `bfloat16`ï¼›è€æ˜¾å¡ç”¨ `float16`ã€‚Unsloth æ–‡æ¡£ä¹Ÿå»ºè®®æŒ‰ç¡¬ä»¶è‡ªåŠ¨é€‰æ‹©ã€‚([Unsloth æ–‡æ¡£][2])
* **load\_in\_4bit** (`True`): æ˜¯å¦ä»¥ **bitsandbytes 4-bit** é‡åŒ–åŠ è½½ï¼ˆQLoRA å¸¸ç”¨ï¼Œæ˜¾å­˜å‹å¥½ï¼‰ã€‚Unsloth ä¼šæŠŠ `quantization_config`ï¼ˆNF4 + double quantï¼‰å†™è¿› `model.config`ã€‚
  ä½•æ—¶ç”¨ï¼šæ˜¾å­˜ç´§/å‡†å¤‡åš QLoRA/LoRAï¼›æ¨ç†æˆ–å¾®è°ƒéƒ½å¯ã€‚([Hugging Face][3])
* **load\_in\_8bit** (`False`): ä»¥ 8-bit é‡åŒ–åŠ è½½ï¼ˆLLM.int8ï¼‰ã€‚ä¸ `load_in_4bit` äº’æ–¥ã€‚([Hugging Face][4])
* **full\_finetuning** (`False`): æ˜¯å¦è¿›è¡Œâ€œå…¨å‚å¾®è°ƒâ€ã€‚è‹¥å¼€äº†å®ƒï¼Œä¼šå¼ºåˆ¶å…³é—­ 4/8bitï¼ˆæºç é‡Œç›´æ¥æ”¹ä¸ºæµ®ç‚¹æƒé‡è®­ç»ƒï¼‰ã€‚
  ä½•æ—¶ç”¨ï¼šä½ çœŸçš„è¦å…¨å‚è®­ç»ƒä¸”æ˜¾å­˜å¤Ÿç”¨ã€‚
* **token** (`None`): Hugging Face è®¿é—®ä»¤ç‰Œï¼Œç”¨äºæ‹‰å–ç§æœ‰æ¨¡å‹/æ¨é€æ¨¡å‹ç­‰ã€‚æ–‡æ¡£åœ¨ä¿å­˜/å¯¼å‡ºç« èŠ‚å¤šæ¬¡å¼ºè°ƒéœ€è®¾ç½® tokenã€‚([Unsloth æ–‡æ¡£][5])
* **device\_map** (`"sequential"`): æ¨¡å‹åˆ†å¸ƒåˆ°è®¾å¤‡çš„æ–¹å¼ã€‚å¸¸è§è¿˜æœ‰ `"auto"`, `"balanced"`, `"balanced_low_0"` ç­‰ï¼ˆç”± Accelerate è®¡ç®—ï¼‰ã€‚
  ä½•æ—¶ç”¨ï¼šå•å¡é»˜è®¤å³å¯ï¼›å¤šå¡/å¤§æ¨¡å‹å»ºè®® `"auto"`ã€‚([Hugging Face][6])
* **rope\_scaling** (`None`): RoPE ç¼©æ”¾é…ç½®ï¼ˆå¦‚æ‰©ä¸Šä¸‹æ–‡æ—¶çš„ YaRN/LLAMA3 æ–¹æ¡ˆï¼Œå–å†³äºåº•æ¨¡æ”¯æŒï¼‰ã€‚
  å¤‡æ³¨ï¼šæ˜¯å¦æ”¯æŒç”± transformers ç‰ˆæœ¬/åº•æ¨¡å†³å®šã€‚([Hugging Face][7])
* **fix\_tokenizer** (`True`): é’ˆå¯¹éƒ¨åˆ†æœ¬åœ°æƒé‡ï¼Œä¼˜å…ˆç”¨åŒç›®å½•ä¸‹çš„ tokenizer ä¸‰ä»¶å¥—ï¼ˆé˜²æ­¢è¢«åº•æ¨¡è¦†ç›–ï¼‰ã€‚
  ä½•æ—¶ç”¨ï¼šæœ¬åœ°è‡ªå¸¦ tokenizerï¼ˆæºç å·²åšå­˜åœ¨æ€§æ£€æŸ¥ï¼‰ã€‚
* **trust\_remote\_code** (`False`): å…è®¸æ‰§è¡Œæ¨¡å‹ä»“åº“è‡ªå®šä¹‰ä»£ç ï¼ˆæŸäº›æ¨¡å‹å¿…éœ€ï¼‰ã€‚([Hugging Face][8])
* **use\_gradient\_checkpointing** (`"unsloth"`): å¯ç”¨ **Unsloth å®šåˆ¶çš„æ¢¯åº¦æ£€æŸ¥ç‚¹**ï¼Œæ›´çœæ˜¾å­˜ï¼›æ–‡æ¡£/ç¤ºä¾‹é‡Œç§°å¯æ˜¾è‘—é™ä½ VRAMã€é€‚åˆé•¿ä¸Šä¸‹æ–‡è®­ç»ƒã€‚å¯è®¾ä¸º `True` æˆ– `"unsloth"`ï¼ˆæ¨èåè€…ï¼‰ã€‚([Unsloth æ–‡æ¡£][9])
* **resize\_model\_vocab** (`None`): éœ€è¦æ—¶è°ƒæ•´è¯è¡¨å¤§å°ï¼ˆä¼šè°ƒç”¨ `resize_token_embeddings`ï¼‰ã€‚
* **revision** (`None`): æŒ‡å®š Hub ä»“åº“çš„åˆ†æ”¯/tag/commitã€‚([Hugging Face][8])
* **use\_exact\_model\_name** (`False`): å…³é—­ Unsloth çš„â€œåç§°é‡å†™/åŠ¨æ€é‡åŒ–åâ€é€»è¾‘ï¼ŒæŒ‰å­—é¢ `model_name` åŠ è½½ï¼ˆä¾‹å¦‚ä½ å·²ä¸‹è½½åˆ°æœ¬åœ°æ—¶ï¼‰ã€‚æ–‡æ¡£åœ¨â€œç¯å¢ƒæ ‡å¿—/ç–‘éš¾æ’è§£â€é‡Œä¹Ÿå±•ç¤ºè¿‡â€œå¼ºåˆ¶ç²¾ç¡®åâ€çš„åœºæ™¯ã€‚([Unsloth æ–‡æ¡£][10])
* **fast\_inference** (`False`): èµ° **vLLM** æ¨ç†è·¯å¾„ï¼ˆéœ€ `pip install vllm`ï¼‰ï¼›Unsloth æ–‡æ¡£æœ‰ä¸“é—¨çš„ vLLM ä¿å­˜/éƒ¨ç½²é¡µé¢ã€‚
  ä½•æ—¶ç”¨ï¼šéƒ¨ç½²æ¨ç†æœåŠ¡ã€éœ€è¦åå/å¹¶å‘æ›´é«˜ã€‚([Unsloth æ–‡æ¡£][11])
* **gpu\_memory\_utilization** (`0.5`): vLLM æ˜¾å­˜åˆ©ç”¨ç‡ï¼ˆä»… `fast_inference=True` æ—¶æœ‰ç”¨ï¼‰ã€‚
* **float8\_kv\_cache** (`False`): KV-cache ç”¨ float8 å­˜å‚¨ä»¥çœæ˜¾å­˜ï¼ˆæ›´åæ¨ç†ä¼˜åŒ–ï¼‰ã€‚
* **random\_state** (`3407`): éšæœºç§å­ï¼ˆç”¨äºè®­ç»ƒ/é‡‡æ ·çš„å¯å¤ç°æ€§ï¼‰ã€‚
* **max\_lora\_rank** (`64`): ç»™ LoRA çš„ rank ä¸Šé™ï¼ˆä¾¿äºåç»­ `get_peft_model` æ—¶åšæ£€æŸ¥/ä¼˜åŒ–ï¼Œä¸€èˆ¬ä¿é»˜è®¤å³å¯ï¼‰ã€‚
* **disable\_log\_stats** (`True`): å…³é—­è‹¥å¹²ç»Ÿè®¡æ—¥å¿—è¾“å‡ºï¼Œå‡å°‘å™ªå£°ã€‚

> è¿”å›å€¼ï¼š`(model, tokenizer)`ï¼›å®˜æ–¹â€œæ¨ç†â€é¡µä¹Ÿç”¨è¯¥äºŒå…ƒç»„ç¤ºä¾‹ï¼Œå¹¶å»ºè®®å†è°ƒç”¨ `FastLanguageModel.for_inference(model)` ä»¥å¼€å¯æœ¬åœ° 2Ã— æ¨ç†ä¼˜åŒ–ã€‚([Unsloth æ–‡æ¡£][12])

---

# FastModel.from\_pretrained(...)ï¼ˆæ›´é€šç”¨ï¼šå«å¤šæ¨¡æ€/è§†è§‰ï¼‰

å¤šæ•°å‚æ•°ä¸ä¸Šé¢ä¸€è‡´ï¼Œå¦æœ‰ï¼š

* **return\_logits** (`False`): ä¸ºè¯„æµ‹ç­‰åœºæ™¯ç›´æ¥è¿”å› logitsï¼ˆä¹Ÿå¯ç”¨ç¯å¢ƒå˜é‡å¼€å…³ï¼‰ã€‚([Unsloth æ–‡æ¡£][10])
* **fullgraph** (`True`): ä¸ Unsloth çš„ compile/å›¾ä¼˜åŒ–ç›¸å…³ï¼ˆæ§åˆ¶æ˜¯å¦å…è®¸å›¾ä¸­æ–­ç­‰ï¼Œå½±å“é€Ÿåº¦/ç¨³å®šæ€§ï¼‰ã€‚
* **auto\_model** (`None`): è‡ªåŠ¨é€‰ `AutoModelForCausalLM`ï¼ˆæ–‡æœ¬ï¼‰æˆ– `AutoModelForVision2Seq`ï¼ˆVLMï¼‰ï¼›æºç ä¼šæ ¹æ® `architectures/vision_config` åˆ¤æ–­æ˜¯å¦æ˜¯å¤šæ¨¡æ€ã€‚
* **whisper\_language / whisper\_task**: è‹¥åŠ è½½ Whisper ç³»åˆ—ï¼ŒæŒ‡å®š ASR è¯­è¨€/ä»»åŠ¡ã€‚
* **unsloth\_force\_compile** (`False`): å¼ºåˆ¶ç¼–è¯‘ï¼ˆdebug/æ€§èƒ½è°ƒä¼˜ç”¨ï¼‰ã€‚
* è§†è§‰/å¤šæ¨¡æ€ç›¸å…³çš„æ•™ç¨‹åœ¨â€œVision Fine-tuningâ€å’Œå„æ¨¡å‹ä¸“é¡¹é¡µï¼ˆPixtralã€Qwen2-VLã€Llama 3.2 Vision ç­‰ï¼‰ã€‚([Unsloth æ–‡æ¡£][13], [CSDNåšå®¢][14])

---

## å¸¸è§ç”¨æ³•å°æŠ„

```python
# 1) QLoRA å¾®è°ƒï¼ˆ4-bit çœæ˜¾å­˜ï¼Œæ¨èï¼‰
from unsloth import FastLanguageModel
model, tok = FastLanguageModel.from_pretrained(
    model_name="unsloth/llama-3.1-8b-unsloth-bnb-4bit",
    load_in_4bit=True, dtype=None, use_gradient_checkpointing="unsloth"
)
# æ¥ç€ï¼šFastLanguageModel.get_peft_model(...) å¼€ LoRA
```

ï¼ˆQLoRA + 4bit çš„èƒŒæ™¯ä¸ä¼˜ç‚¹è¯¦è§ HF å®˜æ–¹é‡åŒ–ä¸ PEFT æ–‡æ¡£ã€‚ï¼‰([Hugging Face][15])

```python
# 2) å…¨å‚å¾®è°ƒï¼ˆéœ€è¦å¤§æ˜¾å­˜ï¼‰
model, tok = FastLanguageModel.from_pretrained(
    model_name="meta-llama/Llama-3.1-8B",
    full_finetuning=True, load_in_4bit=False, load_in_8bit=False,
    dtype=torch.bfloat16, device_map="auto"
)
```

ï¼ˆ`device_map="auto"` ç”± Accelerate è‡ªåŠ¨åšâ€œå¤§æ¨¡å‹åˆ‡åˆ†â€ä»¥é€‚é…å¤šå¡/å•æœºæ˜¾å­˜ã€‚ï¼‰([Hugging Face][6])

```python
# 3) vLLM å¿«é€Ÿæ¨ç†
model, tok = FastLanguageModel.from_pretrained(
    "unsloth/Llama-3.1-8B-Instruct-bnb-4bit",
    fast_inference=True, gpu_memory_utilization=0.8
)
```

ï¼ˆUnsloth æ”¯æŒä¸ vLLM çš„ä¿å­˜/éƒ¨ç½²è”åŠ¨ã€‚ï¼‰([Unsloth æ–‡æ¡£][11])

---

## é‡è¦è¡Œä¸º/å‘ç‚¹ï¼ˆæ¥è‡ªæºç ï¼‰

* **LoRA é€‚é…å™¨ä¸åŸºåº§ä¸èƒ½åŒä»“**ï¼šè‹¥åŒä¸€ç›®å½•åŒæ—¶å­˜åœ¨ `config.json` å’Œ `adapter_config.json`ï¼Œå‡½æ•°ä¼šæŠ¥é”™ï¼Œè¦æ±‚åˆ†ä»“ï¼ˆä¸€ä¸ªæ”¾åŸºåº§ï¼Œä¸€ä¸ªæ”¾ LoRAï¼‰ã€‚è¿™æ˜¯ä¸ºäº†é¿å…â€œåˆ°åº•åŠ è½½è°â€çš„äºŒä¹‰æ€§ã€‚
* **è‡ªåŠ¨é‡åŒ–é…ç½®**ï¼š`load_in_4bit=True` æ—¶ï¼Œå‡½æ•°ä¼šæŠŠ `nf4 + double quant` ç­‰å†™å…¥ `model.config.quantization_config`ï¼Œä¾¿äºåç»­åœ¨ transformers æ¡†æ¶ä¸‹æ­£ç¡®è¯†åˆ«é‡åŒ–çŠ¶æ€ã€‚ç›¸å…³é‡åŒ–åŸç†å‚è§ HF å®˜æ–¹æ–‡æ¡£ã€‚([Hugging Face][3])
* **Unsloth æ¢¯åº¦æ£€æŸ¥ç‚¹**ï¼š`"unsloth"` æ–¹æ¡ˆæ›´çœæ˜¾å­˜ã€é€‚åˆé•¿ä¸Šä¸‹æ–‡ä»»åŠ¡ï¼›å®˜æ–¹ç¤ºä¾‹åœ¨ RL/DPO æ•™ç¨‹é‡Œä¹Ÿé»˜è®¤è¿™ä¹ˆè®¾ã€‚([Unsloth æ–‡æ¡£][9])
* **token ç™»å½•**ï¼šç§æœ‰æ¨¡å‹/æ¨é€åˆ° Hub æ—¶éœ€è¦ HF tokenã€‚([Unsloth æ–‡æ¡£][5])

å¦‚æœä½ ç»™æˆ‘ä½ ç°åœ¨çš„**æ˜¾å¡å‹å·/æ˜¾å­˜**å’Œ**ç›®çš„ï¼ˆå¾®è°ƒ/æ¨ç†/å¤šæ¨¡æ€ï¼Ÿï¼‰**ï¼Œ

[1]: https://huggingface.co/docs/peft/v0.10.0/en/package_reference/peft_model?utm_source=chatgpt.com "Models - Hugging Face"
[2]: https://docs.unsloth.ai/get-started/fine-tuning-llms-guide?utm_source=chatgpt.com "Fine-tuning LLMs Guide | Unsloth Documentation"
[3]: https://huggingface.co/docs/transformers/main/en/quantization/bitsandbytes?utm_source=chatgpt.com "Bitsandbytes"
[4]: https://huggingface.co/docs/transformers/v4.27.0/main_classes/quantization?utm_source=chatgpt.com "Quantize Transformers models - Hugging Face"
[5]: https://docs.unsloth.ai/basics/running-and-saving-models/saving-to-ollama?utm_source=chatgpt.com "Saving to Ollama | Unsloth Documentation"
[6]: https://huggingface.co/docs/accelerate/usage_guides/big_modeling?utm_source=chatgpt.com "Big Model Inference - Hugging Face"
[7]: https://huggingface.co/docs/transformers/main/index?utm_source=chatgpt.com "Transformers - Hugging Face"
[8]: https://huggingface.co/docs/transformers/main_classes/model?utm_source=chatgpt.com "Models - Hugging Face"
[9]: https://docs.unsloth.ai/basics/reinforcement-learning-rl-guide/reinforcement-learning-dpo-orpo-and-kto?utm_source=chatgpt.com "Reinforcement Learning - DPO, ORPO & KTO - Unsloth"
[10]: https://docs.unsloth.ai/basics/troubleshooting-and-faqs/unsloth-environment-flags?utm_source=chatgpt.com "Unsloth Environment Flags | Unsloth Documentation"
[11]: https://docs.unsloth.ai/basics/running-and-saving-models/saving-to-vllm?utm_source=chatgpt.com "Saving to VLLM | Unsloth Documentation"
[12]: https://docs.unsloth.ai/basics/running-and-saving-models/inference "Inference | Unsloth Documentation"
[13]: https://docs.unsloth.ai/basics/vision-fine-tuning?utm_source=chatgpt.com "Vision Fine-tuning | Unsloth Documentation"
[14]: https://blog.csdn.net/raozhongbo/article/details/149329645?utm_source=chatgpt.com "ä½¿ç”¨unslothæ¨¡å‹å¾®è°ƒè¿‡ç¨‹_unslothå¾®è°ƒå…¨æµç¨‹-CSDNåšå®¢"
[15]: https://huggingface.co/docs/transformers/v4.48.0/en/quantization/bitsandbytes?utm_source=chatgpt.com "bitsandbytes"


# è®­ç»ƒæ¨¡å‹çš„ä¿å­˜ç»“æœ
è§£é‡Šä¸‹é¢çš„ARTæ¨¡å‹è®­ç»ƒçš„ä¿å­˜çš„ä¿¡æ¯
tree /workspace/verl/ART/.art/hn_title_generation/
/workspace/verl/ART/.art/hn_title_generation/
â””â”€â”€ models
    â””â”€â”€ 001
        â”œâ”€â”€ checkpoints
        â”‚Â Â  â”œâ”€â”€ 0251
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ README.md
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ adapter_config.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ adapter_model.safetensors
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ added_tokens.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ chat_template.jinja
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ merges.txt
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ special_tokens_map.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ tokenizer.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ tokenizer_config.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ training_args.bin
        â”‚Â Â  â”‚Â Â  â””â”€â”€ vocab.json
        â”‚Â Â  â”œâ”€â”€ 0252
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ README.md
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ adapter_config.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ adapter_model.safetensors
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ added_tokens.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ chat_template.jinja
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ merges.txt
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ special_tokens_map.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ tokenizer.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ tokenizer_config.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ training_args.bin
        â”‚Â Â  â”‚Â Â  â””â”€â”€ vocab.json
        â”‚Â Â  â”œâ”€â”€ 0253
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ README.md
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ adapter_config.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ adapter_model.safetensors
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ added_tokens.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ chat_template.jinja
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ merges.txt
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ special_tokens_map.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ tokenizer.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ tokenizer_config.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ training_args.bin
        â”‚Â Â  â”‚Â Â  â””â”€â”€ vocab.json
        â”‚Â Â  â”œâ”€â”€ 0254
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ README.md
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ adapter_config.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ adapter_model.safetensors
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ added_tokens.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ chat_template.jinja
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ merges.txt
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ special_tokens_map.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ tokenizer.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ tokenizer_config.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ training_args.bin
        â”‚Â Â  â”‚Â Â  â””â”€â”€ vocab.json
        â”‚Â Â  â”œâ”€â”€ 0255
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ README.md
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ adapter_config.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ adapter_model.safetensors
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ added_tokens.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ chat_template.jinja
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ merges.txt
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ special_tokens_map.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ tokenizer.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ tokenizer_config.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ training_args.bin
        â”‚Â Â  â”‚Â Â  â””â”€â”€ vocab.json
        â”‚Â Â  â”œâ”€â”€ 0256
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ README.md
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ adapter_config.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ adapter_model.safetensors
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ added_tokens.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ chat_template.jinja
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ merges.txt
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ special_tokens_map.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ tokenizer.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ tokenizer_config.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ training_args.bin
        â”‚Â Â  â”‚Â Â  â””â”€â”€ vocab.json
        â”‚Â Â  â”œâ”€â”€ 0257
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ README.md
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ adapter_config.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ adapter_model.safetensors
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ added_tokens.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ chat_template.jinja
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ merges.txt
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ special_tokens_map.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ tokenizer.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ tokenizer_config.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ training_args.bin
        â”‚Â Â  â”‚Â Â  â””â”€â”€ vocab.json
        â”‚Â Â  â”œâ”€â”€ 0258
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ README.md
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ adapter_config.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ adapter_model.safetensors
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ added_tokens.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ chat_template.jinja
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ merges.txt
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ special_tokens_map.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ tokenizer.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ tokenizer_config.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ training_args.bin
        â”‚Â Â  â”‚Â Â  â””â”€â”€ vocab.json
        â”‚Â Â  â”œâ”€â”€ 0259
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ README.md
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ adapter_config.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ adapter_model.safetensors
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ added_tokens.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ chat_template.jinja
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ merges.txt
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ special_tokens_map.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ tokenizer.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ tokenizer_config.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ training_args.bin
        â”‚Â Â  â”‚Â Â  â””â”€â”€ vocab.json
        â”‚Â Â  â”œâ”€â”€ 0260
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ README.md
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ adapter_config.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ adapter_model.safetensors
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ added_tokens.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ chat_template.jinja
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ merges.txt
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ special_tokens_map.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ tokenizer.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ tokenizer_config.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ training_args.bin
        â”‚Â Â  â”‚Â Â  â””â”€â”€ vocab.json
        â”‚Â Â  â”œâ”€â”€ 0261
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ README.md
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ adapter_config.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ adapter_model.safetensors
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ added_tokens.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ chat_template.jinja
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ merges.txt
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ special_tokens_map.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ tokenizer.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ tokenizer_config.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ training_args.bin
        â”‚Â Â  â”‚Â Â  â””â”€â”€ vocab.json
        â”‚Â Â  â”œâ”€â”€ 0262
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ README.md
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ adapter_config.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ adapter_model.safetensors
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ added_tokens.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ chat_template.jinja
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ merges.txt
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ special_tokens_map.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ tokenizer.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ tokenizer_config.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ training_args.bin
        â”‚Â Â  â”‚Â Â  â””â”€â”€ vocab.json
        â”‚Â Â  â”œâ”€â”€ 0263
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ README.md
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ adapter_config.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ adapter_model.safetensors
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ added_tokens.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ chat_template.jinja
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ merges.txt
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ special_tokens_map.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ tokenizer.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ tokenizer_config.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ training_args.bin
        â”‚Â Â  â”‚Â Â  â””â”€â”€ vocab.json
        â”‚Â Â  â”œâ”€â”€ 0264
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ README.md
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ adapter_config.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ adapter_model.safetensors
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ added_tokens.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ chat_template.jinja
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ merges.txt
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ special_tokens_map.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ tokenizer.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ tokenizer_config.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ training_args.bin
        â”‚Â Â  â”‚Â Â  â””â”€â”€ vocab.json
        â”‚Â Â  â”œâ”€â”€ 0265
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ README.md
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ adapter_config.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ adapter_model.safetensors
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ added_tokens.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ chat_template.jinja
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ merges.txt
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ special_tokens_map.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ tokenizer.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ tokenizer_config.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ training_args.bin
        â”‚Â Â  â”‚Â Â  â””â”€â”€ vocab.json
        â”‚Â Â  â”œâ”€â”€ 0266
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ README.md
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ adapter_config.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ adapter_model.safetensors
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ added_tokens.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ chat_template.jinja
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ merges.txt
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ special_tokens_map.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ tokenizer.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ tokenizer_config.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ training_args.bin
        â”‚Â Â  â”‚Â Â  â””â”€â”€ vocab.json
        â”‚Â Â  â”œâ”€â”€ 0267
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ README.md
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ adapter_config.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ adapter_model.safetensors
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ added_tokens.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ chat_template.jinja
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ merges.txt
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ special_tokens_map.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ tokenizer.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ tokenizer_config.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ training_args.bin
        â”‚Â Â  â”‚Â Â  â””â”€â”€ vocab.json
        â”‚Â Â  â”œâ”€â”€ 0268
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ README.md
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ adapter_config.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ adapter_model.safetensors
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ added_tokens.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ chat_template.jinja
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ merges.txt
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ special_tokens_map.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ tokenizer.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ tokenizer_config.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ training_args.bin
        â”‚Â Â  â”‚Â Â  â””â”€â”€ vocab.json
        â”‚Â Â  â”œâ”€â”€ 0269
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ README.md
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ adapter_config.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ adapter_model.safetensors
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ added_tokens.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ chat_template.jinja
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ merges.txt
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ special_tokens_map.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ tokenizer.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ tokenizer_config.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ training_args.bin
        â”‚Â Â  â”‚Â Â  â””â”€â”€ vocab.json
        â”‚Â Â  â”œâ”€â”€ 0270
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ README.md
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ adapter_config.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ adapter_model.safetensors
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ added_tokens.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ chat_template.jinja
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ merges.txt
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ special_tokens_map.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ tokenizer.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ tokenizer_config.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ training_args.bin
        â”‚Â Â  â”‚Â Â  â””â”€â”€ vocab.json
        â”‚Â Â  â”œâ”€â”€ 0271
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ README.md
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ adapter_config.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ adapter_model.safetensors
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ added_tokens.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ chat_template.jinja
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ merges.txt
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ special_tokens_map.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ tokenizer.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ tokenizer_config.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ training_args.bin
        â”‚Â Â  â”‚Â Â  â””â”€â”€ vocab.json
        â”‚Â Â  â”œâ”€â”€ 0272
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ README.md
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ adapter_config.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ adapter_model.safetensors
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ added_tokens.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ chat_template.jinja
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ merges.txt
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ special_tokens_map.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ tokenizer.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ tokenizer_config.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ training_args.bin
        â”‚Â Â  â”‚Â Â  â””â”€â”€ vocab.json
        â”‚Â Â  â”œâ”€â”€ 0273
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ README.md
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ adapter_config.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ adapter_model.safetensors
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ added_tokens.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ chat_template.jinja
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ merges.txt
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ special_tokens_map.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ tokenizer.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ tokenizer_config.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ training_args.bin
        â”‚Â Â  â”‚Â Â  â””â”€â”€ vocab.json
        â”‚Â Â  â”œâ”€â”€ 0274
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ README.md
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ adapter_config.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ adapter_model.safetensors
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ added_tokens.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ chat_template.jinja
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ merges.txt
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ special_tokens_map.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ tokenizer.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ tokenizer_config.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ training_args.bin
        â”‚Â Â  â”‚Â Â  â””â”€â”€ vocab.json
        â”‚Â Â  â”œâ”€â”€ 0275
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ README.md
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ adapter_config.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ adapter_model.safetensors
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ added_tokens.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ chat_template.jinja
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ merges.txt
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ special_tokens_map.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ tokenizer.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ tokenizer_config.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ training_args.bin
        â”‚Â Â  â”‚Â Â  â””â”€â”€ vocab.json
        â”‚Â Â  â”œâ”€â”€ 0276
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ README.md
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ adapter_config.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ adapter_model.safetensors
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ added_tokens.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ chat_template.jinja
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ merges.txt
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ special_tokens_map.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ tokenizer.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ tokenizer_config.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ training_args.bin
        â”‚Â Â  â”‚Â Â  â””â”€â”€ vocab.json
        â”‚Â Â  â”œâ”€â”€ 0277
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ README.md
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ adapter_config.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ adapter_model.safetensors
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ added_tokens.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ chat_template.jinja
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ merges.txt
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ special_tokens_map.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ tokenizer.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ tokenizer_config.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ training_args.bin
        â”‚Â Â  â”‚Â Â  â””â”€â”€ vocab.json
        â”‚Â Â  â”œâ”€â”€ 0278
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ README.md
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ adapter_config.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ adapter_model.safetensors
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ added_tokens.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ chat_template.jinja
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ merges.txt
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ special_tokens_map.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ tokenizer.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ tokenizer_config.json
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ training_args.bin
        â”‚Â Â  â”‚Â Â  â””â”€â”€ vocab.json
        â”‚Â Â  â””â”€â”€ 0279
        â”‚Â Â      â”œâ”€â”€ README.md
        â”‚Â Â      â”œâ”€â”€ adapter_config.json
        â”‚Â Â      â”œâ”€â”€ adapter_model.safetensors
        â”‚Â Â      â”œâ”€â”€ added_tokens.json
        â”‚Â Â      â”œâ”€â”€ chat_template.jinja
        â”‚Â Â      â”œâ”€â”€ merges.txt
        â”‚Â Â      â”œâ”€â”€ special_tokens_map.json
        â”‚Â Â      â”œâ”€â”€ tokenizer.json
        â”‚Â Â      â”œâ”€â”€ tokenizer_config.json
        â”‚Â Â      â”œâ”€â”€ training_args.bin
        â”‚Â Â      â””â”€â”€ vocab.json
        â”œâ”€â”€ history.jsonl
        â”œâ”€â”€ logs
        â”‚Â Â  â””â”€â”€ vllm.log
        â”œâ”€â”€ model.json
        â”œâ”€â”€ tensors
        â”‚Â Â  â”œâ”€â”€ advantages.pt
        â”‚Â Â  â”œâ”€â”€ assistant_mask.pt
        â”‚Â Â  â”œâ”€â”€ group_ids.pt
        â”‚Â Â  â”œâ”€â”€ input_pos.pt
        â”‚Â Â  â”œâ”€â”€ logprobs.pt
        â”‚Â Â  â”œâ”€â”€ parent_ids.pt
        â”‚Â Â  â”œâ”€â”€ tokens.pt
        â”‚Â Â  â””â”€â”€ weights.pt
        â””â”€â”€ trajectories
            â”œâ”€â”€ train
            â”‚Â Â  â”œâ”€â”€ 0000.jsonl
            â”‚Â Â  â”œâ”€â”€ 0001.jsonl

            â”‚Â Â  â”œâ”€â”€ 0273.jsonl
            â”‚Â Â  â”œâ”€â”€ 0274.jsonl
            â”‚Â Â  â”œâ”€â”€ 0275.jsonl
            â”‚Â Â  â”œâ”€â”€ 0276.jsonl
            â”‚Â Â  â”œâ”€â”€ 0277.jsonl
            â”‚Â Â  â””â”€â”€ 0278.jsonl
            â””â”€â”€ val
                â”œâ”€â”€ 0051.jsonl
                â”œâ”€â”€ 0101.jsonl
                â”œâ”€â”€ 0151.jsonl
                â”œâ”€â”€ 0201.jsonl
                â””â”€â”€ 0251.jsonl

37 directories, 614 files

æ¨ç† / ç»§ç»­è®­ç»ƒ / å¯¼å‡ºåˆå¹¶

# é¡¶å±‚ä¸è¿è¡Œç¼–å·

* `.art/hn_title_generation/models/001/`
  `001`é€šå¸¸æ˜¯è¿™æ¬¡å®éªŒ / è¿è¡Œ(run)çš„ç¼–å·ã€‚ä¸€ä¸ªå·¥ç¨‹é‡Œå¯ä»¥æœ‰ `002ã€003â€¦` è¡¨ç¤ºä¸åŒæ¬¡è®­ç»ƒã€‚

* `model.json`
  è®°å½•æœ¬æ¬¡è¿è¡Œçš„å…³é”®ä¿¡æ¯ï¼ˆä¾‹å¦‚åŸºåº§æ¨¡å‹åç§°ã€é€‚é…å™¨ç±»å‹/è·¯å¾„ã€è¶…å‚æ‘˜è¦ç­‰ï¼‰ã€‚ç”¨æ¥è®©æ¡†æ¶åœ¨åŠ è½½/æ¢å¤æ—¶çŸ¥é“â€œæˆ‘æ˜¯è°ã€åŸºäºè°â€ã€‚

* `history.jsonl`
  æŒ‰ step è¿½åŠ çš„è®­ç»ƒæ—¥å¿—ï¼ˆJSON Linesï¼‰ã€‚é‡Œé¢é€šå¸¸åŒ…å«è®­ç»ƒ/è¯„ä¼°æŒ‡æ ‡ï¼ˆå¦‚lossã€rewardã€å­¦ä¹ ç‡ã€æ­¥æ•°ç­‰ï¼‰ã€‚ç”¨å®ƒå¯ä»¥ç”»æ›²çº¿æˆ–å®šä½â€œæœ€ä½³ stepâ€ã€‚

* `logs/vllm.log`
  æ¨æ–­/é‡‡æ ·é˜¶æ®µä½¿ç”¨ vLLM æ—¶çš„æ—¥å¿—ï¼ˆç«¯å£ã€ååã€é”™è¯¯å †æ ˆç­‰ï¼‰ã€‚

# checkpointsï¼šæ¯ä¸ªä¿å­˜ç‚¹ä¸€å¥—å¯å¤ç°å·¥ä»¶

`checkpoints/0251 ~ 0279/` è¿™äº›ç¼–å·åŸºæœ¬å°±æ˜¯**å…¨å±€ stepï¼ˆå·¦ä¾§é›¶å¡«å……ï¼‰**ã€‚æ¯ä¸ªå­ç›®å½•éƒ½èƒ½ç‹¬ç«‹ç”¨äºæ¨ç†æˆ–æ¢å¤è®­ç»ƒï¼Œå†…å«ï¼š

* `adapter_model.safetensors` + `adapter_config.json`
  LoRA/QLoRA ç­‰ **PEFT é€‚é…å™¨**æƒé‡ä¸é…ç½®ï¼ˆå¦‚ rã€alphaã€target\_modulesâ€¦ï¼‰ã€‚è¿™æ˜¯â€œè¿™æ¬¡è®­ç»ƒå­¦åˆ°çš„ä¸œè¥¿â€çš„ä¸»ä½“ã€‚
* `training_args.bin`
  ğŸ¤—Transformers çš„ `TrainingArguments` å¯¹è±¡åºåˆ—åŒ–ï¼Œç”¨äº**æ¢å¤è®­ç»ƒè¶…å‚**ã€‚
* `tokenizer.json / tokenizer_config.json / vocab.json / merges.txt / special_tokens_map.json / added_tokens.json`
  **åˆ†è¯å™¨**å…¨å¥—æ–‡ä»¶ï¼Œä¿è¯æ¨ç†/ç»§ç»­è®­ç»ƒæ—¶è¯è¡¨ä¸è®­ç»ƒæ—¶ä¸€è‡´ã€‚
* `chat_template.jinja`
  èŠå¤©æ¨¡æ¿ï¼ˆç³»ç»Ÿ/ç”¨æˆ·/åŠ©æ‰‹æ¶ˆæ¯å¦‚ä½•æ‹¼æˆæ¨¡å‹è¾“å…¥ï¼‰ã€‚æ¨ç†ç”±å®ƒç¡®ä¿**æç¤ºè¯æ ¼å¼**ä¸è®­ç»ƒä¸€è‡´ã€‚
* `README.md`
  è¯¥ checkpoint çš„ç®€è¦è¯´æ˜ï¼ˆä¾‹å¦‚åˆ›å»ºæ—¶é—´ã€åº¦é‡æŒ‡æ ‡æ‘˜è¦ï¼‰ï¼›ä¸åŒç‰ˆæœ¬å†…å®¹å¯èƒ½ç•¥æœ‰å·®å¼‚ã€‚

> é€‰â€œæœ€ä½³â€checkpointï¼š
> ä¸€èˆ¬å‚è€ƒ `history.jsonl`ï¼ˆçœ‹éªŒè¯é›†æŒ‡æ ‡ï¼‰æˆ–å„ checkpoint è‡ªå¸¦çš„ `README.md` æ‘˜è¦ã€‚è‹¥æœªæ˜¾å¼ä¿å­˜â€œbestâ€ï¼Œé€šå¸¸å–**æœ€åä¸€ä¸ª**ï¼ˆè¿™é‡Œæ˜¯ `0279`ï¼‰åšæ¨ç†çš„èµ·ç‚¹ï¼Œç„¶åå†æŒ‰æŒ‡æ ‡å›é€€å¯¹æ¯”ã€‚

# tensorsï¼šä¸­é—´å¼ é‡ç¼“å­˜ï¼ˆéæ¨¡å‹æƒé‡ï¼‰

```
tensors/
â”œâ”€â”€ advantages.pt       # PPO/GRPO ä¸€ç±»ç®—æ³•é‡Œè®¡ç®—å¾—åˆ°çš„ä¼˜åŠ¿ A_t
â”œâ”€â”€ assistant_mask.pt   # æ©ç ï¼šå“ªäº› token å±äºâ€œassistantâ€å›å¤æ®µ
â”œâ”€â”€ group_ids.pt        # æ ·æœ¬åˆ†ç»„/é‡‡æ ·æ‰¹æ¬¡çš„æ˜ å°„
â”œâ”€â”€ input_pos.pt        # ä½ç½®ç´¢å¼•ï¼ˆé‡æ„åºåˆ—æ—¶ç”¨ï¼‰
â”œâ”€â”€ logprobs.pt         # é‡‡æ ·å¾—åˆ°çš„å¯¹æ•°æ¦‚ç‡
â”œâ”€â”€ parent_ids.pt       # å›æº¯åˆ°åŸå§‹è½¨è¿¹/æ ·æœ¬çš„ç´¢å¼•
â”œâ”€â”€ tokens.pt           # å®é™… token åºåˆ—ç¼“å­˜
â””â”€â”€ weights.pt          # è®­ç»ƒç”¨æƒé‡ï¼ˆå¦‚é‡è¦æ€§é‡‡æ ·æƒé‡ç­‰ï¼‰
```

è¿™äº›æ˜¯**åŠ é€Ÿç»§ç»­è®­ç»ƒ/å¤ç°å¯¹é½è®¡ç®—**çš„ç¼“å­˜ï¼›**ä¸æ˜¯**å¯ç›´æ¥ç”¨äºæ¨ç†çš„æ¨¡å‹æƒé‡ã€‚ä¸¢å¤±ä¸ä¼šå½±å“ç”¨ checkpoint æ¨ç†ï¼Œä½†ä¼šå½±å“â€œæ— ç¼æ¢å¤â€æŸäº› RL è®­ç»ƒé˜¶æ®µã€‚

# trajectoriesï¼šè®­ç»ƒ/éªŒè¯â€œè½¨è¿¹â€æ•°æ®

```
trajectories/
â”œâ”€â”€ train/0000.jsonl ~ 0278.jsonl
â””â”€â”€ val/0051.jsonl, 0101.jsonl, ...
```

* æ¯ä¸ª `.jsonl` æ˜¯ä¸€ä¸ª**åˆ†ç‰‡**ï¼ˆshardï¼‰ï¼Œè¡Œçº§ JSON è®°å½•ä¸€æ¬¡â€œå¯¹è¯â†’æ¨¡å‹è¾“å‡ºâ†’æ‰“åˆ†/åé¦ˆâ€çš„**è½¨è¿¹**ã€‚
* å¸¸è§å­—æ®µï¼ˆä¸åŒç‰ˆæœ¬ä¼šæœ‰å·®åˆ«ï¼‰ï¼š`messages`ï¼ˆæˆ– `prompt/completion`ï¼‰ã€`response`ã€`reward(s)`ã€`scores`ã€`meta`ï¼ˆæ¸©åº¦ã€é‡‡æ ·å‚æ•°ã€åŸºåº§æ¨¡å‹å“ˆå¸Œã€æ—¶é—´æˆ³ç­‰ï¼‰ã€ä»¥åŠå¯é€‰çš„ `logprobs`ã€‚
* `val/` æ˜¯éªŒè¯é›†è½¨è¿¹ï¼Œè®­ç»ƒè¿‡ç¨‹ç”¨äºå‘¨æœŸæ€§è¯„ä¼°ã€‚

# å¸¸è§æ“ä½œé€ŸæŸ¥

## 1) ç”¨æŸä¸ª checkpoint åšæ¨ç†ï¼ˆåŠ è½½ LoRA é€‚é…å™¨ï¼‰

```python
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

base_model_path = "<ä½ çš„åŸºåº§æ¨¡å‹è·¯å¾„æˆ–HFæ¨¡å‹å>"   # é€šå¸¸å¯åœ¨ models/001/model.json æ‰¾åˆ°
ckpt_dir = "/workspace/verl/ART/.art/hn_title_generation/models/001/checkpoints/0279"

tok = AutoTokenizer.from_pretrained(ckpt_dir, use_fast=True)
base = AutoModelForCausalLM.from_pretrained(base_model_path, torch_dtype="auto", device_map="auto")
model = PeftModel.from_pretrained(base, ckpt_dir)   # åŠ è½½ adapter_model.safetensors

# å¦‚æœæœ‰ chat_template.jinjaï¼Œå»ºè®®æŒ‰æ¨¡æ¿æ„é€ è¾“å…¥
text = tok.apply_chat_template(
    [{"role":"user","content":"å¸®æˆ‘ä¸ºè¿™ç¯‡æ–‡ç« å†™ä¸€ä¸ªHacker Newsé£æ ¼æ ‡é¢˜ï¼š..."}],
    tokenize=False, add_generation_prompt=True, chat_template=open(f"{ckpt_dir}/chat_template.jinja").read()
)

inputs = tok(text, return_tensors="pt").to(model.device)
outputs = model.generate(**inputs, max_new_tokens=64)
print(tok.decode(outputs[0], skip_special_tokens=True))
```

## 2) æŠŠ LoRA åˆå¹¶æˆâ€œå®Œæ•´æƒé‡â€ï¼ˆä¾¿äºç‹¬ç«‹éƒ¨ç½²ï¼‰

```python
from peft import PeftModel
from transformers import AutoModelForCausalLM

base = AutoModelForCausalLM.from_pretrained(base_model_path, torch_dtype="auto")
model = PeftModel.from_pretrained(base, ckpt_dir)
merged = model.merge_and_unload()                   # åˆå¹¶ LoRA
merged.save_pretrained("/path/to/merged_model")
tok.save_pretrained("/path/to/merged_model")
```

> åˆå¹¶åå°±ä¸å†ä¾èµ– `adapter_model.safetensors`ï¼Œéƒ¨ç½²æ›´ç®€å•ï¼›ä½†ä¼šå ç”¨æ›´å¤šç£ç›˜ã€‚

## 3) ä»æŸä¸ª checkpoint æ¢å¤ç»§ç»­è®­ç»ƒ

å¤§å¤šæ•°åŸºäº ğŸ¤—Trainer/TRL çš„è„šæœ¬æ”¯æŒï¼š

```bash
python train.py \
  --resume_from_checkpoint /workspace/verl/ART/.art/hn_title_generation/models/001/checkpoints/0270
```

å®ƒä¼šè¯»å– `training_args.bin` å’Œè¯¥ç›®å½•å†…åˆ†è¯å™¨/æ¨¡æ¿ï¼Œç»§ç»­æŒ‰ç›¸åŒè®¾ç½®è®­ç»ƒã€‚

# æ¸…ç†ä¸å½’æ¡£å»ºè®®

* **é•¿æœŸä¿ç•™**ï¼š`checkpoints/ï¼ˆæœ€ä½³æˆ–æœ€åä¸€ä¸ªï¼‰`ã€`model.json`ã€`history.jsonl`ã€`chat_template.jinja`ã€‚
* **æŒ‰éœ€ä¿ç•™**ï¼š`trajectories/`ï¼ˆä¸ºäº†è®ºæ–‡å¤ç°/å®¡è®¡å¾ˆé‡è¦ï¼›ä½†ä½“ç§¯å¤§ï¼‰ã€‚
* **å¯æ¸…ç†**ï¼šæ—©æœŸå¤§é‡ checkpoint ä¸ `tensors/`ï¼ˆè‹¥ä¸å†éœ€è¦ä»ä¸­é—´ step ç²¾ç¡®æ¢å¤ï¼‰ã€‚
* **æ—¥å¿—**ï¼š`logs/vllm.log` ç•™ä½œæ’é”™ï¼Œè¿‡æ—§å¯å‹ç¼©å½’æ¡£ã€‚

---

# Art.TrainableModel å’ŒArt.Model